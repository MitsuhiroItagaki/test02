# Databricks notebook source
# MAGIC %md
# MAGIC # Databricks SQLãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ©ãƒ¼åˆ†æãƒ„ãƒ¼ãƒ«
# MAGIC
# MAGIC ã“ã®notebookã¯ã€Databricksã®SQLãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ©ãƒ¼JSONãƒ­ã‚°ãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã¿ã€ãƒœãƒˆãƒ«ãƒãƒƒã‚¯ç‰¹å®šã¨æ”¹å–„æ¡ˆã®æç¤ºã«å¿…è¦ãªãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚’æŠ½å‡ºã—ã¦åˆ†æã‚’è¡Œã„ã¾ã™ã€‚
# MAGIC
# MAGIC ## æ©Ÿèƒ½æ¦‚è¦
# MAGIC
# MAGIC 1. **SQLãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ©ãƒ¼JSONãƒ•ã‚¡ã‚¤ãƒ«ã®èª­ã¿è¾¼ã¿**
# MAGIC    - Databricksã§å‡ºåŠ›ã•ã‚ŒãŸãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ©ãƒ¼ãƒ­ã‚°ã®è§£æ
# MAGIC    - `graphs`ã‚­ãƒ¼ã«æ ¼ç´ã•ã‚ŒãŸå®Ÿè¡Œãƒ—ãƒ©ãƒ³ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã®æŠ½å‡º
# MAGIC
# MAGIC 2. **é‡è¦ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã®æŠ½å‡º**
# MAGIC    - ã‚¯ã‚¨ãƒªåŸºæœ¬æƒ…å ±ï¼ˆIDã€ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹ã€å®Ÿè¡Œæ™‚é–“ãªã©ï¼‰
# MAGIC    - å…¨ä½“ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ï¼ˆå®Ÿè¡Œæ™‚é–“ã€ãƒ‡ãƒ¼ã‚¿é‡ã€ã‚­ãƒ£ãƒƒã‚·ãƒ¥åŠ¹ç‡ãªã©ï¼‰
# MAGIC    - ã‚¹ãƒ†ãƒ¼ã‚¸ãƒ»ãƒãƒ¼ãƒ‰è©³ç´°ãƒ¡ãƒˆãƒªã‚¯ã‚¹
# MAGIC    - ãƒœãƒˆãƒ«ãƒãƒƒã‚¯æŒ‡æ¨™ã®è¨ˆç®—
# MAGIC
# MAGIC 3. **AI ã«ã‚ˆã‚‹ãƒœãƒˆãƒ«ãƒãƒƒã‚¯åˆ†æ**
# MAGIC    - è¨­å®šå¯èƒ½ãªLLMã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆ (Databricks, OpenAI, Azure OpenAI, Anthropic)
# MAGIC    - æŠ½å‡ºãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‹ã‚‰ãƒœãƒˆãƒ«ãƒãƒƒã‚¯ç‰¹å®š
# MAGIC    - å…·ä½“çš„ãªæ”¹å–„æ¡ˆã®æç¤º
# MAGIC
# MAGIC ---
# MAGIC
# MAGIC **äº‹å‰æº–å‚™:**
# MAGIC - LLMã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆã®è¨­å®šï¼ˆDatabricks Model Serving ã¾ãŸã¯ å¤–éƒ¨APIï¼‰
# MAGIC - å¿…è¦ãªAPIã‚­ãƒ¼ã®è¨­å®š
# MAGIC - SQLãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ©ãƒ¼JSONãƒ•ã‚¡ã‚¤ãƒ«ã®æº–å‚™ï¼ˆDBFS ã¾ãŸã¯ FileStoreï¼‰
# MAGIC
# MAGIC ---

# COMMAND ----------

# MAGIC %md
# MAGIC # ğŸ”§ è¨­å®šãƒ»æº–å‚™ã‚»ã‚¯ã‚·ãƒ§ãƒ³
# MAGIC
# MAGIC **ã“ã®ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã§ã¯ãƒ„ãƒ¼ãƒ«ã®åŸºæœ¬è¨­å®šã‚’è¡Œã„ã¾ã™**
# MAGIC
# MAGIC ğŸ“‹ **è¨­å®šå†…å®¹:**
# MAGIC - åˆ†æå¯¾è±¡ãƒ•ã‚¡ã‚¤ãƒ«ã®æŒ‡å®š
# MAGIC - LLMã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆã®è¨­å®š
# MAGIC - åˆ†æé–¢æ•°ã®å®šç¾©
# MAGIC
# MAGIC âš ï¸ **é‡è¦:** ãƒ¡ã‚¤ãƒ³å‡¦ç†ã‚’å®Ÿè¡Œã™ã‚‹å‰ã«ã€ã“ã®ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã®ã™ã¹ã¦ã®ã‚»ãƒ«ã‚’å®Ÿè¡Œã—ã¦ãã ã•ã„

# COMMAND ----------

# MAGIC %md
# MAGIC ## ğŸ“ åˆ†æå¯¾è±¡ãƒ•ã‚¡ã‚¤ãƒ«è¨­å®š
# MAGIC
# MAGIC **æœ€åˆã«ã€åˆ†æå¯¾è±¡ã®SQLãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ©ãƒ¼JSONãƒ•ã‚¡ã‚¤ãƒ«ã‚’æŒ‡å®šã—ã¦ãã ã•ã„ã€‚**
# MAGIC
# MAGIC ã“ã®ã‚»ãƒ«ã§ã¯ä»¥ä¸‹ã®è¨­å®šã‚’è¡Œã„ã¾ã™ï¼š
# MAGIC - ğŸ“‚ SQLãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ©ãƒ¼JSONãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹è¨­å®š
# MAGIC - ğŸ“‹ å¯¾å¿œã™ã‚‹ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹å½¢å¼ã®ä¾‹
# MAGIC - âš™ï¸ åŸºæœ¬çš„ãªç’°å¢ƒè¨­å®š

# COMMAND ----------

# ğŸ“ SQLãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ©ãƒ¼JSONãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹è¨­å®š
# 
# ä»¥ä¸‹ã®JSON_FILE_PATHã‚’å®Ÿéš›ã®ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ã«å¤‰æ›´ã—ã¦ãã ã•ã„ï¼š

# ãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯ç’°å¢ƒç”¨ã®ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹è¨­å®šï¼ˆä»¥ä¸‹ã®ä¸­ã‹ã‚‰é¸æŠã—ã¦ãã ã•ã„ï¼‰

# ã‚ªãƒ—ã‚·ãƒ§ãƒ³1: ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°å‰ãƒ—ãƒ©ãƒ³ãƒ•ã‚¡ã‚¤ãƒ«ï¼ˆæ¨å¥¨ï¼‰
JSON_FILE_PATH = '/Volumes/main/base/mitsuhiro_vol/ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°å‰ãƒ—ãƒ©ãƒ³ãƒ•ã‚¡ã‚¤ãƒ«.json'

# ã‚ªãƒ—ã‚·ãƒ§ãƒ³2: ä»–ã®JSONãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä½¿ç”¨ã™ã‚‹å ´åˆã¯ã€ä»¥ä¸‹ã®ã‚³ãƒ¡ãƒ³ãƒˆã‚¢ã‚¦ãƒˆã‚’è§£é™¤ã—ã¦ç·¨é›†
# JSON_FILE_PATH = '/Volumes/main/base/mitsuhiro_vol/nophoton.json'
# JSON_FILE_PATH = '/Volumes/main/base/mitsuhiro_vol/POC1.json'
# JSON_FILE_PATH = '/Volumes/main/base/mitsuhiro_vol/your_file.json'

# ã‚³ãƒãƒ³ãƒ‰ãƒ©ã‚¤ãƒ³ç’°å¢ƒç”¨ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
import sys
if len(sys.argv) > 1 and not sys.argv[1].startswith('-'):
    # ã‚³ãƒãƒ³ãƒ‰ãƒ©ã‚¤ãƒ³å¼•æ•°ãŒãƒ•ãƒ©ã‚°ï¼ˆ-ã§å§‹ã¾ã‚‹ï¼‰ã§ãªã„å ´åˆã®ã¿ä½¿ç”¨
    JSON_FILE_PATH = sys.argv[1]

# ğŸŒ å‡ºåŠ›è¨€èªè¨­å®šï¼ˆOUTPUT_LANGUAGE: 'ja' = æ—¥æœ¬èª, 'en' = è‹±èªï¼‰
OUTPUT_LANGUAGE = 'ja'

# ğŸ” EXPLAINæ–‡å®Ÿè¡Œè¨­å®šï¼ˆEXPLAIN_ENABLED: 'Y' = å®Ÿè¡Œã™ã‚‹, 'N' = å®Ÿè¡Œã—ãªã„ï¼‰
EXPLAIN_ENABLED = 'Y'

# ğŸ› ãƒ‡ãƒãƒƒã‚°ãƒ¢ãƒ¼ãƒ‰è¨­å®šï¼ˆDEBUG_ENABLE: 'Y' = ä¸­é–“ãƒ•ã‚¡ã‚¤ãƒ«ä¿æŒ, 'N' = æœ€çµ‚ãƒ•ã‚¡ã‚¤ãƒ«ã®ã¿ä¿æŒï¼‰
DEBUG_ENABLE = 'N'

# ğŸ—‚ï¸ ã‚«ã‚¿ãƒ­ã‚°ã¨ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹è¨­å®šï¼ˆEXPLAINæ–‡å®Ÿè¡Œæ™‚ã«ä½¿ç”¨ï¼‰
CATALOG = 'tpcds'
DATABASE = 'tpcds_sf1000_delta_lc'

# ğŸ’¡ ä½¿ç”¨ä¾‹:
# OUTPUT_LANGUAGE = 'ja'  # æ—¥æœ¬èªã§ãƒ•ã‚¡ã‚¤ãƒ«å‡ºåŠ›
# OUTPUT_LANGUAGE = 'en'  # è‹±èªã§ãƒ•ã‚¡ã‚¤ãƒ«å‡ºåŠ›

# ğŸŒ å¤šè¨€èªãƒ¡ãƒƒã‚»ãƒ¼ã‚¸è¾æ›¸
MESSAGES = {
    'ja': {
        'bottleneck_title': 'Databricks SQLãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ©ãƒ¼ ãƒœãƒˆãƒ«ãƒãƒƒã‚¯åˆ†æçµæœ',
        'query_id': 'ã‚¯ã‚¨ãƒªID',
        'analysis_time': 'åˆ†ææ—¥æ™‚',
        'execution_time': 'å®Ÿè¡Œæ™‚é–“',
        'sql_optimization_report': 'SQLæœ€é©åŒ–ãƒ¬ãƒãƒ¼ãƒˆ',
        'optimization_time': 'æœ€é©åŒ–æ—¥æ™‚',
        'original_file': 'ã‚ªãƒªã‚¸ãƒŠãƒ«ãƒ•ã‚¡ã‚¤ãƒ«',
        'optimized_file': 'æœ€é©åŒ–ãƒ•ã‚¡ã‚¤ãƒ«',
        'optimization_analysis': 'æœ€é©åŒ–åˆ†æçµæœ',
        'performance_metrics': 'ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ¡ãƒˆãƒªã‚¯ã‚¹å‚è€ƒæƒ…å ±',
        'read_data': 'èª­ã¿è¾¼ã¿ãƒ‡ãƒ¼ã‚¿',
        'spill': 'ã‚¹ãƒ”ãƒ«',
        'top10_processes': 'æœ€ã‚‚æ™‚é–“ãŒã‹ã‹ã£ã¦ã„ã‚‹å‡¦ç†TOP10'
    },
    'en': {
        'bottleneck_title': 'Databricks SQL Profiler Bottleneck Analysis Results',
        'query_id': 'Query ID',
        'analysis_time': 'Analysis Time',
        'execution_time': 'Execution Time',
        'sql_optimization_report': 'SQL Optimization Report',
        'optimization_time': 'Optimization Time',
        'original_file': 'Original File',
        'optimized_file': 'Optimized File',
        'optimization_analysis': 'Optimization Analysis Results',
        'performance_metrics': 'Performance Metrics Reference',
        'read_data': 'Data Read',
        'spill': 'Spill',
        'top10_processes': 'Top 10 Most Time-Consuming Processes'
    }
}

def get_message(key: str) -> str:
    """å¤šè¨€èªãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’å–å¾—"""
    return MESSAGES.get(OUTPUT_LANGUAGE, MESSAGES['ja']).get(key, key)

# ğŸ“‹ å¯¾å¿œã™ã‚‹ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹å½¢å¼ã®ä¾‹:
# Unity Catalog Volumes:
# JSON_FILE_PATH = '/Volumes/catalog/schema/volume/profiler.json'
# 
# FileStore (æ¨å¥¨):
# JSON_FILE_PATH = '/FileStore/shared_uploads/your_username/profiler_log.json'
# 
# DBFS:
# JSON_FILE_PATH = '/dbfs/FileStore/shared_uploads/your_username/profiler_log.json'
# 
# DBFS URI:
# JSON_FILE_PATH = 'dbfs:/FileStore/shared_uploads/your_username/profiler_log.json'

print("ğŸ“ ã€åˆ†æå¯¾è±¡ãƒ•ã‚¡ã‚¤ãƒ«è¨­å®šå®Œäº†ã€‘")
print("=" * 50)
print(f"ğŸ“„ å¯¾è±¡ãƒ•ã‚¡ã‚¤ãƒ«: {JSON_FILE_PATH}")
print("=" * 50)

# âš™ï¸ åŸºæœ¬çš„ãªç’°å¢ƒè¨­å®š
import json
try:
    import pandas as pd
except ImportError:
    print("Warning: pandas is not installed, some features may not work")
    pd = None
from typing import Dict, List, Any, Optional
from datetime import datetime

print("âœ… åŸºæœ¬ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã‚¤ãƒ³ãƒãƒ¼ãƒˆå®Œäº†")
print("ğŸš€ æ¬¡ã®ã‚»ãƒ«ã«é€²ã‚“ã§ãã ã•ã„")

# COMMAND ----------

# MAGIC %md
# MAGIC ## ğŸ¤– LLMã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆè¨­å®š
# MAGIC
# MAGIC ã“ã®ã‚»ãƒ«ã§ã¯ä»¥ä¸‹ã®è¨­å®šã‚’è¡Œã„ã¾ã™ï¼š
# MAGIC - LLMãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ã®é¸æŠï¼ˆDatabricks/OpenAI/Azure/Anthropicï¼‰
# MAGIC - å„ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ã®æ¥ç¶šè¨­å®š
# MAGIC - å¿…è¦ãªãƒ©ã‚¤ãƒ–ãƒ©ãƒªã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆ

# COMMAND ----------

# ğŸ¤– LLMã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆè¨­å®š
LLM_CONFIG = {
    # ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆã‚¿ã‚¤ãƒ—: 'databricks', 'openai', 'azure_openai', 'anthropic'
    "provider": "databricks",
    
    # Databricks Model Servingè¨­å®šï¼ˆé«˜é€Ÿå®Ÿè¡Œå„ªå…ˆï¼‰
    "databricks": {
        "endpoint_name": "databricks-claude-3-7-sonnet",  # Model Servingã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆå
        "max_tokens": 131072,  # 128K tokensï¼ˆClaude 3.7 Sonnetã®æœ€å¤§åˆ¶é™ï¼‰
        "temperature": 0.0,    # æ±ºå®šçš„ãªå‡ºåŠ›ã®ãŸã‚ï¼ˆ0.1â†’0.0ï¼‰
        # "thinking_enabled": False,  # æ‹¡å¼µæ€è€ƒãƒ¢ãƒ¼ãƒ‰ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: ç„¡åŠ¹ - é«˜é€Ÿå®Ÿè¡Œå„ªå…ˆï¼‰- Claude 3 Sonnetå°‚ç”¨
        # "thinking_budget_tokens": 65536  # æ€è€ƒç”¨ãƒˆãƒ¼ã‚¯ãƒ³äºˆç®— 64K tokensï¼ˆæœ‰åŠ¹æ™‚ã®ã¿ä½¿ç”¨ï¼‰- Claude 3 Sonnetå°‚ç”¨
    },
    
    # OpenAIè¨­å®šï¼ˆå®Œå…¨ãªSQLç”Ÿæˆç”¨ã«æœ€é©åŒ–ï¼‰
    "openai": {
        "api_key": "",  # OpenAI APIã‚­ãƒ¼ (ç’°å¢ƒå¤‰æ•°OPENAI_API_KEYã§ã‚‚å¯)
        "model": "gpt-4o",  # gpt-4o, gpt-4-turbo, gpt-3.5-turbo
        "max_tokens": 16000,  # OpenAIã®åˆ¶é™å†…æœ€å¤§
        "temperature": 0.0    # æ±ºå®šçš„ãªå‡ºåŠ›ã®ãŸã‚ï¼ˆ0.1â†’0.0ï¼‰
    },
    
    # Azure OpenAIè¨­å®šï¼ˆå®Œå…¨ãªSQLç”Ÿæˆç”¨ã«æœ€é©åŒ–ï¼‰
    "azure_openai": {
        "api_key": "",  # Azure OpenAI APIã‚­ãƒ¼ (ç’°å¢ƒå¤‰æ•°AZURE_OPENAI_API_KEYã§ã‚‚å¯)
        "endpoint": "",  # https://your-resource.openai.azure.com/
        "deployment_name": "",  # ãƒ‡ãƒ—ãƒ­ã‚¤ãƒ¡ãƒ³ãƒˆå
        "api_version": "2024-02-01",
        "max_tokens": 16000,  # Azure OpenAIã®åˆ¶é™å†…æœ€å¤§
        "temperature": 0.0    # æ±ºå®šçš„ãªå‡ºåŠ›ã®ãŸã‚ï¼ˆ0.1â†’0.0ï¼‰
    },
    
    # Anthropicè¨­å®šï¼ˆå®Œå…¨ãªSQLç”Ÿæˆç”¨ã«æœ€é©åŒ–ï¼‰
    "anthropic": {
        "api_key": "",  # Anthropic APIã‚­ãƒ¼ (ç’°å¢ƒå¤‰æ•°ANTHROPIC_API_KEYã§ã‚‚å¯)
        "model": "claude-3-5-sonnet-20241022",  # claude-3-5-sonnet-20241022, claude-3-opus-20240229
        "max_tokens": 16000,  # Anthropicã®åˆ¶é™å†…æœ€å¤§
        "temperature": 0.0    # æ±ºå®šçš„ãªå‡ºåŠ›ã®ãŸã‚ï¼ˆ0.1â†’0.0ï¼‰
    }
}

print("ğŸ¤– LLMã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆè¨­å®šå®Œäº†")
print(f"ğŸ¤– LLMãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼: {LLM_CONFIG['provider']}")

if LLM_CONFIG['provider'] == 'databricks':
    print(f"ğŸ”— Databricksã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆ: {LLM_CONFIG['databricks']['endpoint_name']}")
    thinking_status = "æœ‰åŠ¹" if LLM_CONFIG['databricks'].get('thinking_enabled', False) else "ç„¡åŠ¹"
    thinking_budget = LLM_CONFIG['databricks'].get('thinking_budget_tokens', 65536)
    max_tokens = LLM_CONFIG['databricks'].get('max_tokens', 131072)
    print(f"ğŸ§  æ‹¡å¼µæ€è€ƒãƒ¢ãƒ¼ãƒ‰: {thinking_status} (äºˆç®—: {thinking_budget:,} tokens)")
    print(f"ğŸ“Š æœ€å¤§ãƒˆãƒ¼ã‚¯ãƒ³æ•°: {max_tokens:,} tokens ({max_tokens//1024}K)")
    if not LLM_CONFIG['databricks'].get('thinking_enabled', False):
        print("âš¡ é«˜é€Ÿå®Ÿè¡Œãƒ¢ãƒ¼ãƒ‰: æ€è€ƒãƒ—ãƒ­ã‚»ã‚¹ã‚’çœç•¥ã—ã¦è¿…é€Ÿãªçµæœç”Ÿæˆ")
elif LLM_CONFIG['provider'] == 'openai':
    print(f"ğŸ”— OpenAIãƒ¢ãƒ‡ãƒ«: {LLM_CONFIG['openai']['model']}")
elif LLM_CONFIG['provider'] == 'azure_openai':
    print(f"ğŸ”— Azure OpenAIãƒ‡ãƒ—ãƒ­ã‚¤ãƒ¡ãƒ³ãƒˆ: {LLM_CONFIG['azure_openai']['deployment_name']}")
elif LLM_CONFIG['provider'] == 'anthropic':
    print(f"ğŸ”— Anthropicãƒ¢ãƒ‡ãƒ«: {LLM_CONFIG['anthropic']['model']}")

print()
print("ğŸ’¡ LLMãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼åˆ‡ã‚Šæ›¿ãˆä¾‹:")
print('   LLM_CONFIG["provider"] = "openai"      # OpenAI GPT-4ã«åˆ‡ã‚Šæ›¿ãˆ')
print('   LLM_CONFIG["provider"] = "anthropic"   # Anthropic Claudeã«åˆ‡ã‚Šæ›¿ãˆ')
print('   LLM_CONFIG["provider"] = "azure_openai" # Azure OpenAIã«åˆ‡ã‚Šæ›¿ãˆ')
print()
print("ğŸ§  Databricksæ‹¡å¼µæ€è€ƒãƒ¢ãƒ¼ãƒ‰è¨­å®šä¾‹:")
print('   LLM_CONFIG["databricks"]["thinking_enabled"] = False  # æ‹¡å¼µæ€è€ƒãƒ¢ãƒ¼ãƒ‰ç„¡åŠ¹ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆãƒ»é«˜é€Ÿå®Ÿè¡Œï¼‰')
print('   LLM_CONFIG["databricks"]["thinking_enabled"] = True   # æ‹¡å¼µæ€è€ƒãƒ¢ãƒ¼ãƒ‰æœ‰åŠ¹ï¼ˆè©³ç´°åˆ†ææ™‚ã®ã¿ï¼‰')
print('   LLM_CONFIG["databricks"]["thinking_budget_tokens"] = 65536  # æ€è€ƒç”¨ãƒˆãƒ¼ã‚¯ãƒ³äºˆç®—(64K)')
print('   LLM_CONFIG["databricks"]["max_tokens"] = 131072  # æœ€å¤§ãƒˆãƒ¼ã‚¯ãƒ³æ•°(128K)')
print()

# å¿…è¦ãªãƒ©ã‚¤ãƒ–ãƒ©ãƒªã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
try:
    import requests
except ImportError:
    print("Warning: requests is not installed, some features may not work")
    requests = None
import os
try:
    from pyspark.sql import SparkSession
except ImportError:
    print("Warning: pyspark is not installed")
    SparkSession = None
    print("âœ… Spark Version: Not available")

# Databricks Runtimeæƒ…å ±ã‚’å®‰å…¨ã«å–å¾—
try:
    if spark is not None:
        runtime_version = spark.conf.get('spark.databricks.clusterUsageTags.sparkVersion')
    print(f"âœ… Databricks Runtime: {runtime_version}")
except Exception:
    try:
        # ä»£æ›¿æ‰‹æ®µã§DBRæƒ…å ±ã‚’å–å¾—
        dbr_version = spark.conf.get('spark.databricks.clusterUsageTags.clusterName', 'Unknown')
        print(f"âœ… Databricks Cluster: {dbr_version}")
    except Exception:
        print("âœ… Databricks Environment: è¨­å®šæƒ…å ±ã®å–å¾—ã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã—ãŸ")

# COMMAND ----------

# MAGIC %md
# MAGIC ## ğŸ“‚ SQLãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ©ãƒ¼JSONãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿é–¢æ•°
# MAGIC
# MAGIC ã“ã®ã‚»ãƒ«ã§ã¯ä»¥ä¸‹ã®æ©Ÿèƒ½ã‚’å®šç¾©ã—ã¾ã™ï¼š
# MAGIC - SQLãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ©ãƒ¼JSONãƒ•ã‚¡ã‚¤ãƒ«ã®èª­ã¿è¾¼ã¿
# MAGIC - DBFS/FileStore/ãƒ­ãƒ¼ã‚«ãƒ«ãƒ‘ã‚¹ã®è‡ªå‹•åˆ¤åˆ¥
# MAGIC - ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚ºã¨ãƒ‡ãƒ¼ã‚¿æƒ…å ±ã®è¡¨ç¤º

# COMMAND ----------

def load_profiler_json(file_path: str) -> Dict[str, Any]:
    """
    SQLãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ©ãƒ¼JSONãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã‚€
    
    Args:
        file_path: JSONãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹ï¼ˆDBFS ã¾ãŸã¯ ãƒ­ãƒ¼ã‚«ãƒ«ãƒ‘ã‚¹ï¼‰
        
    Returns:
        Dict: ãƒ‘ãƒ¼ã‚¹ã•ã‚ŒãŸJSONãƒ‡ãƒ¼ã‚¿
    """
    try:
        # DBFSãƒ‘ã‚¹ã®å ´åˆã¯é©åˆ‡ã«å‡¦ç†
        if file_path.startswith('/dbfs/'):
            with open(file_path, 'r', encoding='utf-8') as file:
                data = json.load(file)
        elif file_path.startswith('dbfs:/'):
            # dbfs: ãƒ—ãƒ¬ãƒ•ã‚£ãƒƒã‚¯ã‚¹ã‚’/dbfs/ã«å¤‰æ›
            local_path = file_path.replace('dbfs:', '/dbfs')
            with open(local_path, 'r', encoding='utf-8') as file:
                data = json.load(file)
        elif file_path.startswith('/FileStore/'):
            # FileStore ãƒ‘ã‚¹ã‚’ /dbfs/FileStore/ ã«å¤‰æ›
            local_path = '/dbfs' + file_path
            with open(local_path, 'r', encoding='utf-8') as file:
                data = json.load(file)
        else:
            # ãƒ­ãƒ¼ã‚«ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«
            with open(file_path, 'r', encoding='utf-8') as file:
                data = json.load(file)
        
        print(f"âœ… JSONãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ­£å¸¸ã«èª­ã¿è¾¼ã¿ã¾ã—ãŸ: {file_path}")
        print(f"ğŸ“Š ãƒ‡ãƒ¼ã‚¿ã‚µã‚¤ã‚º: {len(str(data)):,} characters")
        return data
    except Exception as e:
        print(f"âŒ ãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {str(e)}")
        return {}

print("âœ… é–¢æ•°å®šç¾©å®Œäº†: load_profiler_json")

# COMMAND ----------

# MAGIC %md
# MAGIC ## ğŸ“Š ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ¡ãƒˆãƒªã‚¯ã‚¹æŠ½å‡ºé–¢æ•°
# MAGIC
# MAGIC ã“ã®ã‚»ãƒ«ã§ã¯ä»¥ä¸‹ã®æ©Ÿèƒ½ã‚’å®šç¾©ã—ã¾ã™ï¼š
# MAGIC - SQLãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ©ãƒ¼ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰ã®ãƒ¡ãƒˆãƒªã‚¯ã‚¹æŠ½å‡º
# MAGIC - ã‚¯ã‚¨ãƒªåŸºæœ¬æƒ…å ±ã®å–å¾—
# MAGIC - å…¨ä½“/ã‚¹ãƒ†ãƒ¼ã‚¸/ãƒãƒ¼ãƒ‰åˆ¥ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æŒ‡æ¨™ã®è¨ˆç®—
# MAGIC - ã‚¹ãƒ”ãƒ«æ¤œå‡ºã¨ãƒœãƒˆãƒ«ãƒãƒƒã‚¯æŒ‡æ¨™ã®åˆ†æ

# COMMAND ----------

def detect_data_format(profiler_data: Dict[str, Any]) -> str:
    """
    JSONãƒ‡ãƒ¼ã‚¿ã®å½¢å¼ã‚’æ¤œå‡º
    """
    # SQLãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ©ãƒ¼å½¢å¼ã®æ¤œå‡º
    if 'graphs' in profiler_data and isinstance(profiler_data['graphs'], list):
        if len(profiler_data['graphs']) > 0:
            return 'sql_profiler'
    
    # SQLã‚¯ã‚¨ãƒªã‚µãƒãƒªãƒ¼å½¢å¼ã®æ¤œå‡ºï¼ˆtest2.jsonå½¢å¼ï¼‰
    if 'query' in profiler_data and 'planMetadatas' in profiler_data:
        query_data = profiler_data.get('query', {})
        if 'metrics' in query_data:
            return 'sql_query_summary'
    
    return 'unknown'

def extract_performance_metrics_from_query_summary(profiler_data: Dict[str, Any]) -> Dict[str, Any]:
    """
    Databricks SQLã‚¯ã‚¨ãƒªã‚µãƒãƒªãƒ¼å½¢å¼ã®JSONã‹ã‚‰åŸºæœ¬ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚’æŠ½å‡º
    (test2.jsonå½¢å¼ã«å¯¾å¿œ)
    """
    try:
        query_data = profiler_data.get('query', {})
        metrics_data = query_data.get('metrics', {})
        
        if not metrics_data:
            print("âš ï¸ ãƒ¡ãƒˆãƒªã‚¯ã‚¹ãƒ‡ãƒ¼ã‚¿ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
            return {}
        
        print(f"âœ… SQLã‚¯ã‚¨ãƒªã‚µãƒãƒªãƒ¼å½¢å¼ã®ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚’æ¤œå‡ºã—ã¾ã—ãŸ")
        print(f"   - å®Ÿè¡Œæ™‚é–“: {metrics_data.get('totalTimeMs', 0):,} ms")
        print(f"   - èª­ã¿è¾¼ã¿ãƒ‡ãƒ¼ã‚¿: {metrics_data.get('readBytes', 0) / 1024 / 1024 / 1024:.2f} GB")
        print(f"   - å‡¦ç†è¡Œæ•°: {metrics_data.get('rowsReadCount', 0):,} è¡Œ")
        
        # åŸºæœ¬ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã®æŠ½å‡º
        overall_metrics = {
            'total_time_ms': metrics_data.get('totalTimeMs', 0),
            'execution_time_ms': metrics_data.get('executionTimeMs', 0),
            'compilation_time_ms': metrics_data.get('compilationTimeMs', 0),
            'read_bytes': metrics_data.get('readBytes', 0),
            'read_remote_bytes': metrics_data.get('readRemoteBytes', 0),
            'read_cache_bytes': metrics_data.get('readCacheBytes', 0),
            'spill_to_disk_bytes': metrics_data.get('spillToDiskBytes', 0),
            'rows_produced_count': metrics_data.get('rowsProducedCount', 0),
            'rows_read_count': metrics_data.get('rowsReadCount', 0),
            'read_files_count': metrics_data.get('readFilesCount', 0),
            'read_partitions_count': metrics_data.get('readPartitionsCount', 0),
            'photon_total_time_ms': metrics_data.get('photonTotalTimeMs', 0),
            'task_total_time_ms': metrics_data.get('taskTotalTimeMs', 0),
            'network_sent_bytes': metrics_data.get('networkSentBytes', 0),
            'photon_enabled': metrics_data.get('photonTotalTimeMs', 0) > 0,
            'photon_utilization_ratio': 0
        }
        
        # Photonåˆ©ç”¨ç‡ã®è¨ˆç®—
        if overall_metrics['task_total_time_ms'] > 0:
            overall_metrics['photon_utilization_ratio'] = min(
                overall_metrics['photon_total_time_ms'] / overall_metrics['task_total_time_ms'], 1.0
            )
        
        # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ’ãƒƒãƒˆç‡ã®è¨ˆç®—
        cache_hit_ratio = 0
        if overall_metrics['read_bytes'] > 0:
            cache_hit_ratio = overall_metrics['read_cache_bytes'] / overall_metrics['read_bytes']
        
        # ãƒœãƒˆãƒ«ãƒãƒƒã‚¯æŒ‡æ¨™ã®è¨ˆç®—
        bottleneck_indicators = {
            'spill_bytes': overall_metrics['spill_to_disk_bytes'],
            'has_spill': overall_metrics['spill_to_disk_bytes'] > 0,
            'cache_hit_ratio': cache_hit_ratio,
            'has_cache_miss': cache_hit_ratio < 0.8,
            'photon_efficiency': overall_metrics['photon_utilization_ratio'],
            'has_shuffle_bottleneck': False,  # è©³ç´°æƒ…å ±ãŒãªã„ãŸã‚åˆ¤å®šä¸å¯
            'remote_read_ratio': 0,
            'has_memory_pressure': overall_metrics['spill_to_disk_bytes'] > 0,
            'max_task_duration_ratio': 1.0,  # ä¸æ˜
            'has_data_skew': False  # è©³ç´°æƒ…å ±ãŒãªã„ãŸã‚åˆ¤å®šä¸å¯
        }
        
        # ãƒªãƒ¢ãƒ¼ãƒˆèª­ã¿è¾¼ã¿æ¯”ç‡ã®è¨ˆç®—
        if overall_metrics['read_bytes'] > 0:
            bottleneck_indicators['remote_read_ratio'] = overall_metrics['read_remote_bytes'] / overall_metrics['read_bytes']
        
        # ã‚¯ã‚¨ãƒªæƒ…å ±ã®æŠ½å‡º
        query_info = {
            'query_id': query_data.get('id', ''),
            'query_text': query_data.get('queryText', '')[:300] + "..." if len(query_data.get('queryText', '')) > 300 else query_data.get('queryText', ''),
            'status': query_data.get('status', ''),
            'query_start_time': query_data.get('queryStartTimeMs', 0),
            'query_end_time': query_data.get('queryEndTimeMs', 0),
            'spark_ui_url': query_data.get('sparkUiUrl', ''),
            'endpoint_id': query_data.get('endpointId', ''),
            'user': query_data.get('user', {}).get('displayName', ''),
            'statement_type': query_data.get('statementType', ''),
            'plans_state': query_data.get('plansState', '')
        }
        
        # è©³ç´°ãªãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æ´å¯Ÿã‚’è¨ˆç®—ï¼ˆå¾Œã§node_metricsã¨ä¸€ç·’ã«å†è¨ˆç®—ï¼‰
        performance_insights = calculate_performance_insights_from_metrics(overall_metrics, None)
        
        # æ“¬ä¼¼çš„ãªãƒãƒ¼ãƒ‰ãƒ¡ãƒˆãƒªã‚¯ã‚¹ï¼ˆã‚µãƒãƒªãƒ¼æƒ…å ±ã‹ã‚‰ç”Ÿæˆï¼‰
        summary_node = {
            'node_id': 'summary_node',
            'name': f'Query Execution Summary ({query_data.get("statementType", "SQL")})',
            'tag': 'QUERY_SUMMARY',
            'key_metrics': {
                'durationMs': overall_metrics['total_time_ms'],
                'rowsNum': overall_metrics['rows_read_count'],
                'peakMemoryBytes': 0,  # ä¸æ˜
                'throughputMBps': performance_insights['parallelization']['throughput_mb_per_second'],
                'dataSelectivity': performance_insights['data_efficiency']['data_selectivity'],
                'cacheHitRatio': performance_insights['cache_efficiency']['cache_hit_ratio']
            },
            'detailed_metrics': {
                'Total Time': {'value': overall_metrics['total_time_ms'], 'display_name': 'Total Time'},
                'Read Bytes': {'value': overall_metrics['read_bytes'], 'display_name': 'Read Bytes'},
                'Spill Bytes': {'value': overall_metrics['spill_to_disk_bytes'], 'display_name': 'Spill to Disk'},
                'Photon Time': {'value': overall_metrics['photon_total_time_ms'], 'display_name': 'Photon Time'},
                'Rows Read': {'value': overall_metrics['rows_read_count'], 'display_name': 'Rows Read Count'},
                'Cache Hit Ratio': {'value': performance_insights['cache_efficiency']['cache_hit_ratio'], 'display_name': 'Cache Hit Ratio'},
                'Filter Rate': {'value': performance_insights['data_efficiency']['data_selectivity'], 'display_name': 'Filter Rate'},
                'Throughput': {'value': performance_insights['parallelization']['throughput_mb_per_second'], 'display_name': 'Throughput (MB/s)'}
            },
            'graph_index': 0,
            'performance_insights': performance_insights
        }
        
        # å®Œå…¨ãªmetricsã§performance_insightsã‚’å†è¨ˆç®—
        complete_metrics = {
            'overall_metrics': overall_metrics,
            'node_metrics': [summary_node]
        }
        performance_insights = calculate_performance_insights_from_metrics(overall_metrics, complete_metrics)
        
        return {
            'data_format': 'sql_query_summary',
            'query_info': query_info,
            'overall_metrics': overall_metrics,
            'bottleneck_indicators': bottleneck_indicators,
            'node_metrics': [summary_node],
            'stage_metrics': [],  # è©³ç´°ã‚¹ãƒ†ãƒ¼ã‚¸æƒ…å ±ãªã—
            'liquid_clustering_analysis': {},  # å¾Œã§è¿½åŠ 
            'raw_profiler_data': profiler_data,
            'performance_insights': performance_insights,  # è©³ç´°ãªãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æ´å¯Ÿã‚’è¿½åŠ 
            'analysis_capabilities': [
                'ãƒ¡ãƒˆãƒªã‚¯ã‚¹ãƒ™ãƒ¼ã‚¹ã®ãƒœãƒˆãƒ«ãƒãƒƒã‚¯åˆ†æï¼ˆã‚­ãƒ£ãƒƒã‚·ãƒ¥åŠ¹ç‡ã€ãƒ•ã‚£ãƒ«ã‚¿ç‡ã€PhotonåŠ¹ç‡ï¼‰',
                'ãƒªã‚½ãƒ¼ã‚¹ä½¿ç”¨çŠ¶æ³åˆ†æï¼ˆã‚¹ãƒ”ãƒ«ã€ä¸¦åˆ—åŒ–åŠ¹ç‡ã€ã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆï¼‰',
                'ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æŒ‡æ¨™è¨ˆç®—ï¼ˆãƒ•ã‚¡ã‚¤ãƒ«åŠ¹ç‡ã€ãƒ‘ãƒ¼ãƒ†ã‚£ã‚·ãƒ§ãƒ³åŠ¹ç‡ï¼‰',
                'ãƒãƒ†ãƒ³ã‚·ãƒ£ãƒ«ãƒœãƒˆãƒ«ãƒãƒƒã‚¯ç‰¹å®šï¼ˆãƒ¡ãƒˆãƒªã‚¯ã‚¹ãƒ™ãƒ¼ã‚¹ï¼‰'
            ],
            'analysis_limitations': [
                'è©³ç´°ãªå®Ÿè¡Œãƒ—ãƒ©ãƒ³æƒ…å ±ï¼ˆãƒãƒ¼ãƒ‰ã€ã‚¨ãƒƒã‚¸ï¼‰ãŒåˆ©ç”¨ã§ãã¾ã›ã‚“',
                'ã‚¹ãƒ†ãƒ¼ã‚¸åˆ¥ãƒ¡ãƒˆãƒªã‚¯ã‚¹ãŒåˆ©ç”¨ã§ãã¾ã›ã‚“', 
                'BROADCASTåˆ†æã¯åŸºæœ¬çš„ãªæ¨å®šã®ã¿å¯èƒ½',
                'Liquid Clusteringåˆ†æã¯ä¸€èˆ¬çš„ãªæ¨å¥¨ã®ã¿å¯èƒ½',
                'ãƒ‡ãƒ¼ã‚¿ã‚¹ã‚­ãƒ¥ãƒ¼æ¤œå‡ºã¯å¹³å‡å€¤ãƒ™ãƒ¼ã‚¹ã®æ¨å®šã®ã¿',
                'ã‚¯ã‚¨ãƒªæ§‹é€ ã®è©³ç´°è§£æã¯è¡Œã„ã¾ã›ã‚“ï¼ˆãƒ¡ãƒˆãƒªã‚¯ã‚¹é‡è¦–ã‚¢ãƒ—ãƒ­ãƒ¼ãƒï¼‰'
            ]
        }
        
    except Exception as e:
        print(f"âš ï¸ SQLã‚¯ã‚¨ãƒªã‚µãƒãƒªãƒ¼å½¢å¼ã®ãƒ¡ãƒˆãƒªã‚¯ã‚¹æŠ½å‡ºã§ã‚¨ãƒ©ãƒ¼: {str(e)}")
        return {}

def extract_performance_metrics(profiler_data: Dict[str, Any]) -> Dict[str, Any]:
    """
    SQLãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ©ãƒ¼ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰ãƒœãƒˆãƒ«ãƒãƒƒã‚¯åˆ†æã«å¿…è¦ãªãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚’æŠ½å‡ºï¼ˆè¤‡æ•°å½¢å¼å¯¾å¿œï¼‰
    """
    # ãƒ‡ãƒ¼ã‚¿å½¢å¼ã‚’æ¤œå‡º
    data_format = detect_data_format(profiler_data)
    
    print(f"ğŸ” æ¤œå‡ºã•ã‚ŒãŸãƒ‡ãƒ¼ã‚¿å½¢å¼: {data_format}")
    
    if data_format == 'sql_query_summary':
        print("ğŸ“Š Databricks SQLã‚¯ã‚¨ãƒªã‚µãƒãƒªãƒ¼å½¢å¼ã¨ã—ã¦å‡¦ç†ä¸­...")
        result = extract_performance_metrics_from_query_summary(profiler_data)
        if result:
            # Liquid Clusteringåˆ†æã‚’è¿½åŠ ï¼ˆåˆ¶é™ä»˜ãï¼‰
            try:
                result["liquid_clustering_analysis"] = analyze_liquid_clustering_opportunities(profiler_data, result)
            except Exception as e:
                print(f"âš ï¸ Liquid Clusteringåˆ†æã‚’ã‚¹ã‚­ãƒƒãƒ—: {str(e)}")
                result["liquid_clustering_analysis"] = {}
        return result
    elif data_format == 'sql_profiler':
        print("ğŸ“Š SQLãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ©ãƒ¼è©³ç´°å½¢å¼ã¨ã—ã¦å‡¦ç†ä¸­...")
        # æ—¢å­˜ã®SQLãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ©ãƒ¼å½¢å¼ã®å‡¦ç†ã‚’ç¶™ç¶š
        pass
    else:
        print(f"âš ï¸ æœªçŸ¥ã®ãƒ‡ãƒ¼ã‚¿å½¢å¼ã§ã™: {data_format}")
        return {}
    
    # æ—¢å­˜ã®SQLãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ©ãƒ¼å½¢å¼ã®å‡¦ç†
    metrics = {
        "query_info": {},
        "overall_metrics": {},
        "stage_metrics": [],
        "node_metrics": [],
        "bottleneck_indicators": {},
        "liquid_clustering_analysis": {},
        "raw_profiler_data": profiler_data  # ãƒ—ãƒ©ãƒ³åˆ†æã®ãŸã‚ã«ç”Ÿãƒ‡ãƒ¼ã‚¿ã‚’ä¿å­˜
    }
    
    # åŸºæœ¬çš„ãªã‚¯ã‚¨ãƒªæƒ…å ±
    if 'query' in profiler_data:
        query = profiler_data['query']
        metrics["query_info"] = {
            "query_id": query.get('id', ''),
            "status": query.get('status', ''),
            "query_start_time": query.get('queryStartTimeMs', 0),
            "query_end_time": query.get('queryEndTimeMs', 0),
            "user": query.get('user', {}).get('displayName', ''),
            "query_text": query.get('queryText', '')[:300] + "..." if len(query.get('queryText', '')) > 300 else query.get('queryText', '')
        }
        
        # å…¨ä½“çš„ãªãƒ¡ãƒˆãƒªã‚¯ã‚¹
        if 'metrics' in query:
            query_metrics = query['metrics']
            metrics["overall_metrics"] = {
                "total_time_ms": query_metrics.get('totalTimeMs', 0),
                "compilation_time_ms": query_metrics.get('compilationTimeMs', 0),
                "execution_time_ms": query_metrics.get('executionTimeMs', 0),
                "read_bytes": query_metrics.get('readBytes', 0),
                "read_remote_bytes": query_metrics.get('readRemoteBytes', 0),
                "read_cache_bytes": query_metrics.get('readCacheBytes', 0),
                "rows_produced_count": query_metrics.get('rowsProducedCount', 0),
                "rows_read_count": query_metrics.get('rowsReadCount', 0),
                "spill_to_disk_bytes": query_metrics.get('spillToDiskBytes', 0),
                "read_files_count": query_metrics.get('readFilesCount', 0),
                "task_total_time_ms": query_metrics.get('taskTotalTimeMs', 0),
                "photon_total_time_ms": query_metrics.get('photonTotalTimeMs', 0),
                # Photonåˆ©ç”¨çŠ¶æ³ã®åˆ†æï¼ˆPhotonå®Ÿè¡Œæ™‚é–“/ã‚¿ã‚¹ã‚¯åˆè¨ˆæ™‚é–“ï¼‰
                "photon_enabled": query_metrics.get('photonTotalTimeMs', 0) > 0,
                "photon_utilization_ratio": min(query_metrics.get('photonTotalTimeMs', 0) / max(query_metrics.get('taskTotalTimeMs', 1), 1), 1.0)
            }
    
    # ã‚°ãƒ©ãƒ•ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰ã‚¹ãƒ†ãƒ¼ã‚¸ã¨ãƒãƒ¼ãƒ‰ã®ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚’æŠ½å‡ºï¼ˆè¤‡æ•°ã‚°ãƒ©ãƒ•å¯¾å¿œï¼‰
    if 'graphs' in profiler_data and profiler_data['graphs']:
        # ã™ã¹ã¦ã®ã‚°ãƒ©ãƒ•ã‚’åˆ†æ
        for graph_index, graph in enumerate(profiler_data['graphs']):
            print(f"ğŸ” ã‚°ãƒ©ãƒ•{graph_index}ã‚’åˆ†æä¸­...")
            
            # ã‚¹ãƒ†ãƒ¼ã‚¸ãƒ‡ãƒ¼ã‚¿
            if 'stageData' in graph:
                for stage in graph['stageData']:
                    stage_metric = {
                        "stage_id": stage.get('stageId', ''),
                        "status": stage.get('status', ''),
                        "duration_ms": stage.get('keyMetrics', {}).get('durationMs', 0),
                        "num_tasks": stage.get('numTasks', 0),
                        "num_failed_tasks": stage.get('numFailedTasks', 0),
                        "num_complete_tasks": stage.get('numCompleteTasks', 0),
                        "start_time_ms": stage.get('startTimeMs', 0),
                        "end_time_ms": stage.get('endTimeMs', 0),
                        "graph_index": graph_index  # ã©ã®ã‚°ãƒ©ãƒ•ç”±æ¥ã‹ã‚’è¨˜éŒ²
                    }
                    metrics["stage_metrics"].append(stage_metric)
            
            # ãƒãƒ¼ãƒ‰ãƒ‡ãƒ¼ã‚¿ï¼ˆé‡è¦ãªã‚‚ã®ã®ã¿ï¼‰
            if 'nodes' in graph:
                for node in graph['nodes']:
                    if not node.get('hidden', False):
                        # keyMetricsã‚’ãã®ã¾ã¾ä½¿ç”¨ï¼ˆdurationMsã¯æ—¢ã«ãƒŸãƒªç§’å˜ä½ï¼‰
                        key_metrics = node.get('keyMetrics', {})
                        
                        node_metric = {
                            "node_id": node.get('id', ''),
                            "name": node.get('name', ''),
                            "tag": node.get('tag', ''),
                            "key_metrics": key_metrics,  # å˜ä½å¤‰æ›æ¸ˆã¿ã®key_metrics
                            "metrics": node.get('metrics', []),  # å…ƒã®metricsé…åˆ—ã‚’ä¿æŒ
                            "metadata": node.get('metadata', []),  # metadataã‚’è¿½åŠ 
                            "graph_index": graph_index  # ã©ã®ã‚°ãƒ©ãƒ•ç”±æ¥ã‹ã‚’è¨˜éŒ²
                        }
                        
                        # é‡è¦ãªãƒ¡ãƒˆãƒªã‚¯ã‚¹ã®ã¿è©³ç´°æŠ½å‡ºï¼ˆã‚¹ãƒ”ãƒ«é–¢é€£ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰è¿½åŠ ãƒ»labelå¯¾å¿œï¼‰
                        detailed_metrics = {}
                        for metric in node.get('metrics', []):
                            metric_key = metric.get('key', '')
                            metric_label = metric.get('label', '')
                            
                            # ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’keyã¨labelã®ä¸¡æ–¹ã§ç¢ºèª
                            key_keywords = ['TIME', 'MEMORY', 'ROWS', 'BYTES', 'DURATION', 'PEAK', 'CUMULATIVE', 'EXCLUSIVE', 
                                           'SPILL', 'DISK', 'PRESSURE', 'SINK']
                            
                            # metric_keyã¾ãŸã¯metric_labelã«é‡è¦ãªã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãŒå«ã¾ã‚Œã‚‹å ´åˆã«æŠ½å‡º
                            is_important_metric = (
                                any(keyword in metric_key.upper() for keyword in key_keywords) or
                                any(keyword in metric_label.upper() for keyword in key_keywords)
                            )
                            
                            if is_important_metric:
                                # ãƒ¡ãƒˆãƒªã‚¯ã‚¹åã¨ã—ã¦ã€labelãŒæœ‰åŠ¹ãªå ´åˆã¯labelã‚’ä½¿ç”¨ã€ãã†ã§ãªã‘ã‚Œã°keyã‚’ä½¿ç”¨
                                metric_name = metric_label if metric_label and metric_label != 'UNKNOWN_KEY' else metric_key
                                detailed_metrics[metric_name] = {
                                    'value': metric.get('value', 0),
                                    'label': metric_label,
                                    'type': metric.get('metricType', ''),
                                    'original_key': metric_key,  # å…ƒã®ã‚­ãƒ¼åã‚’ä¿å­˜
                                    'display_name': metric_name  # è¡¨ç¤ºç”¨ã®åå‰
                                }
                        node_metric['detailed_metrics'] = detailed_metrics
                        metrics["node_metrics"].append(node_metric)
    
    # ãƒœãƒˆãƒ«ãƒãƒƒã‚¯æŒ‡æ¨™ã®è¨ˆç®—
    metrics["bottleneck_indicators"] = calculate_bottleneck_indicators(metrics)
    
    # Liquid Clusteringåˆ†æ
    metrics["liquid_clustering_analysis"] = analyze_liquid_clustering_opportunities(profiler_data, metrics)
    
    return metrics

print("âœ… é–¢æ•°å®šç¾©å®Œäº†: extract_performance_metrics")

# COMMAND ----------

# MAGIC %md
# MAGIC ## ğŸ·ï¸ ãƒãƒ¼ãƒ‰åè§£æãƒ»æ”¹å–„é–¢æ•°
# MAGIC
# MAGIC ã“ã®ã‚»ãƒ«ã§ã¯ä»¥ä¸‹ã®æ©Ÿèƒ½ã‚’å®šç¾©ã—ã¾ã™ï¼š
# MAGIC - æ±ç”¨çš„ãªãƒãƒ¼ãƒ‰åï¼ˆWhole Stage Codegenç­‰ï¼‰ã®å…·ä½“åŒ–
# MAGIC - é–¢é€£ãƒãƒ¼ãƒ‰ã®æ¤œç´¢ã¨æœ€é©ãªå‡¦ç†åã®é¸æŠ
# MAGIC - Photonæƒ…å ±ã‚„ãƒ†ãƒ¼ãƒ–ãƒ«æƒ…å ±ã®ä»˜åŠ 
# MAGIC - å‡¦ç†åã®æ„å‘³çš„ãªæ”¹å–„

# COMMAND ----------

def get_meaningful_node_name(node: Dict[str, Any], extracted_metrics: Dict[str, Any]) -> str:
    """
    ã‚ˆã‚Šæ„å‘³ã®ã‚ã‚‹ãƒãƒ¼ãƒ‰åã‚’å–å¾—ã™ã‚‹é–¢æ•°
    æ±ç”¨çš„ãªåå‰ï¼ˆWhole Stage Codegenãªã©ï¼‰ã‚’å…·ä½“çš„ãªå‡¦ç†åã«å¤‰æ›
    """
    original_name = node.get('name', '')
    node_id = node.get('node_id', node.get('id', ''))
    node_tag = node.get('tag', '')
    
    # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰è©³ç´°æƒ…å ±ã‚’å–å¾—
    metadata = node.get('metadata', [])
    metadata_info = {}
    for meta in metadata:
        key = meta.get('key', '')
        value = meta.get('value', '')
        label = meta.get('label', '')
        if value:
            metadata_info[key] = value
    
    # 1. æ±ç”¨çš„ãªåå‰ã‚’å…·ä½“çš„ãªåå‰ã«ç½®ãæ›ãˆ
    if 'whole stage codegen' in original_name.lower():
        # ã‚ˆã‚Šå…·ä½“çš„ãªå‡¦ç†åã‚’æ¨æ¸¬ã™ã‚‹ãŸã‚ã®ãƒ’ãƒ¥ãƒ¼ãƒªã‚¹ãƒ†ã‚£ãƒƒã‚¯
        
        # ãƒãƒ¼ãƒ‰IDãƒ™ãƒ¼ã‚¹ã§ã®é–¢é€£æ€§ã‚’æ¨æ¸¬ï¼ˆéš£æ¥IDï¼‰
        node_id_num = None
        try:
            node_id_num = int(node_id) if node_id else None
        except:
            pass
        
        if node_id_num:
            # åŒã˜ãƒ•ã‚¡ã‚¤ãƒ«å†…ã®è¿‘ã„IDã®å…·ä½“çš„ãªå‡¦ç†ã‚’æ¢ã™
            all_nodes = extracted_metrics.get('node_metrics', [])
            nearby_specific_nodes = []
            
            for other_node in all_nodes:
                other_id = other_node.get('node_id', '')
                other_name = other_node.get('name', '')
                
                try:
                    other_id_num = int(other_id) if other_id else None
                    if other_id_num and abs(other_id_num - node_id_num) <= 10:  # è¿‘éš£10å€‹ä»¥å†…
                        if is_specific_process_name(other_name):
                            nearby_specific_nodes.append(other_name)
                except:
                    continue
            
            # æœ€ã‚‚å…·ä½“çš„ãªå‡¦ç†åã‚’é¸æŠ
            if nearby_specific_nodes:
                specific_name = get_most_specific_process_name_from_list(nearby_specific_nodes)
                if specific_name and specific_name != original_name:
                    return f"{specific_name} (Whole Stage Codegen)"
        
        # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: tagã‹ã‚‰ã‚ˆã‚Šå…·ä½“çš„ãªæƒ…å ±ã‚’æŠ½å‡º
        if 'CODEGEN' in node_tag:
            # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰å­ã‚¿ã‚°æƒ…å ±ã‚’ç¢ºèª
            child_tag = metadata_info.get('CHILD_TAG', '')
            if child_tag and child_tag != 'Child':
                return f"Whole Stage Codegen ({child_tag})"
    
    # 2. ã‚ˆã‚Šå…·ä½“çš„ãªã‚¿ã‚°æƒ…å ±ã‚’ãƒãƒ¼ãƒ‰åã«åæ˜ 
    tag_to_name_mapping = {
        'PHOTON_SHUFFLE_EXCHANGE_SINK_EXEC': 'Photon Shuffle Exchange',
        'PHOTON_GROUPING_AGG_EXEC': 'Photon Grouping Aggregate', 
        'UNKNOWN_DATA_SOURCE_SCAN_EXEC': 'Data Source Scan',
        'HASH_AGGREGATE_EXEC': 'Hash Aggregate',
        'WHOLE_STAGE_CODEGEN_EXEC': 'Whole Stage Codegen'
    }
    
    if node_tag in tag_to_name_mapping:
        mapped_name = tag_to_name_mapping[node_tag]
        if mapped_name != original_name and mapped_name != 'Whole Stage Codegen':
            # ã‚¿ã‚°ã®æ–¹ãŒã‚ˆã‚Šå…·ä½“çš„ãªå ´åˆã¯ä½¿ç”¨
            enhanced_name = mapped_name
        else:
            enhanced_name = original_name
    else:
        enhanced_name = original_name
    
    # 3. ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰å‡¦ç†ã®è©³ç´°ã‚’è¿½åŠ 
    
    # ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ãƒ»ãƒ†ãƒ¼ãƒ–ãƒ«æƒ…å ±ã‚’è¿½åŠ ï¼ˆå¼·åŒ–ç‰ˆï¼‰
    table_name = None
    
    # è¤‡æ•°ã®ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚­ãƒ¼ã‹ã‚‰ãƒ†ãƒ¼ãƒ–ãƒ«åã‚’æŠ½å‡ºï¼ˆãƒ•ãƒ«ãƒ‘ã‚¹å„ªå…ˆï¼‰
    for key_candidate in ['SCAN_TABLE', 'SCAN_IDENTIFIER', 'TABLE_NAME', 'RELATION', 'SCAN_RELATION']:
        if key_candidate in metadata_info:
            extracted_table = metadata_info[key_candidate]
            # ãƒ•ãƒ«ãƒ‘ã‚¹ï¼ˆcatalog.schema.tableï¼‰ã®å ´åˆã¯ãã®ã¾ã¾ä½¿ç”¨
            if isinstance(extracted_table, str) and extracted_table.count('.') >= 2:
                table_name = extracted_table
                break
            elif isinstance(extracted_table, str) and extracted_table.count('.') == 1:
                # schema.tableå½¢å¼ã®å ´åˆã‚‚ãã®ã¾ã¾ä½¿ç”¨
                table_name = extracted_table
                break
            elif not table_name:  # ã¾ã ãƒ†ãƒ¼ãƒ–ãƒ«åãŒè¦‹ã¤ã‹ã£ã¦ã„ãªã„å ´åˆã®ã¿è¨­å®š
                table_name = extracted_table
    
    # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰ãƒ†ãƒ¼ãƒ–ãƒ«åã‚’æŠ½å‡ºã§ããªã„å ´åˆã€ãƒãƒ¼ãƒ‰åã‹ã‚‰æ¨æ¸¬
    if not table_name and ('scan' in enhanced_name.lower() or 'data source' in enhanced_name.lower()):
        # ãƒãƒ¼ãƒ‰åã‹ã‚‰ãƒ†ãƒ¼ãƒ–ãƒ«åã‚’æ¨æ¸¬
        import re
        
        # "Scan tpcds.tpcds_sf1000_delta_lc.customer" ã®ã‚ˆã†ãªå½¢å¼
        table_patterns = [
            r'[Ss]can\s+([a-zA-Z_][a-zA-Z0-9_.]*[a-zA-Z0-9_])',
            r'[Tt]able\s+([a-zA-Z_][a-zA-Z0-9_.]*[a-zA-Z0-9_])',
            r'([a-zA-Z_][a-zA-Z0-9_]*\.)+([a-zA-Z_][a-zA-Z0-9_]*)',
        ]
        
        for pattern in table_patterns:
            match = re.search(pattern, original_name)
            if match:
                if '.' in match.group(0):
                    # ãƒ•ãƒ«ãƒ†ãƒ¼ãƒ–ãƒ«åï¼ˆcatalog.schema.tableï¼‰ã®å ´åˆã¯ãƒ•ãƒ«ãƒ‘ã‚¹ã‚’ä½¿ç”¨
                    table_name = match.group(0)
                else:
                    table_name = match.group(1) if match.lastindex and match.lastindex >= 1 else match.group(0)
                break
    
    # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã®valuesãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã‹ã‚‰ã‚‚ãƒ†ãƒ¼ãƒ–ãƒ«åã‚’æ¤œç´¢
    if not table_name:
        for meta in metadata:
            values = meta.get('values', [])
            if values:
                for value in values:
                    if isinstance(value, str) and '.' in value and len(value.split('.')) >= 2:
                        # "catalog.schema.table" å½¢å¼ã®å ´åˆ
                        parts = value.split('.')
                        if len(parts) >= 2 and not any(part.isdigit() for part in parts[-2:]):
                            # ãƒ•ãƒ«ãƒ‘ã‚¹ã‚’ä½¿ç”¨ï¼ˆcatalog.schema.tableï¼‰
                            if len(parts) >= 3:
                                table_name = '.'.join(parts)  # ãƒ•ãƒ«ãƒ‘ã‚¹
                            else:
                                table_name = value  # ãã®ã¾ã¾ä½¿ç”¨
                            break
                if table_name:
                    break
    
    # Data Source Scanã®å ´åˆã«ãƒ†ãƒ¼ãƒ–ãƒ«åã‚’è¡¨ç¤º
    if table_name and ('scan' in enhanced_name.lower() or 'data source' in enhanced_name.lower()):
        # ãƒ•ãƒ«ãƒ‘ã‚¹è¡¨ç¤ºã®ãŸã‚ã«åˆ¶é™ã‚’ç·©å’Œï¼ˆ60æ–‡å­—ã¾ã§ï¼‰
        if len(table_name) > 60:
            # ã‚«ã‚¿ãƒ­ã‚°.ã‚¹ã‚­ãƒ¼ãƒ.ãƒ†ãƒ¼ãƒ–ãƒ«å½¢å¼ã®å ´åˆã¯ä¸­é–“ã‚’çœç•¥
            parts = table_name.split('.')
            if len(parts) >= 3:
                table_name = f"{parts[0]}.*.{parts[-1]}"
            else:
                table_name = table_name[:57] + "..."
        enhanced_name = f"Data Source Scan ({table_name})"
    elif 'scan' in enhanced_name.lower() and 'data source' in enhanced_name.lower():
        # ãƒ†ãƒ¼ãƒ–ãƒ«åãŒè¦‹ã¤ã‹ã‚‰ãªã„å ´åˆã§ã‚‚ã€ã‚ˆã‚Šæ˜ç¢ºãªåå‰ã«
        enhanced_name = "Data Source Scan"
    
    # Photonæƒ…å ±ã‚’è¿½åŠ 
    if 'IS_PHOTON' in metadata_info and metadata_info['IS_PHOTON'] == 'true':
        if not enhanced_name.startswith('Photon'):
            enhanced_name = f"Photon {enhanced_name}"
    
    return enhanced_name

def find_related_specific_nodes(target_node_id: str, nodes: list, edges: list) -> list:
    """æŒ‡å®šãƒãƒ¼ãƒ‰ã«é–¢é€£ã™ã‚‹å…·ä½“çš„ãªå‡¦ç†ãƒãƒ¼ãƒ‰ã‚’æ¤œç´¢"""
    
    # ã‚¨ãƒƒã‚¸ã‹ã‚‰é–¢é€£ãƒãƒ¼ãƒ‰ã‚’ç‰¹å®š
    related_node_ids = set()
    
    # ç›´æ¥æ¥ç¶šã•ã‚Œã¦ã„ã‚‹ãƒãƒ¼ãƒ‰
    for edge in edges:
        from_id = edge.get('fromId', '')
        to_id = edge.get('toId', '')
        
        if from_id == target_node_id:
            related_node_ids.add(to_id)
        elif to_id == target_node_id:
            related_node_ids.add(from_id)
    
    # é–¢é€£ãƒãƒ¼ãƒ‰ã®è©³ç´°ã‚’å–å¾—
    related_nodes = []
    for node in nodes:
        node_id = node.get('id', '')
        if node_id in related_node_ids:
            node_name = node.get('name', '')
            # å…·ä½“çš„ãªå‡¦ç†åã‚’æŒã¤ãƒãƒ¼ãƒ‰ã®ã¿é¸æŠ
            if is_specific_process_name(node_name):
                related_nodes.append(node)
    
    return related_nodes

def is_specific_process_name(name: str) -> bool:
    """å…·ä½“çš„ãªå‡¦ç†åã‹ã©ã†ã‹ã‚’åˆ¤å®š"""
    specific_keywords = [
        'columnar to row', 'row to columnar', 'filter', 'project', 'join',
        'aggregate', 'sort', 'exchange', 'broadcast', 'scan', 'union'
    ]
    
    generic_keywords = [
        'whole stage codegen', 'stage', 'query', 'result'
    ]
    
    name_lower = name.lower()
    
    # å…·ä½“çš„ãªã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’å«ã‚€å ´åˆ
    for keyword in specific_keywords:
        if keyword in name_lower:
            return True
    
    # æ±ç”¨çš„ãªã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã®ã¿ã®å ´åˆã¯é™¤å¤–
    for keyword in generic_keywords:
        if keyword in name_lower and len(name_lower.split()) <= 3:
            return False
    
    return True

def get_most_specific_process_name(nodes: list) -> str:
    """æœ€ã‚‚å…·ä½“çš„ãªå‡¦ç†åã‚’é¸æŠ"""
    if not nodes:
        return ""
    
    # å„ªå…ˆé †ä½: ã‚ˆã‚Šå…·ä½“çš„ã§æ„å‘³ã®ã‚ã‚‹å‡¦ç†å
    priority_keywords = [
        'columnar to row', 'row to columnar', 'filter', 'project',
        'hash join', 'broadcast join', 'sort merge join',
        'hash aggregate', 'sort aggregate', 'grouping aggregate'
    ]
    
    for keyword in priority_keywords:
        for node in nodes:
            node_name = node.get('name', '').lower()
            if keyword in node_name:
                return node.get('name', '')
    
    # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: æœ€åˆã®å…·ä½“çš„ãªãƒãƒ¼ãƒ‰å
    for node in nodes:
        node_name = node.get('name', '')
        if is_specific_process_name(node_name):
            return node_name
    
    return ""

def get_most_specific_process_name_from_list(node_names: list) -> str:
    """ãƒãƒ¼ãƒ‰åã®ãƒªã‚¹ãƒˆã‹ã‚‰æœ€ã‚‚å…·ä½“çš„ãªå‡¦ç†åã‚’é¸æŠ"""
    if not node_names:
        return ""
    
    # å„ªå…ˆé †ä½: ã‚ˆã‚Šå…·ä½“çš„ã§æ„å‘³ã®ã‚ã‚‹å‡¦ç†å
    priority_keywords = [
        'columnar to row', 'row to columnar', 'filter', 'project',
        'hash join', 'broadcast join', 'sort merge join',
        'hash aggregate', 'sort aggregate', 'grouping aggregate'
    ]
    
    for keyword in priority_keywords:
        for name in node_names:
            if keyword in name.lower():
                return name
    
    # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: æœ€åˆã®å…·ä½“çš„ãªãƒãƒ¼ãƒ‰å
    for name in node_names:
        if is_specific_process_name(name):
            return name
    
    return ""

def extract_shuffle_attributes(node: Dict[str, Any]) -> list:
    """
    Shuffleãƒãƒ¼ãƒ‰ã‹ã‚‰SHUFFLE_ATTRIBUTESã‚’æŠ½å‡º
    
    Args:
        node: ãƒãƒ¼ãƒ‰æƒ…å ±
        
    Returns:
        list: æ¤œå‡ºã•ã‚ŒãŸShuffle attributes
    """
    shuffle_attributes = []
    
    # metadataã‹ã‚‰SHUFFLE_ATTRIBUTESã‚’æ¤œç´¢
    metadata = node.get('metadata', [])
    if isinstance(metadata, list):
        for item in metadata:
            if isinstance(item, dict):
                item_key = item.get('key', '')
                item_label = item.get('label', '')
                item_values = item.get('values', [])
                
                # keyã¨labelã®ä¸¡æ–¹ã‚’ãƒã‚§ãƒƒã‚¯
                if (item_key == 'SHUFFLE_ATTRIBUTES' or 
                    item_label == 'Shuffle attributes'):
                    if isinstance(item_values, list):
                        shuffle_attributes.extend(item_values)
    
    # raw_metricsã‹ã‚‰ã‚‚æ¤œç´¢ï¼ˆlabelã‚‚ãƒã‚§ãƒƒã‚¯ï¼‰
    raw_metrics = node.get('metrics', [])
    if isinstance(raw_metrics, list):
        for metric in raw_metrics:
            if isinstance(metric, dict):
                metric_key = metric.get('key', '')
                metric_label = metric.get('label', '')
                metric_values = metric.get('values', [])
                
                if (metric_key == 'SHUFFLE_ATTRIBUTES' or 
                    metric_label == 'Shuffle attributes'):
                    if isinstance(metric_values, list):
                        shuffle_attributes.extend(metric_values)
    
    # detailed_metricsã‹ã‚‰ã‚‚æ¤œç´¢
    detailed_metrics = node.get('detailed_metrics', {})
    if isinstance(detailed_metrics, dict):
        for key, info in detailed_metrics.items():
            if (key == 'SHUFFLE_ATTRIBUTES' or 
                (isinstance(info, dict) and info.get('label') == 'Shuffle attributes')):
                values = info.get('values', []) if isinstance(info, dict) else []
                if isinstance(values, list):
                    shuffle_attributes.extend(values)
    
    # é‡è¤‡ã‚’å‰Šé™¤
    return list(set(shuffle_attributes))

def extract_cluster_attributes(node: Dict[str, Any]) -> list:
    """
    ã‚¹ã‚­ãƒ£ãƒ³ãƒãƒ¼ãƒ‰ã‹ã‚‰ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°ã‚­ãƒ¼(SCAN_CLUSTERS)ã‚’æŠ½å‡º
    
    Args:
        node: ãƒãƒ¼ãƒ‰æƒ…å ±
        
    Returns:
        list: æ¤œå‡ºã•ã‚ŒãŸã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°ã‚­ãƒ¼
    """
    cluster_attributes = []
    
    # metadataã‹ã‚‰SCAN_CLUSTERSã‚’æ¤œç´¢
    metadata = node.get('metadata', [])
    if isinstance(metadata, list):
        for item in metadata:
            if isinstance(item, dict):
                item_key = item.get('key', '')
                item_label = item.get('label', '')
                item_values = item.get('values', [])
                
                # keyã¨labelã®ä¸¡æ–¹ã‚’ãƒã‚§ãƒƒã‚¯
                if (item_key == 'SCAN_CLUSTERS' or 
                    item_label == 'Cluster attributes'):
                    if isinstance(item_values, list):
                        cluster_attributes.extend(item_values)
    
    # raw_metricsã‹ã‚‰ã‚‚æ¤œç´¢ï¼ˆlabelã‚‚ãƒã‚§ãƒƒã‚¯ï¼‰
    raw_metrics = node.get('metrics', [])
    if isinstance(raw_metrics, list):
        for metric in raw_metrics:
            if isinstance(metric, dict):
                metric_key = metric.get('key', '')
                metric_label = metric.get('label', '')
                metric_values = metric.get('values', [])
                
                if (metric_key == 'SCAN_CLUSTERS' or 
                    metric_label == 'Cluster attributes'):
                    if isinstance(metric_values, list):
                        cluster_attributes.extend(metric_values)
    
    # detailed_metricsã‹ã‚‰ã‚‚æ¤œç´¢
    detailed_metrics = node.get('detailed_metrics', {})
    if isinstance(detailed_metrics, dict):
        for key, info in detailed_metrics.items():
            if (key == 'SCAN_CLUSTERS' or 
                (isinstance(info, dict) and info.get('label') == 'Cluster attributes')):
                values = info.get('values', []) if isinstance(info, dict) else []
                if isinstance(values, list):
                    cluster_attributes.extend(values)
    
    # é‡è¤‡ã‚’å‰Šé™¤
    return list(set(cluster_attributes))

def extract_parallelism_metrics(node: Dict[str, Any]) -> Dict[str, Any]:
    """
    ãƒãƒ¼ãƒ‰ã‹ã‚‰è¤‡æ•°ã®Tasks totalãƒ¡ãƒˆãƒªã‚¯ã‚¹ã¨AQEShuffleReadãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚’æŠ½å‡º
    
    ã‚·ãƒ£ãƒƒãƒ•ãƒ«æ“ä½œãªã©ã§ã¯ä»¥ä¸‹ã®è¤‡æ•°ã®ãƒ¡ãƒˆãƒªã‚¯ã‚¹ãŒå­˜åœ¨ã™ã‚‹å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ï¼š
    - Tasks total
    - Sink - Tasks total
    - Source - Tasks total
    - AQEShuffleRead - Number of partitions
    - AQEShuffleRead - Partition data size
    
    Args:
        node: ãƒãƒ¼ãƒ‰æƒ…å ±
        
    Returns:
        dict: æ¤œå‡ºã•ã‚ŒãŸãƒ¡ãƒˆãƒªã‚¯ã‚¹
            {
                "tasks_total": å€¤,
                "sink_tasks_total": å€¤,
                "source_tasks_total": å€¤,
                "all_tasks_metrics": [{"name": "Tasks total", "value": å€¤}, ...],
                "aqe_shuffle_partitions": å€¤,
                "aqe_shuffle_data_size": å€¤,
                "aqe_shuffle_avg_partition_size": å€¤,
                "aqe_shuffle_skew_warning": bool,
                "aqe_shuffle_metrics": [{"name": "AQE...", "value": å€¤}, ...]
            }
    """
    parallelism_metrics = {
        "tasks_total": 0,
        "sink_tasks_total": 0,
        "source_tasks_total": 0,
        "all_tasks_metrics": [],
        "aqe_shuffle_partitions": 0,
        "aqe_shuffle_data_size": 0,
        "aqe_shuffle_avg_partition_size": 0,
        "aqe_shuffle_skew_warning": False,
        "aqe_detected_and_handled": False,
        "aqe_shuffle_metrics": []
    }
    
    # å¯¾è±¡ã¨ãªã‚‹Tasks totalãƒ¡ãƒˆãƒªã‚¯ã‚¹åã®ãƒ‘ã‚¿ãƒ¼ãƒ³
    tasks_total_patterns = [
        "Tasks total",
        "Sink - Tasks total",
        "Source - Tasks total"
    ]
    
    # å¯¾è±¡ã¨ãªã‚‹AQEShuffleReadãƒ¡ãƒˆãƒªã‚¯ã‚¹åã®ãƒ‘ã‚¿ãƒ¼ãƒ³
    aqe_shuffle_patterns = [
        "AQEShuffleRead - Number of partitions",
        "AQEShuffleRead - Partition data size"
    ]
    
    # 1. detailed_metricsã‹ã‚‰æ¤œç´¢
    detailed_metrics = node.get('detailed_metrics', {})
    for metric_key, metric_info in detailed_metrics.items():
        metric_value = metric_info.get('value', 0)
        metric_label = metric_info.get('label', '')
        
        # å„Tasks totalãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’ãƒã‚§ãƒƒã‚¯
        for pattern in tasks_total_patterns:
            if metric_key == pattern or metric_label == pattern:
                # ç‰¹å®šã®ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã«ãƒãƒƒãƒ”ãƒ³ã‚°
                if pattern == "Tasks total":
                    parallelism_metrics["tasks_total"] = metric_value
                elif pattern == "Sink - Tasks total":
                    parallelism_metrics["sink_tasks_total"] = metric_value
                elif pattern == "Source - Tasks total":
                    parallelism_metrics["source_tasks_total"] = metric_value
                
                # å…¨ãƒ¡ãƒˆãƒªã‚¯ã‚¹ãƒªã‚¹ãƒˆã«è¿½åŠ 
                parallelism_metrics["all_tasks_metrics"].append({
                    "name": pattern,
                    "value": metric_value
                })
        
        # AQEShuffleReadãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚’ãƒã‚§ãƒƒã‚¯
        for pattern in aqe_shuffle_patterns:
            if metric_key == pattern or metric_label == pattern:
                # ç‰¹å®šã®ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã«ãƒãƒƒãƒ”ãƒ³ã‚°
                if pattern == "AQEShuffleRead - Number of partitions":
                    parallelism_metrics["aqe_shuffle_partitions"] = metric_value
                elif pattern == "AQEShuffleRead - Partition data size":
                    parallelism_metrics["aqe_shuffle_data_size"] = metric_value
                
                # å…¨ãƒ¡ãƒˆãƒªã‚¯ã‚¹ãƒªã‚¹ãƒˆã«è¿½åŠ 
                parallelism_metrics["aqe_shuffle_metrics"].append({
                    "name": pattern,
                    "value": metric_value
                })
    
    # 2. raw_metricsã‹ã‚‰æ¤œç´¢ï¼ˆãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼‰
    raw_metrics = node.get('metrics', [])
    if isinstance(raw_metrics, list):
        for metric in raw_metrics:
            if isinstance(metric, dict):
                metric_key = metric.get('key', '')
                metric_label = metric.get('label', '')
                metric_value = metric.get('value', 0)
                
                # å„Tasks totalãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’ãƒã‚§ãƒƒã‚¯
                for pattern in tasks_total_patterns:
                    if metric_key == pattern or metric_label == pattern:
                        # æ—¢ã«detailed_metricsã§è¦‹ã¤ã‹ã£ã¦ã„ã‚‹å ´åˆã¯ã‚¹ã‚­ãƒƒãƒ—
                        if not any(m["name"] == pattern for m in parallelism_metrics["all_tasks_metrics"]):
                            # ç‰¹å®šã®ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã«ãƒãƒƒãƒ”ãƒ³ã‚°
                            if pattern == "Tasks total":
                                parallelism_metrics["tasks_total"] = metric_value
                            elif pattern == "Sink - Tasks total":
                                parallelism_metrics["sink_tasks_total"] = metric_value
                            elif pattern == "Source - Tasks total":
                                parallelism_metrics["source_tasks_total"] = metric_value
                            
                            # å…¨ãƒ¡ãƒˆãƒªã‚¯ã‚¹ãƒªã‚¹ãƒˆã«è¿½åŠ 
                            parallelism_metrics["all_tasks_metrics"].append({
                                "name": pattern,
                                "value": metric_value
                            })
                
                # AQEShuffleReadãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚’ãƒã‚§ãƒƒã‚¯
                for pattern in aqe_shuffle_patterns:
                    if metric_key == pattern or metric_label == pattern:
                        # æ—¢ã«detailed_metricsã§è¦‹ã¤ã‹ã£ã¦ã„ã‚‹å ´åˆã¯ã‚¹ã‚­ãƒƒãƒ—
                        if not any(m["name"] == pattern for m in parallelism_metrics["aqe_shuffle_metrics"]):
                            # ç‰¹å®šã®ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã«ãƒãƒƒãƒ”ãƒ³ã‚°
                            if pattern == "AQEShuffleRead - Number of partitions":
                                parallelism_metrics["aqe_shuffle_partitions"] = metric_value
                            elif pattern == "AQEShuffleRead - Partition data size":
                                parallelism_metrics["aqe_shuffle_data_size"] = metric_value
                            
                            # å…¨ãƒ¡ãƒˆãƒªã‚¯ã‚¹ãƒªã‚¹ãƒˆã«è¿½åŠ 
                            parallelism_metrics["aqe_shuffle_metrics"].append({
                                "name": pattern,
                                "value": metric_value
                            })
    
    # 3. key_metricsã‹ã‚‰æ¤œç´¢ï¼ˆæœ€å¾Œã®ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼‰
    key_metrics = node.get('key_metrics', {})
    if isinstance(key_metrics, dict):
        for metric_key, metric_value in key_metrics.items():
            for pattern in tasks_total_patterns:
                if metric_key == pattern:
                    # æ—¢ã«è¦‹ã¤ã‹ã£ã¦ã„ã‚‹å ´åˆã¯ã‚¹ã‚­ãƒƒãƒ—
                    if not any(m["name"] == pattern for m in parallelism_metrics["all_tasks_metrics"]):
                        # ç‰¹å®šã®ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã«ãƒãƒƒãƒ”ãƒ³ã‚°
                        if pattern == "Tasks total":
                            parallelism_metrics["tasks_total"] = metric_value
                        elif pattern == "Sink - Tasks total":
                            parallelism_metrics["sink_tasks_total"] = metric_value
                        elif pattern == "Source - Tasks total":
                            parallelism_metrics["source_tasks_total"] = metric_value
                        
                        # å…¨ãƒ¡ãƒˆãƒªã‚¯ã‚¹ãƒªã‚¹ãƒˆã«è¿½åŠ 
                        parallelism_metrics["all_tasks_metrics"].append({
                            "name": pattern,
                            "value": metric_value
                        })
            
            # AQEShuffleReadãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚’ãƒã‚§ãƒƒã‚¯
            for pattern in aqe_shuffle_patterns:
                if metric_key == pattern:
                    # æ—¢ã«è¦‹ã¤ã‹ã£ã¦ã„ã‚‹å ´åˆã¯ã‚¹ã‚­ãƒƒãƒ—
                    if not any(m["name"] == pattern for m in parallelism_metrics["aqe_shuffle_metrics"]):
                        # ç‰¹å®šã®ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã«ãƒãƒƒãƒ”ãƒ³ã‚°
                        if pattern == "AQEShuffleRead - Number of partitions":
                            parallelism_metrics["aqe_shuffle_partitions"] = metric_value
                        elif pattern == "AQEShuffleRead - Partition data size":
                            parallelism_metrics["aqe_shuffle_data_size"] = metric_value
                        
                        # å…¨ãƒ¡ãƒˆãƒªã‚¯ã‚¹ãƒªã‚¹ãƒˆã«è¿½åŠ 
                        parallelism_metrics["aqe_shuffle_metrics"].append({
                            "name": pattern,
                            "value": metric_value
                        })
    
    # å¹³å‡ãƒ‘ãƒ¼ãƒ†ã‚£ã‚·ãƒ§ãƒ³ã‚µã‚¤ã‚ºã®è¨ˆç®—ã¨è­¦å‘Šè¨­å®š
    if parallelism_metrics["aqe_shuffle_partitions"] > 0 and parallelism_metrics["aqe_shuffle_data_size"] > 0:
        avg_partition_size = parallelism_metrics["aqe_shuffle_data_size"] / parallelism_metrics["aqe_shuffle_partitions"]
        parallelism_metrics["aqe_shuffle_avg_partition_size"] = avg_partition_size
        
        # 512MB = 512 * 1024 * 1024 bytes
        threshold_512mb = 512 * 1024 * 1024
        if avg_partition_size >= threshold_512mb:
            parallelism_metrics["aqe_shuffle_skew_warning"] = True
        else:
            # AQEShuffleReadãƒ¡ãƒˆãƒªã‚¯ã‚¹ãŒå­˜åœ¨ã—ã€å¹³å‡ãƒ‘ãƒ¼ãƒ†ã‚£ã‚·ãƒ§ãƒ³ã‚µã‚¤ã‚ºãŒ512MBæœªæº€ã®å ´åˆã€
            # AQEãŒã‚¹ã‚­ãƒ¥ãƒ¼ã‚’æ¤œå‡ºã—ã¦å¯¾å¿œæ¸ˆã¿ã¨åˆ¤å®š
            parallelism_metrics["aqe_detected_and_handled"] = True
    
    return parallelism_metrics

def calculate_filter_rate(node: Dict[str, Any]) -> Dict[str, Any]:
    """
    ãƒãƒ¼ãƒ‰ã‹ã‚‰Size of files prunedã¨Size of files readãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚’æŠ½å‡ºã—ã¦ãƒ•ã‚£ãƒ«ã‚¿ç‡ã‚’è¨ˆç®—
    
    Args:
        node: ãƒãƒ¼ãƒ‰ãƒ‡ãƒ¼ã‚¿
        
    Returns:
        Dict: ãƒ•ã‚£ãƒ«ã‚¿ç‡è¨ˆç®—çµæœ
    """
    import os
    debug_mode = os.environ.get('DEBUG_FILTER_ANALYSIS', 'false').lower() == 'true'
    
    filter_rate = None
    files_pruned_bytes = 0
    files_read_bytes = 0
    debug_info = []
    
    # æ¤œç´¢å¯¾è±¡ã®ãƒ¡ãƒˆãƒªã‚¯ã‚¹åï¼ˆå®Ÿéš›ã®JSONãƒ•ã‚¡ã‚¤ãƒ«ã§ç¢ºèªã•ã‚ŒãŸãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’å„ªå…ˆï¼‰
    pruned_metrics = [
        "Size of files pruned",  # å®Ÿéš›ã«å­˜åœ¨ã™ã‚‹ã“ã¨ã‚’ç¢ºèªæ¸ˆã¿
        "Size of files pruned before dynamic pruning",  # å®Ÿéš›ã«å­˜åœ¨ã™ã‚‹ã“ã¨ã‚’ç¢ºèªæ¸ˆã¿
        "Pruned files size", 
        "Files pruned size",
        "Num pruned files size"
    ]
    
    read_metrics = [
        "Size of files read",  # å®Ÿéš›ã«å­˜åœ¨ã™ã‚‹ã“ã¨ã‚’ç¢ºèªæ¸ˆã¿
        "Files read size",
        "Read files size",
        "Num files read size"
    ]
    
    # detailed_metricsã‹ã‚‰æ¤œç´¢
    detailed_metrics = node.get('detailed_metrics', {})
    if debug_mode:
        debug_info.append(f"detailed_metrics keys: {list(detailed_metrics.keys())[:5]}")
    
    for metric_key, metric_info in detailed_metrics.items():
        metric_label = metric_info.get('label', '')
        metric_value = metric_info.get('value', 0)
        
        # Prunedé–¢é€£ï¼ˆlabelã‚’å„ªå…ˆçš„ã«ãƒã‚§ãƒƒã‚¯ï¼‰
        for target in pruned_metrics:
            if target in metric_label and metric_value > 0:
                files_pruned_bytes += metric_value  # è¤‡æ•°ã®ãƒ¡ãƒˆãƒªã‚¯ã‚¹ãŒã‚ã‚‹å ´åˆã¯åˆè¨ˆ
                if debug_mode:
                    debug_info.append(f"Found pruned metric: {metric_label} = {metric_value}")
                break
        
        # Readé–¢é€£ï¼ˆlabelã‚’å„ªå…ˆçš„ã«ãƒã‚§ãƒƒã‚¯ï¼‰
        for target in read_metrics:
            if target in metric_label and metric_value > 0:
                files_read_bytes += metric_value  # è¤‡æ•°ã®ãƒ¡ãƒˆãƒªã‚¯ã‚¹ãŒã‚ã‚‹å ´åˆã¯åˆè¨ˆ
                if debug_mode:
                    debug_info.append(f"Found read metric: {metric_label} = {metric_value}")
                break
    
    # raw_metricsã‹ã‚‰æ¤œç´¢ï¼ˆãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼‰
    if files_pruned_bytes == 0 or files_read_bytes == 0:
        raw_metrics = node.get('metrics', [])
        if debug_mode:
            debug_info.append(f"Searching in {len(raw_metrics)} raw metrics")
        
        for metric in raw_metrics:
            metric_label = metric.get('label', '')
            metric_value = metric.get('value', 0)
            
            # Prunedé–¢é€£ï¼ˆlabelã‚’å„ªå…ˆçš„ã«ãƒã‚§ãƒƒã‚¯ï¼‰
            for target in pruned_metrics:
                if target in metric_label and metric_value > 0:
                    files_pruned_bytes += metric_value  # è¤‡æ•°ã®ãƒ¡ãƒˆãƒªã‚¯ã‚¹ãŒã‚ã‚‹å ´åˆã¯åˆè¨ˆ
                    if debug_mode:
                        debug_info.append(f"Found pruned metric in raw: {metric_label} = {metric_value}")
                    break
            
            # Readé–¢é€£ï¼ˆlabelã‚’å„ªå…ˆçš„ã«ãƒã‚§ãƒƒã‚¯ï¼‰
            for target in read_metrics:
                if target in metric_label and metric_value > 0:
                    files_read_bytes += metric_value  # è¤‡æ•°ã®ãƒ¡ãƒˆãƒªã‚¯ã‚¹ãŒã‚ã‚‹å ´åˆã¯åˆè¨ˆ
                    if debug_mode:
                        debug_info.append(f"Found read metric in raw: {metric_label} = {metric_value}")
                    break
    
    # ãƒ•ã‚£ãƒ«ã‚¿ç‡è¨ˆç®—ï¼ˆæ­£ã—ã„å¼: ãƒ—ãƒ«ãƒ¼ãƒ‹ãƒ³ã‚°åŠ¹ç‡ï¼‰
    total_available_bytes = files_read_bytes + files_pruned_bytes
    if total_available_bytes > 0:
        filter_rate = files_pruned_bytes / total_available_bytes
    else:
        filter_rate = 0.0
    
    result = {
        "filter_rate": filter_rate,
        "files_pruned_bytes": files_pruned_bytes,
        "files_read_bytes": files_read_bytes,
        "has_filter_metrics": (files_read_bytes > 0 or files_pruned_bytes > 0)
    }
    
    if debug_mode:
        result["debug_info"] = debug_info
    
    return result

def format_filter_rate_display(filter_result: Dict[str, Any]) -> str:
    """
    ãƒ•ã‚£ãƒ«ã‚¿ç‡è¨ˆç®—çµæœã‚’è¡¨ç¤ºç”¨æ–‡å­—åˆ—ã«å¤‰æ›
    
    Args:
        filter_result: calculate_filter_rate()ã®çµæœ
        
    Returns:
        str: è¡¨ç¤ºç”¨æ–‡å­—åˆ—
    """
    if not filter_result["has_filter_metrics"] or filter_result["filter_rate"] is None:
        return None
    
    filter_rate = filter_result["filter_rate"]
    files_read_gb = filter_result["files_read_bytes"] / (1024 * 1024 * 1024)
    files_pruned_gb = filter_result["files_pruned_bytes"] / (1024 * 1024 * 1024)
    
    return f"ğŸ“‚ ãƒ•ã‚£ãƒ«ã‚¿ç‡: {filter_rate:.1%} (èª­ã¿è¾¼ã¿: {files_read_gb:.2f}GB, ãƒ—ãƒ«ãƒ¼ãƒ³: {files_pruned_gb:.2f}GB)"

def extract_detailed_bottleneck_analysis(extracted_metrics: Dict[str, Any]) -> Dict[str, Any]:
    """
    ã‚»ãƒ«33ã‚¹ã‚¿ã‚¤ãƒ«ã®è©³ç´°ãƒœãƒˆãƒ«ãƒãƒƒã‚¯åˆ†æã‚’å®Ÿè¡Œã—ã€æ§‹é€ åŒ–ã•ã‚ŒãŸãƒ‡ãƒ¼ã‚¿ã‚’è¿”ã™
    
    ğŸš¨ é‡è¦: ãƒ‘ãƒ¼ã‚»ãƒ³ãƒ†ãƒ¼ã‚¸è¨ˆç®—ãƒ‡ã‚°ãƒ¬é˜²æ­¢
    - ä¸¦åˆ—å®Ÿè¡Œãƒãƒ¼ãƒ‰ã®æ™‚é–“åˆè¨ˆã‚’å…¨ä½“æ™‚é–“ã¨ã—ã¦ä½¿ç”¨ã™ã‚‹ã“ã¨ã¯çµ¶å¯¾ã«ç¦æ­¢
    - overall_metrics.total_time_msï¼ˆwall-clock timeï¼‰ã‚’å„ªå…ˆä½¿ç”¨
    - ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯æ™‚ã¯æœ€å¤§ãƒãƒ¼ãƒ‰æ™‚é–“ã‚’ä½¿ç”¨ï¼ˆåˆè¨ˆã§ã¯ãªã„ï¼‰
    
    Args:
        extracted_metrics: æŠ½å‡ºã•ã‚ŒãŸãƒ¡ãƒˆãƒªã‚¯ã‚¹
        
    Returns:
        dict: è©³ç´°ãªãƒœãƒˆãƒ«ãƒãƒƒã‚¯åˆ†æçµæœ
    """
    detailed_analysis = {
        "top_bottleneck_nodes": [],
        "shuffle_optimization_hints": [],
        "spill_analysis": {
            "total_spill_gb": 0,
            "spill_nodes": [],
            "critical_spill_nodes": []
        },
        "skew_analysis": {
            "skewed_nodes": [],
            "total_skewed_partitions": 0
        },
        "performance_recommendations": []
    }
    
    # ãƒãƒ¼ãƒ‰ã‚’å®Ÿè¡Œæ™‚é–“ã§ã‚½ãƒ¼ãƒˆï¼ˆTOP10ï¼‰
    sorted_nodes = sorted(extracted_metrics.get('node_metrics', []), 
                         key=lambda x: x.get('key_metrics', {}).get('durationMs', 0), 
                         reverse=True)
    
    # æœ€å¤§10å€‹ã®ãƒãƒ¼ãƒ‰ã‚’å‡¦ç†
    final_sorted_nodes = sorted_nodes[:10]
    
    # ğŸš¨ é‡è¦: æ­£ã—ã„å…¨ä½“æ™‚é–“ã®è¨ˆç®—ï¼ˆãƒ‡ã‚°ãƒ¬é˜²æ­¢ï¼‰
    # 1. overall_metricsã‹ã‚‰å…¨ä½“å®Ÿè¡Œæ™‚é–“ã‚’å–å¾—ï¼ˆwall-clock timeï¼‰
    overall_metrics = extracted_metrics.get('overall_metrics', {})
    total_duration = overall_metrics.get('total_time_ms', 0)
    
    # ğŸš¨ ä¸¦åˆ—å®Ÿè¡Œå•é¡Œã®ä¿®æ­£: task_total_time_msã‚’å„ªå…ˆä½¿ç”¨
    task_total_time_ms = overall_metrics.get('task_total_time_ms', 0)
    
    if task_total_time_ms > 0:
        total_duration = task_total_time_ms
    elif total_duration <= 0:
        # execution_time_msã‚’æ¬¡ã®å„ªå…ˆåº¦ã§ä½¿ç”¨
        execution_time_ms = overall_metrics.get('execution_time_ms', 0)
        if execution_time_ms > 0:
            total_duration = execution_time_ms
        else:
            # æœ€çµ‚ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
            max_node_time = max([node.get('key_metrics', {}).get('durationMs', 0) for node in sorted_nodes], default=1)
            total_duration = int(max_node_time * 1.2)
    
    for i, node in enumerate(final_sorted_nodes):
        duration_ms = node.get('key_metrics', {}).get('durationMs', 0)
        memory_mb = node.get('key_metrics', {}).get('peakMemoryBytes', 0) / 1024 / 1024
        rows_num = node.get('key_metrics', {}).get('rowsNum', 0)
        
        # ä¸¦åˆ—åº¦æƒ…å ±ã®å–å¾—ï¼ˆä¿®æ­£ç‰ˆ: è¤‡æ•°ã®Tasks totalãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚’å–å¾—ï¼‰
        parallelism_data = extract_parallelism_metrics(node)
        
        # å¾“æ¥ã®å˜ä¸€å€¤ï¼ˆäº’æ›æ€§ã®ãŸã‚ï¼‰
        num_tasks = parallelism_data.get('tasks_total', 0)
        
        # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: Sink - Tasks totalã¾ãŸã¯Source - Tasks totalãŒã‚ã‚‹å ´åˆ
        if num_tasks == 0:
            if parallelism_data.get('sink_tasks_total', 0) > 0:
                num_tasks = parallelism_data.get('sink_tasks_total', 0)
            elif parallelism_data.get('source_tasks_total', 0) > 0:
                num_tasks = parallelism_data.get('source_tasks_total', 0)
        
        # ã‚¹ãƒ”ãƒ«æ¤œå‡ºï¼ˆã‚»ãƒ«33ã¨åŒã˜ãƒ­ã‚¸ãƒƒã‚¯ï¼‰
        spill_detected = False
        spill_bytes = 0
        exact_spill_metrics = [
            "Num bytes spilled to disk due to memory pressure",
            "Sink - Num bytes spilled to disk due to memory pressure",
            "Sink/Num bytes spilled to disk due to memory pressure"
        ]
        
        # detailed_metricsã‹ã‚‰æ¤œç´¢
        detailed_metrics = node.get('detailed_metrics', {})
        for metric_key, metric_info in detailed_metrics.items():
            metric_value = metric_info.get('value', 0)
            metric_label = metric_info.get('label', '')
            
            if (metric_key in exact_spill_metrics or metric_label in exact_spill_metrics) and metric_value > 0:
                spill_detected = True
                spill_bytes = max(spill_bytes, metric_value)
                break
        
        # raw_metricsã‹ã‚‰æ¤œç´¢ï¼ˆãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼‰
        if not spill_detected:
            raw_metrics = node.get('metrics', [])
            for metric in raw_metrics:
                metric_key = metric.get('key', '')
                metric_label = metric.get('label', '')
                metric_value = metric.get('value', 0)
                
                if (metric_key in exact_spill_metrics or metric_label in exact_spill_metrics) and metric_value > 0:
                    spill_detected = True
                    spill_bytes = max(spill_bytes, metric_value)
                    break
        
        # ã‚¹ã‚­ãƒ¥ãƒ¼æ¤œå‡ºï¼ˆAQEãƒ™ãƒ¼ã‚¹ï¼‰
        skew_detected = False
        skewed_partitions = 0
        target_skew_metric = "AQEShuffleRead - Number of skewed partitions"
        
        for metric_key, metric_info in detailed_metrics.items():
            if metric_key == target_skew_metric:
                try:
                    skewed_partitions = int(metric_info.get('value', 0))
                    if skewed_partitions > 0:
                        skew_detected = True
                    break
                except (ValueError, TypeError):
                    continue
        
        node_name = get_meaningful_node_name(node, extracted_metrics)
        # ğŸš¨ é‡è¦: æ­£ã—ã„ãƒ‘ãƒ¼ã‚»ãƒ³ãƒ†ãƒ¼ã‚¸è¨ˆç®—ï¼ˆãƒ‡ã‚°ãƒ¬é˜²æ­¢ï¼‰
        # wall-clock timeã«å¯¾ã™ã‚‹å„ãƒãƒ¼ãƒ‰ã®å®Ÿè¡Œæ™‚é–“ã®å‰²åˆ
        time_percentage = min((duration_ms / max(total_duration, 1)) * 100, 100.0)
        
        # ã‚¹ã‚­ãƒ¥ãƒ¼åˆ¤å®šï¼ˆAQEã‚¹ã‚­ãƒ¥ãƒ¼æ¤œå‡ºã¨AQEShuffleReadå¹³å‡ãƒ‘ãƒ¼ãƒ†ã‚£ã‚·ãƒ§ãƒ³ã‚µã‚¤ã‚ºã®ä¸¡æ–¹ã‚’è€ƒæ…®ï¼‰
        aqe_shuffle_skew_warning = parallelism_data.get('aqe_shuffle_skew_warning', False)
        combined_skew_detected = skew_detected or aqe_shuffle_skew_warning
        
        # ãƒãƒ¼ãƒ‰åˆ†æçµæœã‚’æ§‹é€ åŒ–
        node_analysis = {
            "rank": i + 1,
            "node_id": node.get('node_id', node.get('id', 'N/A')),
            "node_name": node_name,
            "duration_ms": duration_ms,
            "time_percentage": time_percentage,
            "memory_mb": memory_mb,
            "rows_processed": rows_num,
            "num_tasks": num_tasks,
            "parallelism_data": parallelism_data,  # è¤‡æ•°ã®Tasks totalãƒ¡ãƒˆãƒªã‚¯ã‚¹æƒ…å ±ã‚’è¿½åŠ 
            "spill_detected": spill_detected,
            "spill_bytes": spill_bytes,
            "spill_gb": spill_bytes / 1024 / 1024 / 1024 if spill_bytes > 0 else 0,
            "skew_detected": combined_skew_detected,  # AQEã‚¹ã‚­ãƒ¥ãƒ¼æ¤œå‡ºã¨AQEShuffleReadè­¦å‘Šã®ä¸¡æ–¹ã‚’è€ƒæ…®
            "aqe_skew_detected": skew_detected,  # å¾“æ¥ã®AQEã‚¹ã‚­ãƒ¥ãƒ¼æ¤œå‡ºã®ã¿
            "aqe_shuffle_skew_warning": aqe_shuffle_skew_warning,  # AQEShuffleReadå¹³å‡ãƒ‘ãƒ¼ãƒ†ã‚£ã‚·ãƒ§ãƒ³ã‚µã‚¤ã‚ºè­¦å‘Š
            "skewed_partitions": skewed_partitions,
            "is_shuffle_node": "shuffle" in node_name.lower(),
            "severity": "CRITICAL" if duration_ms >= 10000 else "HIGH" if duration_ms >= 5000 else "MEDIUM" if duration_ms >= 1000 else "LOW"
        }
        
        # Shuffleãƒãƒ¼ãƒ‰ã®å ´åˆã€ã‚¹ãƒ”ãƒ«ãŒæ¤œå‡ºã•ã‚Œã¦ã„ã‚‹å ´åˆã®ã¿REPARTITIONãƒ’ãƒ³ãƒˆã‚’è¿½åŠ 
        if node_analysis["is_shuffle_node"] and spill_detected and spill_bytes > 0:
            shuffle_attributes = extract_shuffle_attributes(node)
            if shuffle_attributes:
                suggested_partitions = max(num_tasks * 2, 200)
                
                # Shuffleå±æ€§ã§æ¤œå‡ºã•ã‚ŒãŸã‚«ãƒ©ãƒ ã‚’å…¨ã¦ä½¿ç”¨ï¼ˆå®Œå…¨ä¸€è‡´ï¼‰
                repartition_columns = ", ".join(shuffle_attributes)
                
                repartition_hint = {
                    "node_id": node_analysis["node_id"],
                    "attributes": shuffle_attributes,
                    "suggested_sql": f"REPARTITION({suggested_partitions}, {repartition_columns})",
                    "reason": f"ã‚¹ãƒ”ãƒ«({node_analysis['spill_gb']:.2f}GB)æ”¹å–„",
                    "priority": "HIGH",
                    "estimated_improvement": "å¤§å¹…ãªæ€§èƒ½æ”¹å–„ãŒæœŸå¾…",

                }
                detailed_analysis["shuffle_optimization_hints"].append(repartition_hint)
                node_analysis["repartition_hint"] = repartition_hint
        

        # ãƒ•ã‚£ãƒ«ã‚¿ç‡è¨ˆç®—ã¨æƒ…å ±æ›´æ–°
        filter_result = calculate_filter_rate(node)
        node_analysis.update({
            "filter_rate": filter_result["filter_rate"],
            "files_pruned_bytes": filter_result["files_pruned_bytes"],
            "files_read_bytes": filter_result["files_read_bytes"],
            "has_filter_metrics": filter_result["has_filter_metrics"]
        })
        
        # ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°ã‚­ãƒ¼æƒ…å ±ã®è¿½åŠ 
        cluster_attributes = extract_cluster_attributes(node)
        node_analysis.update({
            "cluster_attributes": cluster_attributes,
            "has_clustering": len(cluster_attributes) > 0
        })
        
        detailed_analysis["top_bottleneck_nodes"].append(node_analysis)
        
        # ã‚¹ãƒ”ãƒ«åˆ†æã¸ã®è¿½åŠ 
        if spill_detected:
            detailed_analysis["spill_analysis"]["total_spill_gb"] += node_analysis["spill_gb"]
            detailed_analysis["spill_analysis"]["spill_nodes"].append({
                "node_id": node_analysis["node_id"],
                "node_name": node_name,
                "spill_gb": node_analysis["spill_gb"],
                "rank": i + 1
            })
            
            if node_analysis["spill_gb"] > 1.0:  # 1GBä»¥ä¸Šã¯é‡è¦
                detailed_analysis["spill_analysis"]["critical_spill_nodes"].append(node_analysis["node_id"])
        
        # ã‚¹ã‚­ãƒ¥ãƒ¼åˆ†æã¸ã®è¿½åŠ 
        if skew_detected:
            detailed_analysis["skew_analysis"]["total_skewed_partitions"] += skewed_partitions
            detailed_analysis["skew_analysis"]["skewed_nodes"].append({
                "node_id": node_analysis["node_id"],
                "node_name": node_name,
                "skewed_partitions": skewed_partitions,
                "rank": i + 1
            })
    
    # å…¨ä½“çš„ãªæ¨å¥¨äº‹é …ã®ç”Ÿæˆ
    if detailed_analysis["spill_analysis"]["total_spill_gb"] > 5.0:
        detailed_analysis["performance_recommendations"].append({
            "type": "memory_optimization",
            "priority": "CRITICAL",
            "description": f"å¤§é‡ã‚¹ãƒ”ãƒ«({detailed_analysis['spill_analysis']['total_spill_gb']:.1f}GB)æ¤œå‡º: ãƒ¡ãƒ¢ãƒªè¨­å®šã¨ãƒ‘ãƒ¼ãƒ†ã‚£ã‚·ãƒ§ãƒ‹ãƒ³ã‚°æˆ¦ç•¥ã®è¦‹ç›´ã—ãŒå¿…è¦"
        })
    
    if len(detailed_analysis["shuffle_optimization_hints"]) > 0:
        detailed_analysis["performance_recommendations"].append({
            "type": "shuffle_optimization", 
            "priority": "HIGH",
            "description": f"{len(detailed_analysis['shuffle_optimization_hints'])}å€‹ã®ã‚¹ãƒ”ãƒ«ç™ºç”ŸShuffleãƒãƒ¼ãƒ‰ã§ãƒ¡ãƒ¢ãƒªæœ€é©åŒ–ãŒå¿…è¦"
        })
    
    if detailed_analysis["skew_analysis"]["total_skewed_partitions"] > 10:
        detailed_analysis["performance_recommendations"].append({
            "type": "skew_optimization",
            "priority": "HIGH", 
            "description": f"ãƒ‡ãƒ¼ã‚¿ã‚¹ã‚­ãƒ¥ãƒ¼({detailed_analysis['skew_analysis']['total_skewed_partitions']}ãƒ‘ãƒ¼ãƒ†ã‚£ã‚·ãƒ§ãƒ³)æ¤œå‡º: ãƒ‡ãƒ¼ã‚¿åˆ†æ•£ã®è¦‹ç›´ã—ãŒå¿…è¦"
        })
    
    return detailed_analysis

print("âœ… é–¢æ•°å®šç¾©å®Œäº†: get_meaningful_node_name, extract_shuffle_attributes, extract_detailed_bottleneck_analysis")

# COMMAND ----------

# MAGIC %md
# MAGIC ## ğŸ¯ ãƒœãƒˆãƒ«ãƒãƒƒã‚¯æŒ‡æ¨™è¨ˆç®—é–¢æ•°
# MAGIC
# MAGIC ã“ã®ã‚»ãƒ«ã§ã¯ä»¥ä¸‹ã®æ©Ÿèƒ½ã‚’å®šç¾©ã—ã¾ã™ï¼š
# MAGIC - å®Ÿè¡Œæ™‚é–“ã¨ã‚³ãƒ³ãƒ‘ã‚¤ãƒ«æ™‚é–“ã®æ¯”ç‡åˆ†æ
# MAGIC - ã‚­ãƒ£ãƒƒã‚·ãƒ¥åŠ¹ç‡ã¨ãƒ‡ãƒ¼ã‚¿å‡¦ç†åŠ¹ç‡ã®è¨ˆç®—
# MAGIC - Photonåˆ©ç”¨ç‡ã®åˆ†æ
# MAGIC - ã‚¹ãƒ”ãƒ«æ¤œå‡ºã¨ã‚·ãƒ£ãƒƒãƒ•ãƒ«/ä¸¦åˆ—åº¦ã®å•é¡Œç‰¹å®š

# COMMAND ----------

def calculate_bottleneck_indicators(metrics: Dict[str, Any]) -> Dict[str, Any]:
    """ãƒœãƒˆãƒ«ãƒãƒƒã‚¯æŒ‡æ¨™ã‚’è¨ˆç®—"""
    indicators = {}
    
    overall = metrics.get('overall_metrics', {})
    total_time = overall.get('total_time_ms', 0)
    execution_time = overall.get('execution_time_ms', 0)
    compilation_time = overall.get('compilation_time_ms', 0)
    
    if total_time > 0:
        indicators['compilation_ratio'] = compilation_time / total_time
        indicators['execution_ratio'] = execution_time / total_time
    
    # ã‚­ãƒ£ãƒƒã‚·ãƒ¥åŠ¹ç‡
    read_bytes = overall.get('read_bytes', 0)
    cache_bytes = overall.get('read_cache_bytes', 0)
    if read_bytes > 0:
        indicators['cache_hit_ratio'] = cache_bytes / read_bytes
    
    # ãƒ‡ãƒ¼ã‚¿å‡¦ç†åŠ¹ç‡ï¼ˆå®¹é‡ãƒ™ãƒ¼ã‚¹ï¼‰
    read_bytes = overall.get('read_bytes', 0)
    
    # å®¹é‡ãƒ™ãƒ¼ã‚¹ã®ãƒ•ã‚£ãƒ«ã‚¿ç‡ã‚’è¨ˆç®—ï¼ˆæ­£ã—ã„å®Ÿè£…ï¼‰
    data_selectivity = calculate_filter_rate_percentage(overall, metrics)
    
    indicators['data_selectivity'] = data_selectivity
    
    # Photonä½¿ç”¨ç‡ï¼ˆã‚¿ã‚¹ã‚¯å®Ÿè¡Œæ™‚é–“ã«å¯¾ã™ã‚‹å‰²åˆï¼‰
    task_time = overall.get('task_total_time_ms', 0)
    photon_time = overall.get('photon_total_time_ms', 0)
    if task_time > 0:
        indicators['photon_ratio'] = min(photon_time / task_time, 1.0)  # æœ€å¤§100%ã«åˆ¶é™
    else:
        indicators['photon_ratio'] = 0.0
    
    # ã‚¹ãƒ”ãƒ«æ¤œå‡ºï¼ˆè©³ç´°ç‰ˆï¼šSink - Num bytes spilled to disk due to memory pressure ãƒ™ãƒ¼ã‚¹ï¼‰
    spill_detected = False
    total_spill_bytes = 0
    spill_details = []
    
    # ã‚¿ãƒ¼ã‚²ãƒƒãƒˆãƒ¡ãƒˆãƒªã‚¯ã‚¹åï¼ˆè¤‡æ•°ãƒ‘ã‚¿ãƒ¼ãƒ³å¯¾å¿œï¼‰
    target_spill_metrics = [
        "Sink - Num bytes spilled to disk due to memory pressure",
        "Num bytes spilled to disk due to memory pressure"
    ]
    
    # å„ãƒãƒ¼ãƒ‰ã§ã‚¹ãƒ”ãƒ«æ¤œå‡ºã‚’å®Ÿè¡Œ
    for node in metrics.get('node_metrics', []):
        node_spill_found = False
        
        # 1. detailed_metricsã‹ã‚‰æ¤œç´¢
        detailed_metrics = node.get('detailed_metrics', {})
        for metric_key, metric_info in detailed_metrics.items():
            metric_value = metric_info.get('value', 0)
            metric_label = metric_info.get('label', '')
            
            # è¤‡æ•°ã®ã‚¹ãƒ”ãƒ«ãƒ¡ãƒˆãƒªã‚¯ã‚¹åã‚’ãƒã‚§ãƒƒã‚¯
            if ((metric_key in target_spill_metrics or 
                 metric_label in target_spill_metrics) and metric_value > 0):
                spill_detected = True
                node_spill_found = True
                total_spill_bytes += metric_value
                spill_details.append({
                    'node_id': node.get('node_id', ''),
                    'node_name': node.get('name', ''),
                    'spill_bytes': metric_value,
                    'spill_metric': metric_key if metric_key in target_spill_metrics else metric_label,
                    'source': 'detailed_metrics'
                })
                break
        
        # 2. raw_metricsã‹ã‚‰æ¤œç´¢ï¼ˆã“ã®ãƒãƒ¼ãƒ‰ã§ã¾ã è¦‹ã¤ã‹ã‚‰ãªã„å ´åˆï¼‰
        if not node_spill_found:
            raw_metrics = node.get('metrics', [])
            for metric in raw_metrics:
                metric_key = metric.get('key', '')
                metric_label = metric.get('label', '')
                metric_value = metric.get('value', 0)
                
                # è¤‡æ•°ã®ã‚¹ãƒ”ãƒ«ãƒ¡ãƒˆãƒªã‚¯ã‚¹åã‚’ãƒã‚§ãƒƒã‚¯
                if ((metric_key in target_spill_metrics or 
                     metric_label in target_spill_metrics) and metric_value > 0):
                    spill_detected = True
                    node_spill_found = True
                    total_spill_bytes += metric_value
                    spill_details.append({
                        'node_id': node.get('node_id', ''),
                        'node_name': node.get('name', ''),
                        'spill_bytes': metric_value,
                        'spill_metric': metric_key if metric_key in target_spill_metrics else metric_label,
                        'source': 'raw_metrics'
                    })
                    break
        
        # 3. key_metricsã‹ã‚‰æ¤œç´¢ï¼ˆæœ€å¾Œã®ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼‰
        if not node_spill_found:
            key_metrics = node.get('key_metrics', {})
            for key_metric_name, key_metric_value in key_metrics.items():
                # è¤‡æ•°ã®ã‚¹ãƒ”ãƒ«ãƒ¡ãƒˆãƒªã‚¯ã‚¹åã‚’ãƒã‚§ãƒƒã‚¯
                if key_metric_name in target_spill_metrics and key_metric_value > 0:
                    spill_detected = True
                    node_spill_found = True
                    total_spill_bytes += key_metric_value
                    spill_details.append({
                        'node_id': node.get('node_id', ''),
                        'node_name': node.get('name', ''),
                        'spill_bytes': key_metric_value,
                        'spill_metric': key_metric_name,
                        'source': 'key_metrics'
                    })
                    break
    
    # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: overall_metricsã‹ã‚‰ã®ç°¡æ˜“æ¤œå‡º
    if not spill_detected:
        fallback_spill_bytes = overall.get('spill_to_disk_bytes', 0)
        if fallback_spill_bytes > 0:
            spill_detected = True
            total_spill_bytes = fallback_spill_bytes
            spill_details.append({
                'node_id': 'overall',
                'node_name': 'Overall Metrics',
                'spill_bytes': fallback_spill_bytes,
                'source': 'overall_metrics'
            })
    
    indicators['has_spill'] = spill_detected
    indicators['spill_bytes'] = total_spill_bytes
    indicators['spill_details'] = spill_details
    indicators['spill_nodes_count'] = len(spill_details)
    
    # æœ€ã‚‚æ™‚é–“ã®ã‹ã‹ã‚‹ã‚¹ãƒ†ãƒ¼ã‚¸
    stage_durations = [(s['stage_id'], s['duration_ms']) for s in metrics.get('stage_metrics', []) if s['duration_ms'] > 0]
    if stage_durations:
        slowest_stage = max(stage_durations, key=lambda x: x[1])
        indicators['slowest_stage_id'] = slowest_stage[0]
        indicators['slowest_stage_duration'] = slowest_stage[1]
    
    # æœ€ã‚‚ãƒ¡ãƒ¢ãƒªã‚’ä½¿ç”¨ã™ã‚‹ãƒãƒ¼ãƒ‰
    memory_usage = []
    for node in metrics.get('node_metrics', []):
        peak_memory = node.get('key_metrics', {}).get('peakMemoryBytes', 0)
        if peak_memory > 0:
            memory_usage.append((node['node_id'], node['name'], peak_memory))
    
    if memory_usage:
        highest_memory_node = max(memory_usage, key=lambda x: x[2])
        indicators['highest_memory_node_id'] = highest_memory_node[0]
        indicators['highest_memory_node_name'] = highest_memory_node[1]
        indicators['highest_memory_bytes'] = highest_memory_node[2]
    
    # ä¸¦åˆ—åº¦ã¨ã‚·ãƒ£ãƒƒãƒ•ãƒ«å•é¡Œã®æ¤œå‡º
    shuffle_nodes = []
    low_parallelism_stages = []
    
    # ã‚·ãƒ£ãƒƒãƒ•ãƒ«ãƒãƒ¼ãƒ‰ã®ç‰¹å®š
    for node in metrics.get('node_metrics', []):
        node_name = node.get('name', '').upper()
        if any(keyword in node_name for keyword in ['SHUFFLE', 'EXCHANGE']):
            shuffle_nodes.append({
                'node_id': node['node_id'],
                'name': node['name'],
                'duration_ms': node.get('key_metrics', {}).get('durationMs', 0),
                'rows': node.get('key_metrics', {}).get('rowsNum', 0)
            })
    
    # ä½ä¸¦åˆ—åº¦ã‚¹ãƒ†ãƒ¼ã‚¸ã®æ¤œå‡º
    for stage in metrics.get('stage_metrics', []):
        num_tasks = stage.get('num_tasks', 0)
        duration_ms = stage.get('duration_ms', 0)
        
        # ä¸¦åˆ—åº¦ãŒä½ã„ï¼ˆã‚¿ã‚¹ã‚¯æ•°ãŒå°‘ãªã„ï¼‰ã‹ã¤å®Ÿè¡Œæ™‚é–“ãŒé•·ã„ã‚¹ãƒ†ãƒ¼ã‚¸
        if num_tasks > 0 and num_tasks < 10 and duration_ms > 5000:  # 10ã‚¿ã‚¹ã‚¯æœªæº€ã€5ç§’ä»¥ä¸Š
            low_parallelism_stages.append({
                'stage_id': stage['stage_id'],
                'num_tasks': num_tasks,
                'duration_ms': duration_ms,
                'avg_task_duration': duration_ms / max(num_tasks, 1)
            })
    
    indicators['shuffle_operations_count'] = len(shuffle_nodes)
    indicators['low_parallelism_stages_count'] = len(low_parallelism_stages)
    indicators['has_shuffle_bottleneck'] = len(shuffle_nodes) > 0 and any(s['duration_ms'] > 10000 for s in shuffle_nodes)
    indicators['has_low_parallelism'] = len(low_parallelism_stages) > 0
    
    # ã‚·ãƒ£ãƒƒãƒ•ãƒ«ã®è©³ç´°æƒ…å ±
    if shuffle_nodes:
        total_shuffle_time = sum(s['duration_ms'] for s in shuffle_nodes)
        indicators['total_shuffle_time_ms'] = total_shuffle_time
        indicators['shuffle_time_ratio'] = total_shuffle_time / max(total_time, 1)
        
        # æœ€ã‚‚æ™‚é–“ã®ã‹ã‹ã‚‹ã‚·ãƒ£ãƒƒãƒ•ãƒ«æ“ä½œ
        slowest_shuffle = max(shuffle_nodes, key=lambda x: x['duration_ms'])
        indicators['slowest_shuffle_duration_ms'] = slowest_shuffle['duration_ms']
        indicators['slowest_shuffle_node'] = slowest_shuffle['name']
    
    # ä½ä¸¦åˆ—åº¦ã®è©³ç´°æƒ…å ±
    if low_parallelism_stages:
        indicators['low_parallelism_details'] = low_parallelism_stages
        avg_parallelism = sum(s['num_tasks'] for s in low_parallelism_stages) / len(low_parallelism_stages)
        indicators['average_low_parallelism'] = avg_parallelism
    
    # AQEShuffleReadè­¦å‘Šã®æ¤œå‡º
    aqe_shuffle_skew_warning_detected = False
    aqe_detected_and_handled = False
    
    for node in metrics.get('node_metrics', []):
        parallelism_data = extract_parallelism_metrics(node)
        if parallelism_data.get('aqe_shuffle_skew_warning', False):
            aqe_shuffle_skew_warning_detected = True
        if parallelism_data.get('aqe_detected_and_handled', False):
            aqe_detected_and_handled = True
    
    # å„ªå…ˆé †ä½: 512MBä»¥ä¸Šã®è­¦å‘ŠãŒã‚ã‚Œã°ã€ãã‚Œã‚’å„ªå…ˆ
    # è­¦å‘ŠãŒãªã„å ´åˆã®ã¿ã€AQEå¯¾å¿œæ¸ˆã¿ã¨åˆ¤å®š
    indicators['has_aqe_shuffle_skew_warning'] = aqe_shuffle_skew_warning_detected
    indicators['has_skew'] = aqe_detected_and_handled and not aqe_shuffle_skew_warning_detected
    
    return indicators

print("âœ… é–¢æ•°å®šç¾©å®Œäº†: calculate_bottleneck_indicators")

# COMMAND ----------

# MAGIC %md
# MAGIC ## ğŸ§¬ Liquid Clusteringåˆ†æé–¢æ•°
# MAGIC
# MAGIC ã“ã®ã‚»ãƒ«ã§ã¯ä»¥ä¸‹ã®æ©Ÿèƒ½ã‚’å®šç¾©ã—ã¾ã™ï¼š
# MAGIC - ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ©ãƒ¼ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰ã®ã‚«ãƒ©ãƒ æƒ…å ±æŠ½å‡º
# MAGIC - ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼ã€JOINã€GROUP BYæ¡ä»¶ã®åˆ†æ
# MAGIC - ãƒ‡ãƒ¼ã‚¿ã‚¹ã‚­ãƒ¥ãƒ¼ã¨ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹å½±éŸ¿ã®è©•ä¾¡
# MAGIC - ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°æ¨å¥¨ã‚«ãƒ©ãƒ ã®ç‰¹å®š

# COMMAND ----------

def calculate_performance_insights_from_metrics(overall_metrics: Dict[str, Any], metrics: Optional[Dict[str, Any]] = None) -> Dict[str, Any]:
    """
    ãƒ¡ãƒˆãƒªã‚¯ã‚¹æƒ…å ±ã®ã¿ã‹ã‚‰è©³ç´°ãªãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æ´å¯Ÿã‚’è¨ˆç®—
    """
    insights = {}
    
    # åŸºæœ¬ãƒ‡ãƒ¼ã‚¿
    total_time_ms = overall_metrics.get('total_time_ms', 0)
    read_bytes = overall_metrics.get('read_bytes', 0)
    read_cache_bytes = overall_metrics.get('read_cache_bytes', 0)
    read_remote_bytes = overall_metrics.get('read_remote_bytes', 0)
    rows_read = overall_metrics.get('rows_read_count', 0)
    rows_produced = overall_metrics.get('rows_produced_count', 0)
    read_files = overall_metrics.get('read_files_count', 0)
    read_partitions = overall_metrics.get('read_partitions_count', 0)
    photon_time = overall_metrics.get('photon_total_time_ms', 0)
    task_time = overall_metrics.get('task_total_time_ms', 0)
    spill_bytes = overall_metrics.get('spill_to_disk_bytes', 0)
    
    # 1. ãƒ‡ãƒ¼ã‚¿åŠ¹ç‡åˆ†æï¼ˆå®¹é‡ãƒ™ãƒ¼ã‚¹ï¼‰
    # metricsãŒNoneã®å ´åˆã¯ç©ºã®è¾æ›¸ã§åˆæœŸåŒ–
    if metrics is None:
        metrics = {'node_metrics': []}
    
    # å®¹é‡ãƒ™ãƒ¼ã‚¹ã®ãƒ•ã‚£ãƒ«ã‚¿ç‡ã‚’è¨ˆç®—ï¼ˆæ­£ã—ã„å®Ÿè£…ï¼‰
    filter_rate_capacity = calculate_filter_rate_percentage(overall_metrics, metrics)
    
    insights['data_efficiency'] = {
        'data_selectivity': filter_rate_capacity,
        'avg_bytes_per_file': read_bytes / max(read_files, 1),
        'avg_bytes_per_partition': read_bytes / max(read_partitions, 1),
        'avg_rows_per_file': rows_read / max(read_files, 1),
        'avg_rows_per_partition': rows_read / max(read_partitions, 1)
    }
    
    # 2. ã‚­ãƒ£ãƒƒã‚·ãƒ¥åŠ¹ç‡åˆ†æ
    cache_hit_ratio = read_cache_bytes / max(read_bytes, 1)
    insights['cache_efficiency'] = {
        'cache_hit_ratio': cache_hit_ratio,
        'cache_hit_percentage': cache_hit_ratio * 100,
        'remote_read_ratio': read_remote_bytes / max(read_bytes, 1),
        'cache_effectiveness': 'high' if cache_hit_ratio > 0.8 else 'medium' if cache_hit_ratio > 0.5 else 'low'
    }
    
    # 3. ä¸¦åˆ—åŒ–åŠ¹ç‡åˆ†æ
    insights['parallelization'] = {
        'files_per_second': read_files / max(total_time_ms / 1000, 1),
        'partitions_per_second': read_partitions / max(total_time_ms / 1000, 1),
        'throughput_mb_per_second': (read_bytes / 1024 / 1024) / max(total_time_ms / 1000, 1),
        'rows_per_second': rows_read / max(total_time_ms / 1000, 1)
    }
    
    # 4. PhotonåŠ¹ç‡åˆ†æ
    photon_efficiency = photon_time / max(task_time, 1)
    insights['photon_analysis'] = {
        'photon_enabled': photon_time > 0,
        'photon_efficiency': photon_efficiency,
        'photon_utilization_percentage': photon_efficiency * 100,
        'photon_effectiveness': 'high' if photon_efficiency > 0.8 else 'medium' if photon_efficiency > 0.5 else 'low'
    }
    
    # 5. ãƒªã‚½ãƒ¼ã‚¹ä½¿ç”¨çŠ¶æ³
    insights['resource_usage'] = {
        'memory_pressure': spill_bytes > 0,
        'spill_gb': spill_bytes / 1024 / 1024 / 1024,
        'data_processed_gb': read_bytes / 1024 / 1024 / 1024,
        'data_reduction_ratio': 1 - (rows_produced / max(rows_read, 1))
    }
    
    # 6. ãƒœãƒˆãƒ«ãƒãƒƒã‚¯æŒ‡æ¨™
    bottlenecks = []
    if cache_hit_ratio < 0.3:
        bottlenecks.append('ä½ã‚­ãƒ£ãƒƒã‚·ãƒ¥åŠ¹ç‡')
    if read_remote_bytes / max(read_bytes, 1) > 0.8:
        bottlenecks.append('é«˜ãƒªãƒ¢ãƒ¼ãƒˆèª­ã¿è¾¼ã¿æ¯”ç‡')
    if photon_efficiency < 0.5 and photon_time > 0:
        bottlenecks.append('ä½PhotonåŠ¹ç‡')
    if spill_bytes > 0:
        bottlenecks.append('ãƒ¡ãƒ¢ãƒªã‚¹ãƒ”ãƒ«ç™ºç”Ÿ')
    if insights['data_efficiency']['data_selectivity'] < 0.2:
        bottlenecks.append('ä½ãƒ•ã‚£ãƒ«ã‚¿åŠ¹ç‡')
    
    insights['potential_bottlenecks'] = bottlenecks
    
    return insights

def calculate_filter_rate_percentage(overall_metrics: Dict[str, Any], metrics: Dict[str, Any]) -> float:
    """
    å®¹é‡ãƒ™ãƒ¼ã‚¹ã®ãƒ•ã‚£ãƒ«ã‚¿ç‡ã‚’è¨ˆç®—ã™ã‚‹ï¼ˆoverall_metrics.read_bytesä½¿ç”¨ç‰ˆï¼‰
    
    âŒ ãƒ‡ã‚°ãƒ¬é˜²æ­¢æ³¨æ„: ã“ã®é–¢æ•°ã¯å¿…ãšoverall_metrics.read_bytesã‚’ä½¿ç”¨ã—ã¦ãã ã•ã„ï¼
    âŒ files_read_bytesï¼ˆã‚¹ã‚­ãƒ£ãƒ³ãƒãƒ¼ãƒ‰é›†è¨ˆï¼‰ã¯ä½¿ç”¨ã—ãªã„ã§ãã ã•ã„ï¼
    
    Args:
        overall_metrics: å…¨ä½“ãƒ¡ãƒˆãƒªã‚¯ã‚¹ï¼ˆread_bytesã‚’ä½¿ç”¨ï¼‰
        metrics: å…¨ãƒ¡ãƒˆãƒªã‚¯ã‚¹ï¼ˆnode_metricsã‚’å«ã‚€ã€pruned_byteså–å¾—ç”¨ï¼‰
        
    Returns:
        float: ãƒ•ã‚£ãƒ«ã‚¿ç‡ï¼ˆ0.0-1.0ã€é«˜ã„å€¤ã»ã©åŠ¹ç‡çš„ï¼‰
               ãƒ—ãƒ«ãƒ¼ãƒ‹ãƒ³ã‚°åŠ¹ç‡ = files_pruned_bytes / (overall_read_bytes + files_pruned_bytes)
    """
    import os
    debug_mode = os.environ.get('DEBUG_FILTER_ANALYSIS', 'false').lower() == 'true'
    
    # âŒ ãƒ‡ã‚°ãƒ¬é˜²æ­¢: å¿…ãšoverall_metrics.read_bytesã‚’ä½¿ç”¨ï¼
    overall_read_bytes = overall_metrics.get('read_bytes', 0)
    
    if debug_mode:
        print(f"ğŸ” ãƒ•ã‚£ãƒ«ã‚¿ç‡è¨ˆç®—ãƒ‡ãƒãƒƒã‚°ï¼ˆoverall_metrics.read_bytesä½¿ç”¨ç‰ˆï¼‰:")
        print(f"   overall_read_bytes: {overall_read_bytes:,} ({overall_read_bytes / (1024**4):.2f} TB)")
    
    try:
        # pruned_bytesã®ã¿node_metricsã‹ã‚‰å–å¾—ï¼ˆread_bytesã¯ä½¿ç”¨ã—ãªã„ï¼‰
        node_metrics = metrics.get('node_metrics', [])
        total_files_pruned_bytes = 0
        filter_metrics_found = False
        
        # å…¨ã¦ã®ã‚¹ã‚­ãƒ£ãƒ³ãƒãƒ¼ãƒ‰ã‹ã‚‰prunedæƒ…å ±ã®ã¿ã‚’é›†è¨ˆ
        for node in node_metrics:
            if node.get('tag') in ['FileScan', 'BatchScan', 'TableScan', 'UNKNOWN_DATA_SOURCE_SCAN_EXEC']:
                filter_result = calculate_filter_rate(node)
                if filter_result.get('has_filter_metrics', False):
                    files_pruned_bytes = filter_result.get('files_pruned_bytes', 0)
                    
                    if files_pruned_bytes > 0:
                        total_files_pruned_bytes += files_pruned_bytes
                        filter_metrics_found = True
                        
                        if debug_mode:
                            print(f"   ãƒãƒ¼ãƒ‰ {node.get('node_id', 'unknown')}: files_pruned_bytes = {files_pruned_bytes:,}")
        
        # âŒ ãƒ‡ã‚°ãƒ¬é˜²æ­¢: overall_read_bytes + pruned_bytes ã§è¨ˆç®—
        if filter_metrics_found and overall_read_bytes > 0:
            # æ­£ã—ã„è¨ˆç®—: ãƒ—ãƒ«ãƒ¼ãƒ‹ãƒ³ã‚°åŠ¹ç‡ = files_pruned / (overall_read + files_pruned)
            total_available_bytes = overall_read_bytes + total_files_pruned_bytes
            if total_available_bytes > 0:
                overall_filter_rate = total_files_pruned_bytes / total_available_bytes
            else:
                overall_filter_rate = 0.0
                
            if debug_mode:
                print(f"   âŒ ãƒ‡ã‚°ãƒ¬é˜²æ­¢ç‰ˆ: overall_read_bytesä½¿ç”¨")
                print(f"     overall_read_bytes: {overall_read_bytes:,} ({overall_read_bytes / (1024**4):.2f} TB)")
                print(f"     total_files_pruned_bytes: {total_files_pruned_bytes:,} ({total_files_pruned_bytes / (1024**4):.2f} TB)")
                print(f"     total_available_bytes: {total_available_bytes:,} ({total_available_bytes / (1024**4):.2f} TB)")
                print(f"     ãƒ—ãƒ«ãƒ¼ãƒ‹ãƒ³ã‚°åŠ¹ç‡: {overall_filter_rate*100:.2f}%")
            return overall_filter_rate
        
        if debug_mode:
            print(f"   ãƒ•ã‚£ãƒ«ã‚¿ãƒ¡ãƒˆãƒªã‚¯ã‚¹: {'æ¤œå‡º' if filter_metrics_found else 'æœªæ¤œå‡º'}")
            print(f"   overall_read_bytes: {overall_read_bytes:,}")
            if not filter_metrics_found:
                print(f"   âš ï¸ ãƒ—ãƒ«ãƒ¼ãƒ‹ãƒ³ã‚°æƒ…å ±ãŒåˆ©ç”¨ã§ãã¾ã›ã‚“")
            if overall_read_bytes == 0:
                print(f"   âš ï¸ èª­ã¿è¾¼ã¿ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“")
        
        # ãƒ—ãƒ«ãƒ¼ãƒ‹ãƒ³ã‚°æƒ…å ±ãŒãªã„å ´åˆã¯0ã‚’è¿”ã™
        return 0.0
        
    except Exception as e:
        if debug_mode:
            print(f"   ãƒ•ã‚£ãƒ«ã‚¿ç‡è¨ˆç®—ã‚¨ãƒ©ãƒ¼: {e}")
        return 0.0

def extract_liquid_clustering_data(profiler_data: Dict[str, Any], metrics: Dict[str, Any]) -> Dict[str, Any]:
    """
    SQLãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ©ãƒ¼ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰Liquid Clusteringåˆ†æã«å¿…è¦ãªãƒ‡ãƒ¼ã‚¿ã‚’æŠ½å‡ºï¼ˆLLMåˆ†æç”¨ï¼‰
    """
    extracted_data = {
        "filter_columns": [],
        "join_columns": [],
        "groupby_columns": [],
        "aggregate_columns": [],
        "table_info": {},
        "scan_nodes": [],
        "join_nodes": [],
        "filter_nodes": [],
        "metadata_summary": {}
    }
    
    print(f"ğŸ” Liquid Clusteringåˆ†æç”¨ãƒ‡ãƒ¼ã‚¿æŠ½å‡ºé–‹å§‹")
    
    # ãƒ‡ãƒ¼ã‚¿å½¢å¼ã‚’ç¢ºèª
    data_format = metrics.get('data_format', '')
    if data_format == 'sql_query_summary':
        print("ğŸ“Š SQLã‚¯ã‚¨ãƒªã‚µãƒãƒªãƒ¼å½¢å¼: åˆ¶é™ä»˜ãã®Liquid Clusteringåˆ†æ")
        # test2.jsonå½¢å¼ã®å ´åˆã¯åˆ¶é™ä»˜ãã®åˆ†æã‚’è¡Œã†
        query_info = metrics.get('query_info', {})
        query_text = query_info.get('query_text', '')
        
        # ãƒ¡ãƒˆãƒªã‚¯ã‚¹æƒ…å ±ã®ã¿ã‹ã‚‰åŸºæœ¬çš„ãªãƒ†ãƒ¼ãƒ–ãƒ«æƒ…å ±ã‚’ç”Ÿæˆ
        # test2.jsonå½¢å¼ã§ã¯ planMetadatas ãŒç©ºã®ãŸã‚ã€graphs metadata ã¯åˆ©ç”¨ä¸å¯
        # ãƒ¡ãƒˆãƒªã‚¯ã‚¹é‡è¦–ã®ã‚¢ãƒ—ãƒ­ãƒ¼ãƒã§ãƒœãƒˆãƒ«ãƒãƒƒã‚¯åˆ†æã‚’è¡Œã†
        
        # å…¨ä½“çš„ãªãƒ•ã‚£ãƒ«ã‚¿ç‡æƒ…å ±ã‚’è¨ˆç®—
        overall_filter_rate = calculate_filter_rate_percentage(overall_metrics, metrics)
        read_bytes = overall_metrics.get('read_bytes', 0)
        read_gb = read_bytes / (1024**3) if read_bytes > 0 else 0
        
        # ãƒ—ãƒ«ãƒ¼ãƒ³é‡ã‚’æ¨å®šï¼ˆãƒ•ã‚£ãƒ«ã‚¿ç‡ã‹ã‚‰é€†ç®—ï¼‰
        if overall_filter_rate > 0 and read_bytes > 0:
            pruned_bytes = (read_bytes * overall_filter_rate) / (1 - overall_filter_rate)
            pruned_gb = pruned_bytes / (1024**3)
        else:
            pruned_bytes = 0
            pruned_gb = 0
        
        extracted_data["table_info"]["metrics_summary"] = {
            "node_name": "Metrics-Based Analysis",
            "node_tag": "QUERY_SUMMARY", 
            "node_id": "summary",
            "files_count": overall_metrics.get('read_files_count', 0),
            "partitions_count": overall_metrics.get('read_partitions_count', 0),
            "data_size_gb": read_gb,
            "rows_read": overall_metrics.get('rows_read_count', 0),
            "rows_produced": overall_metrics.get('rows_produced_count', 0),
            "data_selectivity": overall_filter_rate,
            "avg_file_size_mb": (overall_metrics.get('read_bytes', 0) / 1024 / 1024) / max(overall_metrics.get('read_files_count', 1), 1),
            "avg_partition_size_mb": (overall_metrics.get('read_bytes', 0) / 1024 / 1024) / max(overall_metrics.get('read_partitions_count', 1), 1),
            "note": "è©³ç´°ãªãƒ†ãƒ¼ãƒ–ãƒ«æƒ…å ±ã¯SQLã‚¯ã‚¨ãƒªã‚µãƒãƒªãƒ¼å½¢å¼ã§ã¯åˆ©ç”¨ä¸å¯ã€‚ãƒ¡ãƒˆãƒªã‚¯ã‚¹ãƒ™ãƒ¼ã‚¹åˆ†æã‚’å®Ÿè¡Œã€‚",
            "current_clustering_keys": [],  # ç¾åœ¨ã®ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°ã‚­ãƒ¼
            "filter_info": {  # ãƒ•ã‚£ãƒ«ã‚¿ç‡æƒ…å ±ã‚’è¿½åŠ 
                "filter_rate": overall_filter_rate,
                "files_read_bytes": read_bytes,
                "files_pruned_bytes": pruned_bytes,
                "has_filter_metrics": read_bytes > 0
            }
        }
        
        # ã‚µãƒãƒªãƒ¼ãƒãƒ¼ãƒ‰ã®æƒ…å ±ã‚’ä½¿ç”¨
        for node in metrics.get('node_metrics', []):
            node_name = node.get('name', '')
            extracted_data["scan_nodes"].append({
                "name": node_name,
                "type": node.get('tag', ''),
                "rows": node.get('key_metrics', {}).get('rowsNum', 0),
                "duration_ms": node.get('key_metrics', {}).get('durationMs', 0),
                "node_id": node.get('node_id', '')
            })
        
        # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚µãƒãƒªãƒ¼ï¼ˆåˆ¶é™ä»˜ãï¼‰
        view_count = sum(1 for table in extracted_data["table_info"].values() if table.get('is_view', False))
        actual_table_count = sum(len(table.get('underlying_tables', [])) for table in extracted_data["table_info"].values())
        
        extracted_data["metadata_summary"] = {
            "total_nodes": len(metrics.get('node_metrics', [])),
            "total_graphs": 0,
            "filter_expressions_count": 0,
            "join_expressions_count": 0,
            "groupby_expressions_count": 0,
            "aggregate_expressions_count": 0,
            "tables_identified": len(extracted_data["table_info"]),
            "views_identified": view_count,
            "underlying_tables_estimated": actual_table_count,
            "scan_nodes_count": len(extracted_data["scan_nodes"]),
            "join_nodes_count": 0,
            "filter_nodes_count": 0,
            "analysis_limitation": "SQLã‚¯ã‚¨ãƒªã‚µãƒãƒªãƒ¼å½¢å¼ã®ãŸã‚è©³ç´°åˆ†æãŒåˆ¶é™ã•ã‚Œã¦ã„ã¾ã™"
        }
        
        print(f"âœ… åˆ¶é™ä»˜ããƒ‡ãƒ¼ã‚¿æŠ½å‡ºå®Œäº†: {extracted_data['metadata_summary']}")
        
        # ãƒ“ãƒ¥ãƒ¼æƒ…å ±ã®è©³ç´°è¡¨ç¤º
        if view_count > 0:
            print(f"ğŸ” ãƒ“ãƒ¥ãƒ¼æƒ…å ±ã®è©³ç´°:")
            for table_name, table_info in extracted_data["table_info"].items():
                if table_info.get('is_view', False):
                    print(f"  ğŸ“Š ãƒ“ãƒ¥ãƒ¼: {table_name}")
                    print(f"     ã‚¨ã‚¤ãƒªã‚¢ã‚¹: {table_info.get('alias', 'ãªã—')}")
                    print(f"     ãƒ†ãƒ¼ãƒ–ãƒ«ç¨®åˆ¥: {table_info.get('table_type', 'unknown')}")
                    
                    underlying_tables = table_info.get('underlying_tables', [])
                    if underlying_tables:
                        print(f"     æ¨å®šå®Ÿãƒ†ãƒ¼ãƒ–ãƒ«æ•°: {len(underlying_tables)}")
                        for i, underlying_table in enumerate(underlying_tables[:3]):  # æœ€å¤§3å€‹è¡¨ç¤º
                            print(f"       - {underlying_table}")
                        if len(underlying_tables) > 3:
                            print(f"       ... ãŠã‚ˆã³ {len(underlying_tables) - 3} å€‹ã®è¿½åŠ ãƒ†ãƒ¼ãƒ–ãƒ«")
                    print()
        
        return extracted_data
    
    # é€šå¸¸ã®SQLãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ©ãƒ¼å½¢å¼ã®å‡¦ç†
    # ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ©ãƒ¼ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰å®Ÿè¡Œã‚°ãƒ©ãƒ•æƒ…å ±ã‚’å–å¾—ï¼ˆè¤‡æ•°ã‚°ãƒ©ãƒ•å¯¾å¿œï¼‰
    graphs = profiler_data.get('graphs', [])
    if not graphs:
        print("âš ï¸ ã‚°ãƒ©ãƒ•ãƒ‡ãƒ¼ã‚¿ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
        return extracted_data

    # ã™ã¹ã¦ã®ã‚°ãƒ©ãƒ•ã‹ã‚‰ãƒãƒ¼ãƒ‰ã‚’åé›†
    all_nodes = []
    for graph_index, graph in enumerate(graphs):
        nodes = graph.get('nodes', [])
        for node in nodes:
            node['graph_index'] = graph_index
            all_nodes.append(node)
    
    print(f"ğŸ” {len(graphs)}å€‹ã®ã‚°ãƒ©ãƒ•ã‹ã‚‰{len(all_nodes)}å€‹ã®ãƒãƒ¼ãƒ‰ã‚’å‡¦ç†ä¸­")

    # ãƒãƒ¼ãƒ‰ã‹ã‚‰ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿æƒ…å ±ã‚’æŠ½å‡º
    for node in all_nodes:
        node_name = node.get('name', '')
        node_tag = node.get('tag', '')
        node_metadata = node.get('metadata', [])
        
        # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰é‡è¦ãªæƒ…å ±ã‚’æŠ½å‡º
        for metadata_item in node_metadata:
            key = metadata_item.get('key', '')
            values = metadata_item.get('values', [])
            value = metadata_item.get('value', '')
            
            # ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼æ¡ä»¶ã®æŠ½å‡º
            if key == 'FILTERS' and values:
                for filter_expr in values:
                    extracted_data["filter_columns"].append({
                        "expression": filter_expr,
                        "node_name": node_name,
                        "node_tag": node_tag
                    })
            
            # GROUP BYå¼ã®æŠ½å‡º
            elif key == 'GROUPING_EXPRESSIONS' and values:
                for group_expr in values:
                    extracted_data["groupby_columns"].append({
                        "expression": group_expr,
                        "node_name": node_name,
                        "node_tag": node_tag
                    })
            
            # JOINæ¡ä»¶ã®æŠ½å‡º
            elif key in ['LEFT_KEYS', 'RIGHT_KEYS'] and values:
                for join_key in values:
                    extracted_data["join_columns"].append({
                        "expression": join_key,
                        "key_type": key,
                        "node_name": node_name,
                        "node_tag": node_tag
                    })
            
            # é›†ç´„é–¢æ•°ã®æŠ½å‡º
            elif key == 'AGGREGATE_EXPRESSIONS' and values:
                for agg_expr in values:
                    extracted_data["aggregate_columns"].append({
                        "expression": agg_expr,
                        "node_name": node_name,
                        "node_tag": node_tag
                    })
            
            # ãƒ†ãƒ¼ãƒ–ãƒ«æƒ…å ±ã®æŠ½å‡º
            elif key == 'SCAN_IDENTIFIER':
                table_name = value
                extracted_data["table_info"][table_name] = {
                    "node_name": node_name,
                    "node_tag": node_tag,
                    "node_id": node.get('id', ''),
                    "current_clustering_keys": []  # ç¾åœ¨ã®ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°ã‚­ãƒ¼ã‚’è¿½åŠ 
                }

    # ãƒãƒ¼ãƒ‰ã‚¿ã‚¤ãƒ—åˆ¥ã®åˆ†é¡ã¨ç¾åœ¨ã®ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°ã‚­ãƒ¼æƒ…å ±ã®é–¢é€£ä»˜ã‘
    node_metrics = metrics.get('node_metrics', [])
    for node in node_metrics:
        node_name = node.get('name', '')
        node_type = node.get('tag', '')
        key_metrics = node.get('key_metrics', {})
        
        if any(keyword in node_name.upper() for keyword in ['SCAN', 'FILESCAN', 'PARQUET', 'DELTA']):
            # ã‚¹ã‚­ãƒ£ãƒ³ãƒãƒ¼ãƒ‰ã‹ã‚‰ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°ã‚­ãƒ¼ã‚’æŠ½å‡º
            cluster_attributes = extract_cluster_attributes(node)
            
            # ãƒãƒ¼ãƒ‰ã‹ã‚‰ãƒ†ãƒ¼ãƒ–ãƒ«åã‚’æŠ½å‡ºã—ã¦ãƒãƒƒãƒ”ãƒ³ã‚°
            node_metadata = node.get('metadata', [])
            table_name_from_node = None
            
            for meta in node_metadata:
                meta_key = meta.get('key', '')
                meta_value = meta.get('value', '')
                if meta_key == 'SCAN_IDENTIFIER' and meta_value:
                    table_name_from_node = meta_value
                    break
            
            # ãƒ†ãƒ¼ãƒ–ãƒ«åãŒè¦‹ã¤ã‹ã‚‰ãªã„å ´åˆã¯ãƒãƒ¼ãƒ‰åã‹ã‚‰æ¨æ¸¬
            if not table_name_from_node:
                import re
                table_patterns = [
                    r'[Ss]can\s+([a-zA-Z_][a-zA-Z0-9_.]*[a-zA-Z0-9_])',
                    r'([a-zA-Z_][a-zA-Z0-9_]*\.)+([a-zA-Z_][a-zA-Z0-9_]*)',
                ]
                
                for pattern in table_patterns:
                    match = re.search(pattern, node_name)
                    if match:
                        if '.' in match.group(0):
                            table_name_from_node = match.group(0)
                        else:
                            table_name_from_node = match.group(1) if match.lastindex and match.lastindex >= 1 else match.group(0)
                        break
            
            # ãƒ•ã‚£ãƒ«ã‚¿ç‡æƒ…å ±ã‚’è¨ˆç®—
            filter_result = calculate_filter_rate(node)
            filter_rate_info = {
                "filter_rate": filter_result.get("filter_rate", 0),
                "files_read_bytes": filter_result.get("files_read_bytes", 0),
                "files_pruned_bytes": filter_result.get("files_pruned_bytes", 0),
                "has_filter_metrics": filter_result.get("has_filter_metrics", False)
            }
            
            # ãƒ†ãƒ¼ãƒ–ãƒ«æƒ…å ±ã«ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°ã‚­ãƒ¼ã¨ãƒ•ã‚£ãƒ«ã‚¿ç‡ã‚’è¿½åŠ 
            if table_name_from_node:
                # æ—¢å­˜ã®ãƒ†ãƒ¼ãƒ–ãƒ«æƒ…å ±ã‚’æ›´æ–°
                if table_name_from_node in extracted_data["table_info"]:
                    extracted_data["table_info"][table_name_from_node]["current_clustering_keys"] = cluster_attributes
                    extracted_data["table_info"][table_name_from_node]["filter_info"] = filter_rate_info
                else:
                    # æ–°ã—ã„ãƒ†ãƒ¼ãƒ–ãƒ«æƒ…å ±ã‚’ä½œæˆ
                    extracted_data["table_info"][table_name_from_node] = {
                        "node_name": node_name,
                        "node_tag": node_type,
                        "node_id": node.get('node_id', ''),
                        "current_clustering_keys": cluster_attributes,
                        "filter_info": filter_rate_info
                    }
            
            extracted_data["scan_nodes"].append({
                "name": node_name,
                "type": node_type,
                "rows": key_metrics.get('rowsNum', 0),
                "duration_ms": key_metrics.get('durationMs', 0),
                "node_id": node.get('node_id', ''),
                "table_name": table_name_from_node,
                "current_clustering_keys": cluster_attributes
            })
        elif any(keyword in node_name.upper() for keyword in ['JOIN', 'HASH']):
            extracted_data["join_nodes"].append({
                "name": node_name,
                "type": node_type,
                "duration_ms": key_metrics.get('durationMs', 0),
                "node_id": node.get('node_id', '')
            })
        elif any(keyword in node_name.upper() for keyword in ['FILTER']):
            extracted_data["filter_nodes"].append({
                "name": node_name,
                "type": node_type,
                "duration_ms": key_metrics.get('durationMs', 0),
                "node_id": node.get('node_id', '')
            })

    # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚µãƒãƒªãƒ¼
    extracted_data["metadata_summary"] = {
        "total_nodes": len(all_nodes),
        "total_graphs": len(graphs),
        "filter_expressions_count": len(extracted_data["filter_columns"]),
        "join_expressions_count": len(extracted_data["join_columns"]),
        "groupby_expressions_count": len(extracted_data["groupby_columns"]),
        "aggregate_expressions_count": len(extracted_data["aggregate_columns"]),
        "tables_identified": len(extracted_data["table_info"]),
        "scan_nodes_count": len(extracted_data["scan_nodes"]),
        "join_nodes_count": len(extracted_data["join_nodes"]),
        "filter_nodes_count": len(extracted_data["filter_nodes"])
    }
    
    print(f"âœ… ãƒ‡ãƒ¼ã‚¿æŠ½å‡ºå®Œäº†: {extracted_data['metadata_summary']}")
    
    # ç¾åœ¨ã®ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°ã‚­ãƒ¼æƒ…å ±ã®è©³ç´°è¡¨ç¤º
    clustering_info_found = False
    for table_name, table_info in extracted_data["table_info"].items():
        current_keys = table_info.get('current_clustering_keys', [])
        if current_keys:
            if not clustering_info_found:
                print(f"ğŸ” ç¾åœ¨ã®ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°ã‚­ãƒ¼æƒ…å ±:")
                clustering_info_found = True
            print(f"  ğŸ“Š ãƒ†ãƒ¼ãƒ–ãƒ«: {table_name}")
            print(f"     ç¾åœ¨ã®ã‚­ãƒ¼: {', '.join(current_keys)}")
            print(f"     ãƒãƒ¼ãƒ‰: {table_info.get('node_name', 'Unknown')}")
            print()
    
    if not clustering_info_found:
        print(f"â„¹ï¸ ç¾åœ¨ã®ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°ã‚­ãƒ¼ã¯æ¤œå‡ºã•ã‚Œã¾ã›ã‚“ã§ã—ãŸ")
    
    return extracted_data

def analyze_liquid_clustering_opportunities(profiler_data: Dict[str, Any], metrics: Dict[str, Any]) -> Dict[str, Any]:
    """
    LLMã‚’ä½¿ç”¨ã—ã¦Liquid Clusteringã®åˆ†æã¨æ¨å¥¨äº‹é …ã‚’ç”Ÿæˆ
    """
    print(f"ğŸ¤– LLMã«ã‚ˆã‚‹Liquid Clusteringåˆ†æã‚’é–‹å§‹")
    
    # åŸºæœ¬ãƒ‡ãƒ¼ã‚¿ã®æŠ½å‡º
    extracted_data = extract_liquid_clustering_data(profiler_data, metrics)
    
    # LLMåˆ†æç”¨ã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆä½œæˆ
    overall_metrics = metrics.get('overall_metrics', {})
    bottleneck_indicators = metrics.get('bottleneck_indicators', {})
    
    # ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æ¦‚è¦
    total_time_sec = overall_metrics.get('total_time_ms', 0) / 1000
    read_gb = overall_metrics.get('read_bytes', 0) / 1024 / 1024 / 1024
    rows_produced = overall_metrics.get('rows_produced_count', 0)
    rows_read = overall_metrics.get('rows_read_count', 0)
    
    # æŠ½å‡ºã—ãŸã‚«ãƒ©ãƒ æƒ…å ±ã®ã‚µãƒãƒªãƒ¼ä½œæˆï¼ˆä¸Šä½5å€‹ã¾ã§ï¼‰
    filter_summary = []
    for i, item in enumerate(extracted_data["filter_columns"][:5]):
        filter_summary.append(f"  {i+1}. {item['expression']} (ãƒãƒ¼ãƒ‰: {item['node_name']})")
    
    join_summary = []
    for i, item in enumerate(extracted_data["join_columns"][:5]):
        join_summary.append(f"  {i+1}. {item['expression']} (ã‚¿ã‚¤ãƒ—: {item['key_type']}, ãƒãƒ¼ãƒ‰: {item['node_name']})")
    
    groupby_summary = []
    for i, item in enumerate(extracted_data["groupby_columns"][:5]):
        groupby_summary.append(f"  {i+1}. {item['expression']} (ãƒãƒ¼ãƒ‰: {item['node_name']})")
    
    aggregate_summary = []
    for i, item in enumerate(extracted_data["aggregate_columns"][:5]):
        aggregate_summary.append(f"  {i+1}. {item['expression']} (ãƒãƒ¼ãƒ‰: {item['node_name']})")
    
    # ãƒ†ãƒ¼ãƒ–ãƒ«æƒ…å ±ã®ã‚µãƒãƒªãƒ¼ï¼ˆç¾åœ¨ã®ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°ã‚­ãƒ¼æƒ…å ±ã¨ãƒ•ã‚£ãƒ«ã‚¿ç‡ã‚’å«ã‚€ï¼‰
    table_summary = []
    for table_name, table_info in extracted_data["table_info"].items():
        current_keys = table_info.get('current_clustering_keys', [])
        current_keys_str = ', '.join(current_keys) if current_keys else 'è¨­å®šãªã—'
        
        # ãƒ•ã‚£ãƒ«ã‚¿ç‡æƒ…å ±ã‚’è¿½åŠ 
        filter_info = table_info.get('filter_info', {})
        filter_rate = filter_info.get('filter_rate', 0)
        files_read_bytes = filter_info.get('files_read_bytes', 0)
        files_pruned_bytes = filter_info.get('files_pruned_bytes', 0)
        
        # ãƒã‚¤ãƒˆæ•°ã‚’GBå˜ä½ã«å¤‰æ›
        read_gb = files_read_bytes / (1024**3) if files_read_bytes > 0 else 0
        pruned_gb = files_pruned_bytes / (1024**3) if files_pruned_bytes > 0 else 0
        
        if filter_info.get('has_filter_metrics', False):
            filter_str = f", ãƒ•ã‚£ãƒ«ã‚¿ç‡: {filter_rate*100:.1f}% (èª­ã¿è¾¼ã¿: {read_gb:.2f}GB, ãƒ—ãƒ«ãƒ¼ãƒ³: {pruned_gb:.2f}GB)"
        else:
            filter_str = ", ãƒ•ã‚£ãƒ«ã‚¿ç‡: æƒ…å ±ãªã—"
        
        table_summary.append(f"  - {table_name} (ãƒãƒ¼ãƒ‰: {table_info['node_name']}, ç¾åœ¨ã®ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°ã‚­ãƒ¼: {current_keys_str}{filter_str})")
    
    # ã‚¹ã‚­ãƒ£ãƒ³ãƒãƒ¼ãƒ‰ã®ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æƒ…å ±
    scan_performance = []
    for scan in extracted_data["scan_nodes"]:
        efficiency = scan['rows'] / max(scan['duration_ms'], 1)
        scan_performance.append(f"  - {scan['name']}: {scan['rows']:,}è¡Œ, {scan['duration_ms']:,}ms, åŠ¹ç‡={efficiency:.1f}è¡Œ/ms")

    clustering_prompt = f"""
ã‚ãªãŸã¯Databricksã®Liquid Clusteringå°‚é–€å®¶ã§ã™ã€‚ä»¥ä¸‹ã®SQLãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ©ãƒ¼ãƒ‡ãƒ¼ã‚¿ã‚’åˆ†æã—ã€æœ€é©ãªLiquid Clusteringã®æ¨å¥¨äº‹é …ã‚’æç¤ºã—ã¦ãã ã•ã„ã€‚

ã€ã‚¯ã‚¨ãƒªãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æ¦‚è¦ã€‘
- å®Ÿè¡Œæ™‚é–“: {total_time_sec:.1f}ç§’
- ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿: {read_gb:.2f}GB
- å‡ºåŠ›è¡Œæ•°: {rows_produced:,}è¡Œ
- èª­ã¿è¾¼ã¿è¡Œæ•°: {rows_read:,}è¡Œ
- ãƒ•ã‚£ãƒ«ã‚¿ç‡: {calculate_filter_rate_percentage(overall_metrics, metrics):.4f}

ã€æŠ½å‡ºã•ã‚ŒãŸã‚«ãƒ©ãƒ ä½¿ç”¨ãƒ‘ã‚¿ãƒ¼ãƒ³ã€‘

ğŸ” ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼æ¡ä»¶ ({len(extracted_data["filter_columns"])}å€‹):
{chr(10).join(filter_summary)}

ğŸ”— JOINæ¡ä»¶ ({len(extracted_data["join_columns"])}å€‹):
{chr(10).join(join_summary)}

ğŸ“Š GROUP BY ({len(extracted_data["groupby_columns"])}å€‹):
{chr(10).join(groupby_summary)}

ğŸ“ˆ é›†ç´„é–¢æ•° ({len(extracted_data["aggregate_columns"])}å€‹):
{chr(10).join(aggregate_summary)}

ã€ãƒ†ãƒ¼ãƒ–ãƒ«æƒ…å ±ã€‘
ãƒ†ãƒ¼ãƒ–ãƒ«æ•°: {len(extracted_data["table_info"])}å€‹
{chr(10).join(table_summary)}

ã€ã‚¹ã‚­ãƒ£ãƒ³ãƒãƒ¼ãƒ‰ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã€‘
{chr(10).join(scan_performance)}

ã€ç¾åœ¨ã®ãƒœãƒˆãƒ«ãƒãƒƒã‚¯æŒ‡æ¨™ã€‘
- ã‚¹ãƒ”ãƒ«ç™ºç”Ÿ: {'ã‚ã‚Š' if bottleneck_indicators.get('has_spill', False) else 'ãªã—'}
- ã‚·ãƒ£ãƒƒãƒ•ãƒ«æ“ä½œ: {bottleneck_indicators.get('shuffle_operations_count', 0)}å›
- ä½ä¸¦åˆ—åº¦ã‚¹ãƒ†ãƒ¼ã‚¸: {bottleneck_indicators.get('low_parallelism_stages_count', 0)}å€‹

ã€åˆ†æè¦æ±‚ã€‘
1. å„ãƒ†ãƒ¼ãƒ–ãƒ«ã«å¯¾ã™ã‚‹æœ€é©ãªLiquid Clusteringã‚«ãƒ©ãƒ ã®æ¨å¥¨ï¼ˆæœ€å¤§4ã‚«ãƒ©ãƒ ï¼‰
2. ã‚«ãƒ©ãƒ é¸å®šã®æ ¹æ‹ ï¼ˆãƒ•ã‚£ãƒ«ã‚¿ãƒ¼ã€JOINã€GROUP BYã§ã®ä½¿ç”¨é »åº¦ã¨é‡è¦åº¦ï¼‰
3. ç¾åœ¨ã®ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°ã‚­ãƒ¼ã¨æ¨å¥¨ã‚­ãƒ¼ã®æ¯”è¼ƒåˆ†æ
4. å®Ÿè£…å„ªå…ˆé †ä½ï¼ˆãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹å‘ä¸ŠåŠ¹æœé †ï¼‰
5. å…·ä½“çš„ãªSQLå®Ÿè£…ä¾‹ï¼ˆæ­£ã—ã„Databricks SQLæ§‹æ–‡ã€ç¾åœ¨ã®ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°ã‚­ãƒ¼æƒ…å ±ã‚’ã‚³ãƒ¡ãƒ³ãƒˆã«æ˜è¨˜ï¼‰
6. æœŸå¾…ã•ã‚Œã‚‹ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æ”¹å–„åŠ¹æœï¼ˆæ•°å€¤ã§ï¼‰

ã€åˆ¶ç´„äº‹é …ã€‘
- ãƒ‘ãƒ¼ãƒ†ã‚£ã‚·ãƒ§ãƒ‹ãƒ³ã‚°ã‚„ZORDERã¯ææ¡ˆã—ãªã„ï¼ˆLiquid Clusteringã®ã¿ï¼‰
- æ­£ã—ã„Databricks SQLæ§‹æ–‡ã‚’ä½¿ç”¨ï¼š
  * æ–°è¦ãƒ†ãƒ¼ãƒ–ãƒ«: CREATE TABLE ... CLUSTER BY (col1, col2, ...)
  * æ—¢å­˜ãƒ†ãƒ¼ãƒ–ãƒ«: ALTER TABLE table_name CLUSTER BY (col1, col2, ...)
- æœ€å¤§4ã‚«ãƒ©ãƒ ã¾ã§ã®æ¨å¥¨
- ãƒ‡ãƒ¼ã‚¿ã‚¹ã‚­ãƒ¥ãƒ¼ã‚„ä¸¦åˆ—åº¦ã®å•é¡Œã‚‚è€ƒæ…®

ã€ğŸš¨ é‡è¦ãªLiquid Clusteringä»•æ§˜ã®ç†è§£ã€‘
- **ã‚«ãƒ©ãƒ é †åº**: Liquid Clusteringã§ã¯ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°ã‚­ãƒ¼ã®é †åºå¤‰æ›´ã¯ã€Œãƒãƒ¼ãƒ‰ãƒ¬ãƒ™ãƒ«ã®ãƒ‡ãƒ¼ã‚¿å±€æ‰€æ€§ã€ã«å½±éŸ¿ã—ã¾ã›ã‚“
- **å®Ÿéš›ã®æ”¹å–„åŠ¹æœ**: å‘ä¸Šã™ã‚‹ã®ã¯ã€Œã‚¹ã‚­ãƒ£ãƒ³åŠ¹ç‡ã€ã€Œãƒ•ã‚¡ã‚¤ãƒ«ãƒ—ãƒ«ãƒ¼ãƒ‹ãƒ³ã‚°åŠ¹æœã€ã€Œã‚¯ã‚¨ãƒªãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã€ã§ã™
- **æŠ€è¡“çš„ç‰¹æ€§**: CLUSTER BYå†…ã®ã‚«ãƒ©ãƒ é †åºã¯ä»»æ„ã§ã‚ã‚Šã€(col1, col2, col3) ã¨ (col3, col1, col2) ã¯åŒç­‰ã®ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹

ã€ğŸš¨ çµ¶å¯¾ã«ä½¿ç”¨ç¦æ­¢ã®èª¤ã£ãŸè¡¨ç¾ã€‘
âŒ ã€Œé †åºã‚’å¤‰æ›´ã™ã‚‹ã“ã¨ã§ãƒ‡ãƒ¼ã‚¿å±€æ‰€æ€§ã‚’å‘ä¸Šã€
âŒ ã€Œã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°ã‚­ãƒ¼é †åºã§ãƒ‡ãƒ¼ã‚¿å±€æ‰€æ€§æ”¹å–„ã€  
âŒ ã€Œé †åºå¤‰æ›´ã«ã‚ˆã‚‹ãƒãƒ¼ãƒ‰ãƒ¬ãƒ™ãƒ«ãƒ‡ãƒ¼ã‚¿é…ç½®æœ€é©åŒ–ã€
âœ… ã€Œé †åºå¤‰æ›´ã«ã‚ˆã‚‹å…·ä½“çš„ãªæ”¹å–„åŠ¹æœãªã—ï¼ˆLiquid Clusteringä»•æ§˜ï¼‰ã€
âœ… ã€Œã‚¹ã‚­ãƒ£ãƒ³åŠ¹ç‡ã¨ãƒ•ã‚¡ã‚¤ãƒ«ãƒ—ãƒ«ãƒ¼ãƒ‹ãƒ³ã‚°åŠ¹æœã®å‘ä¸Šã€
âœ… ã€ŒWHEREå¥ã‚„JOINæ¡ä»¶ã®ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æ”¹å–„ã€

ç°¡æ½”ã§å®Ÿè·µçš„ãªåˆ†æçµæœã‚’æ—¥æœ¬èªã§æä¾›ã—ã¦ãã ã•ã„ã€‚

ã€é‡è¦ãªå‡ºåŠ›å½¢å¼æŒ‡ç¤ºã€‘
å„ãƒ†ãƒ¼ãƒ–ãƒ«ã®åˆ†æã§ã¯ã€å¿…ãšä»¥ä¸‹ã®å½¢å¼ã§ç¾åœ¨ã®ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°ã‚­ãƒ¼æƒ…å ±ã¨ãƒ•ã‚£ãƒ«ã‚¿ç‡ã‚’å«ã‚ã¦ãã ã•ã„ï¼š

## ãƒ†ãƒ¼ãƒ–ãƒ«åˆ¥æ¨å¥¨ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°

### 1. [ãƒ†ãƒ¼ãƒ–ãƒ«å] ãƒ†ãƒ¼ãƒ–ãƒ« (æœ€å„ªå…ˆ/é«˜å„ªå…ˆåº¦/ä¸­å„ªå…ˆåº¦)
**ç¾åœ¨ã®ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°ã‚­ãƒ¼**: [ç¾åœ¨è¨­å®šã•ã‚Œã¦ã„ã‚‹ã‚­ãƒ¼ ã¾ãŸã¯ "è¨­å®šãªã—"]
**æ¨å¥¨ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°ã‚«ãƒ©ãƒ **: [æ¨å¥¨ã‚«ãƒ©ãƒ 1], [æ¨å¥¨ã‚«ãƒ©ãƒ 2], [æ¨å¥¨ã‚«ãƒ©ãƒ 3], [æ¨å¥¨ã‚«ãƒ©ãƒ 4]

```sql
ALTER TABLE [ãƒ†ãƒ¼ãƒ–ãƒ«å] 
CLUSTER BY ([æ¨å¥¨ã‚«ãƒ©ãƒ 1], [æ¨å¥¨ã‚«ãƒ©ãƒ 2], [æ¨å¥¨ã‚«ãƒ©ãƒ 3], [æ¨å¥¨ã‚«ãƒ©ãƒ 4]);
OPTIMIZE [ãƒ†ãƒ¼ãƒ–ãƒ«å] FULL;
```

**é¸å®šæ ¹æ‹ **:
- [ã‚«ãƒ©ãƒ 1]: [ä½¿ç”¨ãƒ‘ã‚¿ãƒ¼ãƒ³ã¨é‡è¦åº¦]
- [ã‚«ãƒ©ãƒ 2]: [ä½¿ç”¨ãƒ‘ã‚¿ãƒ¼ãƒ³ã¨é‡è¦åº¦]
- [ä»¥ä¸‹åŒæ§˜...]
- ğŸš¨é‡è¦: ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°ã‚­ãƒ¼é †åºå¤‰æ›´ã¯ãƒãƒ¼ãƒ‰ãƒ¬ãƒ™ãƒ«ã®ãƒ‡ãƒ¼ã‚¿å±€æ‰€æ€§ã«å½±éŸ¿ã—ãªã„ï¼ˆLiquid Clusteringä»•æ§˜ï¼‰
- âœ…æ”¹å–„åŠ¹æœ: ã‚¹ã‚­ãƒ£ãƒ³åŠ¹ç‡ã¨ãƒ•ã‚¡ã‚¤ãƒ«ãƒ—ãƒ«ãƒ¼ãƒ‹ãƒ³ã‚°åŠ¹æœã®å‘ä¸Šï¼ˆé †åºç„¡é–¢ä¿‚ï¼‰

**æœŸå¾…ã•ã‚Œã‚‹æ”¹å–„åŠ¹æœ**:
- [å…·ä½“çš„ãªæ•°å€¤ã§ã®æ”¹å–„è¦‹è¾¼ã¿]

**ãƒ•ã‚£ãƒ«ã‚¿ç‡**: [X.X]% (èª­ã¿è¾¼ã¿: [XX.XX]GB, ãƒ—ãƒ«ãƒ¼ãƒ³: [XX.XX]GB)

ã“ã®å½¢å¼ã«ã‚ˆã‚Šã€ç¾åœ¨ã®è¨­å®šã€æ¨å¥¨è¨­å®šã€ãŠã‚ˆã³å„ãƒ†ãƒ¼ãƒ–ãƒ«ã®ç¾åœ¨ã®ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°åŠ¹ç‡ã‚’æ˜ç¢ºã«è¡¨ç¤ºã—ã¦ãã ã•ã„ã€‚ãƒ•ã‚£ãƒ«ã‚¿ç‡æƒ…å ±ã¯ä¸Šè¨˜ãƒ†ãƒ¼ãƒ–ãƒ«æƒ…å ±ã‹ã‚‰æ­£ç¢ºãªæ•°å€¤ã‚’ä½¿ç”¨ã—ã¦ãã ã•ã„ã€‚
"""

    try:
        # LLMåˆ†æã®å®Ÿè¡Œ
        provider = LLM_CONFIG["provider"]
        print(f"ğŸ¤– {provider}ã‚’ä½¿ç”¨ã—ã¦Liquid Clusteringåˆ†æä¸­...")
        
        if provider == "databricks":
            llm_analysis = _call_databricks_llm(clustering_prompt)
        elif provider == "openai":
            llm_analysis = _call_openai_llm(clustering_prompt)
        elif provider == "azure_openai":
            llm_analysis = _call_azure_openai_llm(clustering_prompt)
        elif provider == "anthropic":
            llm_analysis = _call_anthropic_llm(clustering_prompt)
        else:
            llm_analysis = f"âŒ ã‚µãƒãƒ¼ãƒˆã•ã‚Œã¦ã„ãªã„LLMãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼: {provider}"
        
        # åˆ†æçµæœã®æ§‹é€ åŒ–
        clustering_analysis = {
            "llm_analysis": llm_analysis,
            "extracted_data": extracted_data,
            "performance_context": {
                "total_time_sec": total_time_sec,
                "read_gb": read_gb,
                "rows_produced": rows_produced,
                "rows_read": rows_read,
                "data_selectivity": calculate_filter_rate_percentage(overall_metrics, metrics)
            },
            "summary": {
                "analysis_method": "LLM-based",
                "tables_identified": len(extracted_data["table_info"]),
                "total_filter_columns": len(extracted_data["filter_columns"]),
                "total_join_columns": len(extracted_data["join_columns"]),
                "total_groupby_columns": len(extracted_data["groupby_columns"]),
                "total_aggregate_columns": len(extracted_data["aggregate_columns"]),
                "scan_nodes_count": len(extracted_data["scan_nodes"]),
                "llm_provider": provider
            }
        }
        
        print("âœ… LLM Liquid Clusteringåˆ†æå®Œäº†")
        return clustering_analysis
        
    except Exception as e:
        error_msg = f"LLMåˆ†æã‚¨ãƒ©ãƒ¼: {str(e)}"
        print(f"âŒ {error_msg}")
        
        # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: åŸºæœ¬çš„ãªæŠ½å‡ºãƒ‡ãƒ¼ã‚¿ã®ã¿ã‚’è¿”ã™
        return {
            "llm_analysis": f"âŒ LLMåˆ†æã«å¤±æ•—ã—ã¾ã—ãŸ: {error_msg}",
            "extracted_data": extracted_data,
            "summary": {
                "analysis_method": "extraction-only",
                "tables_identified": len(extracted_data["table_info"]),
                "total_filter_columns": len(extracted_data["filter_columns"]),
                "error": error_msg
            }
        }

def save_liquid_clustering_analysis(clustering_analysis: Dict[str, Any], output_dir: str = "/tmp") -> Dict[str, str]:
    """
    Liquid Clusteringåˆ†æçµæœã‚’ãƒ•ã‚¡ã‚¤ãƒ«ã«å‡ºåŠ›
    """
    import os
    import json
    from datetime import datetime
    
    # ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—ä»˜ããƒ•ã‚¡ã‚¤ãƒ«åã®ç”Ÿæˆ
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    
    # å‡ºåŠ›ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹
    json_path = f"{output_dir}/liquid_clustering_analysis_{timestamp}.json"
    markdown_path = f"{output_dir}/liquid_clustering_analysis_{timestamp}.md"
    sql_path = f"{output_dir}/liquid_clustering_implementation_{timestamp}.sql"
    
    file_paths = {}
    
    try:
        # 1. JSONå½¢å¼ã§ã®è©³ç´°ãƒ‡ãƒ¼ã‚¿ä¿å­˜
        # setå‹ã‚’listå‹ã«å¤‰æ›ã—ã¦JSON serializable ã«ã™ã‚‹
        json_data = convert_sets_to_lists(clustering_analysis)
        
        with open(json_path, 'w', encoding='utf-8') as f:
            json.dump(json_data, f, ensure_ascii=False, indent=2)
        
        file_paths['json'] = json_path
        print(f"âœ… JSONå½¢å¼ã®è©³ç´°ãƒ‡ãƒ¼ã‚¿ã‚’ä¿å­˜: {json_path}")
        
        # 2. Markdownå½¢å¼ã§ã®åˆ†æãƒ¬ãƒãƒ¼ãƒˆä¿å­˜
        markdown_content = generate_liquid_clustering_markdown_report(clustering_analysis)
        
        with open(markdown_path, 'w', encoding='utf-8') as f:
            f.write(markdown_content)
        
        file_paths['markdown'] = markdown_path
        print(f"âœ… Markdownå½¢å¼ã®åˆ†æãƒ¬ãƒãƒ¼ãƒˆã‚’ä¿å­˜: {markdown_path}")
        
        # 3. SQLå®Ÿè£…ä¾‹ãƒ•ã‚¡ã‚¤ãƒ«ã®ç”Ÿæˆ
        sql_content = generate_liquid_clustering_sql_implementations(clustering_analysis)
        
        with open(sql_path, 'w', encoding='utf-8') as f:
            f.write(sql_content)
        
        file_paths['sql'] = sql_path
        print(f"âœ… SQLå®Ÿè£…ä¾‹ã‚’ä¿å­˜: {sql_path}")
        
        return file_paths
        
    except Exception as e:
        error_msg = f"ãƒ•ã‚¡ã‚¤ãƒ«å‡ºåŠ›ã‚¨ãƒ©ãƒ¼: {str(e)}"
        print(f"âŒ {error_msg}")
        return {"error": error_msg}

def generate_liquid_clustering_markdown_report(clustering_analysis: Dict[str, Any]) -> str:
    """
    Liquid Clusteringåˆ†æçµæœã®Markdownãƒ¬ãƒãƒ¼ãƒˆã‚’ç”Ÿæˆ
    """
    from datetime import datetime
    
    timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
    
    # åŸºæœ¬æƒ…å ±ã®å–å¾—
    summary = clustering_analysis.get('summary', {})
    performance_context = clustering_analysis.get('performance_context', {})
    extracted_data = clustering_analysis.get('extracted_data', {})
    llm_analysis = clustering_analysis.get('llm_analysis', '')
    
    markdown_content = f"""# Liquid Clustering åˆ†æãƒ¬ãƒãƒ¼ãƒˆ

**ç”Ÿæˆæ—¥æ™‚**: {timestamp}  
**åˆ†ææ–¹æ³•**: {summary.get('analysis_method', 'Unknown')}  
**LLMãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼**: {summary.get('llm_provider', 'Unknown')}

## ğŸ“Š ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æ¦‚è¦

| é …ç›® | å€¤ |
|------|-----|
| å®Ÿè¡Œæ™‚é–“ | {performance_context.get('total_time_sec', 0):.1f}ç§’ |
| ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ | {performance_context.get('read_gb', 0):.2f}GB |
| å‡ºåŠ›è¡Œæ•° | {performance_context.get('rows_produced', 0):,}è¡Œ |
| èª­ã¿è¾¼ã¿è¡Œæ•° | {performance_context.get('rows_read', 0):,}è¡Œ |
| ãƒ•ã‚£ãƒ«ã‚¿ç‡ | {performance_context.get('data_selectivity', 0):.4f} |

## ğŸ” æŠ½å‡ºã•ã‚ŒãŸãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿

### ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼æ¡ä»¶ ({summary.get('total_filter_columns', 0)}å€‹)
"""
    
    # ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼æ¡ä»¶ã®è©³ç´°
    filter_columns = extracted_data.get('filter_columns', [])
    for i, filter_item in enumerate(filter_columns[:10], 1):  # æœ€å¤§10å€‹ã¾ã§è¡¨ç¤º
        markdown_content += f"{i}. `{filter_item.get('expression', '')}` (ãƒãƒ¼ãƒ‰: {filter_item.get('node_name', '')})\n"
    
    if len(filter_columns) > 10:
        markdown_content += f"... ä»– {len(filter_columns) - 10}å€‹\n"
    
    markdown_content += f"""
### JOINæ¡ä»¶ ({summary.get('total_join_columns', 0)}å€‹)
"""
    
    # JOINæ¡ä»¶ã®è©³ç´°
    join_columns = extracted_data.get('join_columns', [])
    for i, join_item in enumerate(join_columns[:10], 1):
        markdown_content += f"{i}. `{join_item.get('expression', '')}` ({join_item.get('key_type', '')})\n"
    
    if len(join_columns) > 10:
        markdown_content += f"... ä»– {len(join_columns) - 10}å€‹\n"
    
    markdown_content += f"""
### GROUP BYæ¡ä»¶ ({summary.get('total_groupby_columns', 0)}å€‹)
"""
    
    # GROUP BYæ¡ä»¶ã®è©³ç´°
    groupby_columns = extracted_data.get('groupby_columns', [])
    for i, groupby_item in enumerate(groupby_columns[:10], 1):
        markdown_content += f"{i}. `{groupby_item.get('expression', '')}` (ãƒãƒ¼ãƒ‰: {groupby_item.get('node_name', '')})\n"
    
    if len(groupby_columns) > 10:
        markdown_content += f"... ä»– {len(groupby_columns) - 10}å€‹\n"
    
    markdown_content += f"""
### é›†ç´„é–¢æ•° ({summary.get('total_aggregate_columns', 0)}å€‹)
"""
    
    # é›†ç´„é–¢æ•°ã®è©³ç´°
    aggregate_columns = extracted_data.get('aggregate_columns', [])
    for i, agg_item in enumerate(aggregate_columns[:10], 1):
        markdown_content += f"{i}. `{agg_item.get('expression', '')}` (ãƒãƒ¼ãƒ‰: {agg_item.get('node_name', '')})\n"
    
    if len(aggregate_columns) > 10:
        markdown_content += f"... ä»– {len(aggregate_columns) - 10}å€‹\n"
    
    markdown_content += f"""
## ğŸ·ï¸ è­˜åˆ¥ã•ã‚ŒãŸãƒ†ãƒ¼ãƒ–ãƒ« ({summary.get('tables_identified', 0)}å€‹)

"""
    
    # ãƒ†ãƒ¼ãƒ–ãƒ«æƒ…å ±ã®è©³ç´°ï¼ˆç¾åœ¨ã®ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°ã‚­ãƒ¼ã‚’å«ã‚€ï¼‰
    table_info = extracted_data.get('table_info', {})
    for table_name, table_details in table_info.items():
        current_keys = table_details.get('current_clustering_keys', [])
        current_keys_str = ', '.join(current_keys) if current_keys else 'è¨­å®šãªã—'
        markdown_content += f"- **{table_name}** (ãƒãƒ¼ãƒ‰: {table_details.get('node_name', '')})\n"
        markdown_content += f"  - ç¾åœ¨ã®ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°ã‚­ãƒ¼: `{current_keys_str}`\n"
    
    markdown_content += f"""
## ğŸ” ã‚¹ã‚­ãƒ£ãƒ³ãƒãƒ¼ãƒ‰åˆ†æ ({summary.get('scan_nodes_count', 0)}å€‹)

"""
    
    # ã‚¹ã‚­ãƒ£ãƒ³ãƒãƒ¼ãƒ‰ã®è©³ç´°
    scan_nodes = extracted_data.get('scan_nodes', [])
    for scan in scan_nodes:
        efficiency = scan.get('rows', 0) / max(scan.get('duration_ms', 1), 1)
        markdown_content += f"- **{scan.get('name', '')}**: {scan.get('rows', 0):,}è¡Œ, {scan.get('duration_ms', 0):,}ms, åŠ¹ç‡={efficiency:.1f}è¡Œ/ms\n"
    
    markdown_content += f"""
## ğŸ¤– LLMåˆ†æçµæœ

{llm_analysis}

## ğŸ“‹ åˆ†æã‚µãƒãƒªãƒ¼

- **åˆ†æå¯¾è±¡ãƒ†ãƒ¼ãƒ–ãƒ«æ•°**: {summary.get('tables_identified', 0)}
- **ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼æ¡ä»¶æ•°**: {summary.get('total_filter_columns', 0)}
- **JOINæ¡ä»¶æ•°**: {summary.get('total_join_columns', 0)}
- **GROUP BYæ¡ä»¶æ•°**: {summary.get('total_groupby_columns', 0)}
- **é›†ç´„é–¢æ•°æ•°**: {summary.get('total_aggregate_columns', 0)}
- **ã‚¹ã‚­ãƒ£ãƒ³ãƒãƒ¼ãƒ‰æ•°**: {summary.get('scan_nodes_count', 0)}

---
*ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆæ™‚åˆ»: {timestamp}*
"""
    
    return markdown_content

def generate_liquid_clustering_sql_implementations(clustering_analysis: Dict[str, Any]) -> str:
    """
    Liquid Clusteringå®Ÿè£…ç”¨ã®SQLä¾‹ã‚’ç”Ÿæˆ
    """
    from datetime import datetime
    
    timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
    
    # åŸºæœ¬æƒ…å ±ã®å–å¾—
    extracted_data = clustering_analysis.get('extracted_data', {})
    table_info = extracted_data.get('table_info', {})
    
    sql_content = f"""-- =====================================================
-- Liquid Clustering å®Ÿè£…SQLä¾‹
-- ç”Ÿæˆæ—¥æ™‚: {timestamp}
-- =====================================================

-- ã€é‡è¦ã€‘
-- ä»¥ä¸‹ã®SQLä¾‹ã¯åˆ†æçµæœã«åŸºã¥ãæ¨å¥¨äº‹é …ã§ã™ã€‚
-- å®Ÿéš›ã®å®Ÿè£…å‰ã«ã€ãƒ†ãƒ¼ãƒ–ãƒ«æ§‹é€ ã‚„ãƒ‡ãƒ¼ã‚¿ç‰¹æ€§ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚

"""
    
    # ãƒ†ãƒ¼ãƒ–ãƒ«ã”ã¨ã®SQLå®Ÿè£…ä¾‹ã‚’ç”Ÿæˆ
    for table_name, table_details in table_info.items():
        # ç¾åœ¨ã®ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°ã‚­ãƒ¼æƒ…å ±ã‚’å–å¾—
        current_keys = table_details.get('current_clustering_keys', [])
        current_keys_str = ', '.join(current_keys) if current_keys else 'è¨­å®šãªã—'
        
        sql_content += f"""
-- =====================================================
-- ãƒ†ãƒ¼ãƒ–ãƒ«: {table_name}
-- ç¾åœ¨ã®ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°ã‚­ãƒ¼: {current_keys_str}
-- =====================================================

-- æ—¢å­˜ãƒ†ãƒ¼ãƒ–ãƒ«ã«Liquid Clusteringã‚’é©ç”¨ã™ã‚‹å ´åˆ:
-- ALTER TABLE {table_name} CLUSTER BY (column1, column2, column3, column4);

-- æ–°è¦ãƒ†ãƒ¼ãƒ–ãƒ«ä½œæˆæ™‚ã«Liquid Clusteringã‚’è¨­å®šã™ã‚‹å ´åˆ:
-- CREATE TABLE {table_name}_clustered
-- CLUSTER BY (column1, column2, column3, column4)
-- AS SELECT * FROM {table_name};

-- Delta Live Tablesã§ã®è¨­å®šä¾‹:
-- @dlt.table(
--   cluster_by=["column1", "column2", "column3", "column4"]
-- )
-- def {table_name.split('.')[-1]}_clustered():
--   return spark.table("{table_name}")

-- ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°çŠ¶æ³ã®ç¢ºèª:
-- DESCRIBE DETAIL {table_name};

-- ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°çµ±è¨ˆã®ç¢ºèª:
-- ANALYZE TABLE {table_name} COMPUTE STATISTICS FOR ALL COLUMNS;

"""
    
    sql_content += f"""
-- =====================================================
-- ä¸€èˆ¬çš„ãªLiquid Clusteringå®Ÿè£…ãƒ‘ã‚¿ãƒ¼ãƒ³
-- =====================================================

-- ãƒ‘ã‚¿ãƒ¼ãƒ³1: ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼é »åº¦ã®é«˜ã„ã‚«ãƒ©ãƒ ã‚’å„ªå…ˆ
-- æ¨å¥¨é †åº: 1) ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼æ¡ä»¶ã‚«ãƒ©ãƒ  2) JOINæ¡ä»¶ã‚«ãƒ©ãƒ  3) GROUP BYã‚«ãƒ©ãƒ 

-- ãƒ‘ã‚¿ãƒ¼ãƒ³2: ã‚«ãƒ¼ãƒ‡ã‚£ãƒŠãƒªãƒ†ã‚£ã‚’è€ƒæ…®ã—ãŸé †åº
-- ä½ã‚«ãƒ¼ãƒ‡ã‚£ãƒŠãƒªãƒ†ã‚£ â†’ é«˜ã‚«ãƒ¼ãƒ‡ã‚£ãƒŠãƒªãƒ†ã‚£ã®é †ã§é…ç½®

-- ãƒ‘ã‚¿ãƒ¼ãƒ³3: ãƒ‡ãƒ¼ã‚¿ã‚¢ã‚¯ã‚»ã‚¹ãƒ‘ã‚¿ãƒ¼ãƒ³ã«åŸºã¥ãé…ç½®
-- ã‚ˆãä¸€ç·’ã«ä½¿ç”¨ã•ã‚Œã‚‹ã‚«ãƒ©ãƒ ã‚’è¿‘ã„ä½ç½®ã«é…ç½®

-- =====================================================
-- å®Ÿè£…å¾Œã®ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æ¤œè¨¼SQL
-- =====================================================

-- 1. ã‚¯ã‚¨ãƒªå®Ÿè¡Œè¨ˆç”»ã®ç¢ºèª
-- EXPLAIN SELECT ... FROM table_name WHERE ...;

-- 2. ãƒ•ã‚¡ã‚¤ãƒ«ã‚¹ã‚­ãƒƒãƒ—çµ±è¨ˆã®ç¢ºèª
-- SELECT * FROM table_name WHERE filter_column = 'value';
-- -- SQLãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ©ãƒ¼ã§ãƒ•ã‚¡ã‚¤ãƒ«ã‚¹ã‚­ãƒƒãƒ—æ•°ã‚’ç¢ºèª

-- 3. ãƒ‡ãƒ¼ã‚¿é…ç½®ã®ç¢ºèª
-- SELECT 
--   file_path,
--   count(*) as row_count,
--   min(cluster_column1) as min_val,
--   max(cluster_column1) as max_val
-- FROM table_name
-- GROUP BY file_path
-- ORDER BY file_path;

-- =====================================================
-- æ³¨æ„äº‹é …
-- =====================================================

-- 1. Liquid Clusteringã¯æœ€å¤§4ã‚«ãƒ©ãƒ ã¾ã§æŒ‡å®šå¯èƒ½
-- 2. ãƒ‘ãƒ¼ãƒ†ã‚£ã‚·ãƒ§ãƒ‹ãƒ³ã‚°ã¨ã¯ä½µç”¨ä¸å¯
-- 3. æ—¢å­˜ã®ZORDER BYã¯è‡ªå‹•çš„ã«ç„¡åŠ¹åŒ–ã•ã‚Œã‚‹
-- 4. ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°ã®åŠ¹æœã¯æ™‚é–“ã¨ã¨ã‚‚ã«å‘ä¸Šã™ã‚‹ï¼ˆOPTIMIZEå®Ÿè¡Œã§æœ€é©åŒ–ï¼‰
-- 5. å®šæœŸçš„ãªOPTIMIZEå®Ÿè¡Œã‚’æ¨å¥¨
-- 6. **é‡è¦**: ã‚«ãƒ©ãƒ ã®æŒ‡å®šé †åºã¯ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã«å½±éŸ¿ã—ã¾ã›ã‚“
--    * CLUSTER BY (col1, col2, col3) ã¨ CLUSTER BY (col3, col1, col2) ã¯åŒç­‰
--    * å¾“æ¥ã®ãƒ‘ãƒ¼ãƒ†ã‚£ã‚·ãƒ§ãƒ‹ãƒ³ã‚°ã‚„Z-ORDERã¨ã¯ç•°ãªã‚‹é‡è¦ãªç‰¹æ€§

-- OPTIMIZEå®Ÿè¡Œä¾‹:
-- OPTIMIZE table_name;

-- =====================================================
-- ç”Ÿæˆæƒ…å ±
-- =====================================================
-- ç”Ÿæˆæ—¥æ™‚: {timestamp}
-- åˆ†æå¯¾è±¡ãƒ†ãƒ¼ãƒ–ãƒ«æ•°: {len(table_info)}
-- åŸºã¥ã„ãŸåˆ†æ: LLMã«ã‚ˆã‚‹Liquid Clusteringåˆ†æ
"""
    
    return sql_content

print("âœ… é–¢æ•°å®šç¾©å®Œäº†: analyze_liquid_clustering_opportunities, save_liquid_clustering_analysis")

# COMMAND ----------

# MAGIC %md
# MAGIC ## ğŸ¤– LLMã«ã‚ˆã‚‹ãƒœãƒˆãƒ«ãƒãƒƒã‚¯åˆ†æé–¢æ•°
# MAGIC
# MAGIC ã“ã®ã‚»ãƒ«ã§ã¯ä»¥ä¸‹ã®æ©Ÿèƒ½ã‚’å®šç¾©ã—ã¾ã™ï¼š
# MAGIC - æŠ½å‡ºã•ã‚ŒãŸãƒ¡ãƒˆãƒªã‚¯ã‚¹ã®LLMåˆ†æç”¨ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆ
# MAGIC - è¤‡æ•°LLMãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ã®å¯¾å¿œï¼ˆDatabricks/OpenAI/Azure/Anthropicï¼‰
# MAGIC - æ—¥æœ¬èªã§ã®è©³ç´°ãªåˆ†æãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ
# MAGIC - ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°ã¨ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯åˆ†æ

# COMMAND ----------

def analyze_bottlenecks_with_llm(metrics: Dict[str, Any]) -> str:
    """
    åŒ…æ‹¬çš„ãªãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹åˆ†æãƒ¬ãƒãƒ¼ãƒˆã‚’ç”Ÿæˆ
    ã‚»ãƒ«33ï¼ˆTOP10ãƒ—ãƒ­ã‚»ã‚¹ï¼‰ã€ã‚»ãƒ«35ï¼ˆLiquid Clusteringï¼‰ã€ã‚»ãƒ«47ï¼ˆæœ€é©åŒ–å®Ÿè¡Œï¼‰ã®æƒ…å ±ã‚’çµ±åˆ
    
    ğŸš¨ é‡è¦: ãƒ‘ãƒ¼ã‚»ãƒ³ãƒ†ãƒ¼ã‚¸è¨ˆç®—ãƒ‡ã‚°ãƒ¬é˜²æ­¢
    - ä¸¦åˆ—å®Ÿè¡Œãƒãƒ¼ãƒ‰ã®æ™‚é–“åˆè¨ˆã‚’å…¨ä½“æ™‚é–“ã¨ã—ã¦ä½¿ç”¨ã™ã‚‹ã“ã¨ã¯çµ¶å¯¾ã«ç¦æ­¢
    - overall_metrics.total_time_msï¼ˆwall-clock timeï¼‰ã‚’å„ªå…ˆä½¿ç”¨
    - ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯æ™‚ã¯æœ€å¤§ãƒãƒ¼ãƒ‰æ™‚é–“ã‚’ä½¿ç”¨ï¼ˆåˆè¨ˆã§ã¯ãªã„ï¼‰
    """
    from datetime import datetime
    
    print("ğŸ“Š åŒ…æ‹¬çš„ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹åˆ†æãƒ¬ãƒãƒ¼ãƒˆã‚’ç”Ÿæˆä¸­...")
    
    # ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆæ™‚åˆ»
    timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
    
    # === 1. åŸºæœ¬ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã®å–å¾— ===
    overall_metrics = metrics.get('overall_metrics', {})
    bottleneck_indicators = metrics.get('bottleneck_indicators', {})
    
    total_time_sec = overall_metrics.get('total_time_ms', 0) / 1000
    read_gb = overall_metrics.get('read_bytes', 0) / 1024 / 1024 / 1024
    cache_hit_ratio = bottleneck_indicators.get('cache_hit_ratio', 0) * 100
    data_selectivity = bottleneck_indicators.get('data_selectivity', 0) * 100
    
    # Photonæƒ…å ±
    photon_enabled = overall_metrics.get('photon_enabled', False)
    photon_utilization = min(overall_metrics.get('photon_utilization_ratio', 0) * 100, 100.0)
    
    # ä¸¦åˆ—åº¦ãƒ»ã‚·ãƒ£ãƒƒãƒ•ãƒ«æƒ…å ±
    shuffle_count = bottleneck_indicators.get('shuffle_operations_count', 0)
    has_shuffle_bottleneck = bottleneck_indicators.get('has_shuffle_bottleneck', False)
    has_low_parallelism = bottleneck_indicators.get('has_low_parallelism', False)
    low_parallelism_count = bottleneck_indicators.get('low_parallelism_stages_count', 0)
    
    # ã‚¹ãƒ”ãƒ«æƒ…å ±
    has_spill = bottleneck_indicators.get('has_spill', False)
    spill_bytes = bottleneck_indicators.get('spill_bytes', 0)
    spill_gb = spill_bytes / 1024 / 1024 / 1024 if spill_bytes > 0 else 0
    
    # ã‚¹ã‚­ãƒ¥ãƒ¼æ¤œå‡ºæƒ…å ±
    has_skew = bottleneck_indicators.get('has_skew', False)
    has_aqe_shuffle_skew_warning = bottleneck_indicators.get('has_aqe_shuffle_skew_warning', False)
    
    # === 2. ã‚»ãƒ«33: TOP10ãƒ—ãƒ­ã‚»ã‚¹åˆ†ææƒ…å ±ã®å–å¾— ===
    # å…¨ãƒãƒ¼ãƒ‰ã‚’å®Ÿè¡Œæ™‚é–“ã§ã‚½ãƒ¼ãƒˆï¼ˆãƒ‘ãƒ¼ã‚»ãƒ³ãƒ†ãƒ¼ã‚¸è¨ˆç®—ç”¨ï¼‰
    all_sorted_nodes = sorted(metrics['node_metrics'], 
                             key=lambda x: x['key_metrics'].get('durationMs', 0), 
                             reverse=True)
    
    # TOP5ãƒœãƒˆãƒ«ãƒãƒƒã‚¯æŠ½å‡ºç”¨
    sorted_nodes = all_sorted_nodes[:5]
    
    # ğŸš¨ é‡è¦: æ­£ã—ã„å…¨ä½“æ™‚é–“ã®è¨ˆç®—ï¼ˆãƒ‡ã‚°ãƒ¬é˜²æ­¢ï¼‰
    # 1. overall_metrics.total_time_msã‚’å„ªå…ˆä½¿ç”¨ï¼ˆwall-clock timeï¼‰
    total_time_ms = overall_metrics.get('total_time_ms', 0)
    
    # ğŸš¨ ä¸¦åˆ—å®Ÿè¡Œå•é¡Œã®ä¿®æ­£: task_total_time_msã‚’å„ªå…ˆä½¿ç”¨
    # å€‹åˆ¥ãƒãƒ¼ãƒ‰æ™‚é–“ã¯ä¸¦åˆ—ã‚¿ã‚¹ã‚¯ã®ç´¯ç©æ™‚é–“ã®ãŸã‚ã€åŒã˜ãç´¯ç©æ™‚é–“ã§ã‚ã‚‹task_total_time_msã¨æ¯”è¼ƒ
    task_total_time_ms = overall_metrics.get('task_total_time_ms', 0)
    
    if task_total_time_ms > 0:
        total_time_ms = task_total_time_ms
        print(f"âœ… ãƒ‡ãƒãƒƒã‚°: ä¸¦åˆ—å®Ÿè¡Œå¯¾å¿œ - task_total_time_msä½¿ç”¨: {total_time_ms:,} ms ({total_time_ms/3600000:.1f}æ™‚é–“)")
    elif total_time_ms <= 0:
        # execution_time_msã‚’æ¬¡ã®å„ªå…ˆåº¦ã§ä½¿ç”¨
        execution_time_ms = overall_metrics.get('execution_time_ms', 0)
        if execution_time_ms > 0:
            total_time_ms = execution_time_ms
            print(f"âš ï¸ ãƒ‡ãƒãƒƒã‚°: task_total_time_msåˆ©ç”¨ä¸å¯ã€execution_time_msä½¿ç”¨: {total_time_ms} ms")
        else:
            # æœ€çµ‚ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: å…¨ãƒãƒ¼ãƒ‰ã®åˆè¨ˆæ™‚é–“
            max_node_time = max([node['key_metrics'].get('durationMs', 0) for node in all_sorted_nodes], default=1)
            total_time_ms = int(max_node_time * 1.2)
            print(f"âš ï¸ ãƒ‡ãƒãƒƒã‚°: æœ€çµ‚ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ - æ¨å®šæ™‚é–“ä½¿ç”¨: {total_time_ms} ms")
    
    print(f"ğŸ“Š ãƒ‡ãƒãƒƒã‚°: ãƒ‘ãƒ¼ã‚»ãƒ³ãƒ†ãƒ¼ã‚¸è¨ˆç®—ã«ä½¿ç”¨ã™ã‚‹å…¨ä½“æ™‚é–“: {total_time_ms:,} ms ({total_time_ms/1000:.1f} sec)")
    
    critical_processes = []
    for i, node in enumerate(sorted_nodes):
        duration_ms = node['key_metrics'].get('durationMs', 0)
        duration_sec = duration_ms / 1000
        
        # ãƒ‘ãƒ¼ã‚»ãƒ³ãƒ†ãƒ¼ã‚¸è¨ˆç®—ï¼ˆ100%ã‚’ä¸Šé™ã¨ã™ã‚‹ï¼‰
        percentage = min((duration_ms / max(total_time_ms, 1)) * 100, 100.0)
        
        # ãƒœãƒˆãƒ«ãƒãƒƒã‚¯ã®é‡è¦åº¦åˆ¤å®š
        severity = "CRITICAL" if duration_ms >= 10000 else "HIGH" if duration_ms >= 5000 else "MEDIUM"
        
        # æ„å‘³ã®ã‚ã‚‹ãƒãƒ¼ãƒ‰åã‚’å–å¾—
        node_name = get_meaningful_node_name(node, metrics)
        short_name = node_name[:80] + "..." if len(node_name) > 80 else node_name
        
        critical_processes.append({
            'rank': i + 1,
            'name': short_name,
            'duration_sec': duration_sec,
            'percentage': percentage,
            'severity': severity
        })
    
    # === 3. ã‚»ãƒ«35: Liquid Clusteringåˆ†ææƒ…å ±ã®å–å¾— ===
    liquid_analysis = metrics.get('liquid_clustering_analysis', {})
    extracted_data = liquid_analysis.get('extracted_data', {})
    
    # ãƒ†ãƒ¼ãƒ–ãƒ«æƒ…å ±
    table_info = extracted_data.get('table_info', {})
    identified_tables = list(table_info.keys())[:5]  # TOP5ãƒ†ãƒ¼ãƒ–ãƒ«
    
    # ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼ãƒ»JOINãƒ»GROUP BYæƒ…å ±
    filter_columns = extracted_data.get('filter_columns', [])[:10]
    join_columns = extracted_data.get('join_columns', [])[:10]
    groupby_columns = extracted_data.get('groupby_columns', [])[:10]
    
    # === 4. ã‚»ãƒ«47: è©³ç´°ãƒœãƒˆãƒ«ãƒãƒƒã‚¯åˆ†æã®å–å¾— ===
    try:
        detailed_bottleneck = extract_detailed_bottleneck_analysis(metrics)
    except Exception as e:
        print(f"âš ï¸ è©³ç´°ãƒœãƒˆãƒ«ãƒãƒƒã‚¯åˆ†æã§ã‚¨ãƒ©ãƒ¼: {e}")
        detailed_bottleneck = {
            'top_bottleneck_nodes': [],
            'performance_recommendations': []
        }
    
    # === 5. åŒ…æ‹¬çš„ãƒ¬ãƒãƒ¼ãƒˆã®ç”Ÿæˆ ===
    
    report_lines = []
    
    # ã‚¿ã‚¤ãƒˆãƒ«ã¨ã‚µãƒãƒªãƒ¼
    report_lines.append("# ğŸ“Š Databricks SQLãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹åŒ…æ‹¬åˆ†æãƒ¬ãƒãƒ¼ãƒˆ")
    report_lines.append(f"**ç”Ÿæˆæ—¥æ™‚**: {timestamp}")
    report_lines.append("")
    
    # ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æ¦‚è¦
    report_lines.append("## 1. ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æ¦‚è¦")
    report_lines.append("")
    report_lines.append("### ä¸»è¦ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æŒ‡æ¨™")
    report_lines.append("")
    report_lines.append("| æŒ‡æ¨™ | å€¤ | è©•ä¾¡ |")
    report_lines.append("|------|-----|------|")
    report_lines.append(f"| å®Ÿè¡Œæ™‚é–“ | {total_time_sec:.1f}ç§’ | {'âœ… è‰¯å¥½' if total_time_sec < 60 else 'âš ï¸ æ”¹å–„å¿…è¦'} |")
    report_lines.append(f"| ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ | {read_gb:.2f}GB | {'âœ… è‰¯å¥½' if read_gb < 10 else 'âš ï¸ å¤§å®¹é‡'} |")
    report_lines.append(f"| Photonæœ‰åŠ¹ | {'ã¯ã„' if photon_enabled else 'ã„ã„ãˆ'} | {'âœ… è‰¯å¥½' if photon_enabled else 'âŒ æœªæœ‰åŠ¹'} |")
    report_lines.append(f"| ã‚­ãƒ£ãƒƒã‚·ãƒ¥åŠ¹ç‡ | {cache_hit_ratio:.1f}% | {'âœ… è‰¯å¥½' if cache_hit_ratio > 80 else 'âš ï¸ æ”¹å–„å¿…è¦'} |")
    report_lines.append(f"| ãƒ•ã‚£ãƒ«ã‚¿ç‡ | {data_selectivity:.1f}% | {'âœ… è‰¯å¥½' if data_selectivity > 50 else 'âš ï¸ ãƒ•ã‚£ãƒ«ã‚¿æ¡ä»¶ã‚’ç¢ºèª'} |")
    report_lines.append(f"| ã‚·ãƒ£ãƒƒãƒ•ãƒ«æ“ä½œ | {shuffle_count}å› | {'âœ… è‰¯å¥½' if shuffle_count < 5 else 'âš ï¸ å¤šæ•°'} |")
    report_lines.append(f"| ã‚¹ãƒ”ãƒ«ç™ºç”Ÿ | {'ã¯ã„' if has_spill else 'ã„ã„ãˆ'} | {'âŒ å•é¡Œã‚ã‚Š' if has_spill else 'âœ… è‰¯å¥½'} |")
    
    # ã‚¹ã‚­ãƒ¥ãƒ¼æ¤œå‡ºã®åˆ¤å®š
    if has_skew:
        skew_status = "AQEã§æ¤œå‡ºãƒ»å¯¾å¿œæ¸ˆ"
        skew_evaluation = "ğŸ”§ AQEå¯¾å¿œæ¸ˆ"
    elif has_aqe_shuffle_skew_warning:
        skew_status = "æ½œåœ¨çš„ãªã‚¹ã‚­ãƒ¥ãƒ¼ã®å¯èƒ½æ€§ã‚ã‚Š"
        skew_evaluation = "âš ï¸ æ”¹å–„å¿…è¦"
    else:
        skew_status = "æœªæ¤œå‡º"
        skew_evaluation = "âœ… è‰¯å¥½"
    
    report_lines.append(f"| ã‚¹ã‚­ãƒ¥ãƒ¼æ¤œå‡º | {skew_status} | {skew_evaluation} |")
    report_lines.append("")
    
    # ä¸»è¦ãƒœãƒˆãƒ«ãƒãƒƒã‚¯åˆ†æ
    report_lines.append("## 2. ä¸»è¦ãƒœãƒˆãƒ«ãƒãƒƒã‚¯åˆ†æ")
    report_lines.append("")
    
    # Photonåˆ†æ
    photon_status = "æœ‰åŠ¹" if photon_enabled else "ç„¡åŠ¹"
    photon_recommendation = ""
    if not photon_enabled:
        photon_recommendation = " â†’ **Photonæœ‰åŠ¹åŒ–ã‚’å¼·ãæ¨å¥¨**"
    elif photon_utilization < 50:
        photon_recommendation = " â†’ **Photonåˆ©ç”¨ç‡å‘ä¸ŠãŒå¿…è¦**"
    elif photon_utilization < 80:
        photon_recommendation = " â†’ **Photonè¨­å®šã®æœ€é©åŒ–ã‚’æ¨å¥¨**"
    else:
        photon_recommendation = " â†’ **æœ€é©åŒ–æ¸ˆã¿**"
    
    report_lines.append("### Photonã‚¨ãƒ³ã‚¸ãƒ³")
    report_lines.append(f"- **çŠ¶æ…‹**: {photon_status} (åˆ©ç”¨ç‡: {photon_utilization:.1f}%){photon_recommendation}")
    report_lines.append("")
    
    # ä¸¦åˆ—åº¦ãƒ»ã‚·ãƒ£ãƒƒãƒ•ãƒ«åˆ†æ
    report_lines.append("### ä¸¦åˆ—åº¦ãƒ»ã‚·ãƒ£ãƒƒãƒ•ãƒ«")
    shuffle_status = "âŒ ãƒœãƒˆãƒ«ãƒãƒƒã‚¯ã‚ã‚Š" if has_shuffle_bottleneck else "âœ… è‰¯å¥½"
    parallelism_status = "âŒ ä½ä¸¦åˆ—åº¦ã‚ã‚Š" if has_low_parallelism else "âœ… é©åˆ‡"
    
    report_lines.append(f"- **ã‚·ãƒ£ãƒƒãƒ•ãƒ«æ“ä½œ**: {shuffle_count}å› ({shuffle_status})")
    report_lines.append(f"- **ä¸¦åˆ—åº¦**: {parallelism_status}")
    if has_low_parallelism:
        report_lines.append(f"  - ä½ä¸¦åˆ—åº¦ã‚¹ãƒ†ãƒ¼ã‚¸: {low_parallelism_count}å€‹")
    report_lines.append("")
    
    # ã‚¹ãƒ”ãƒ«åˆ†æ
    report_lines.append("### ãƒ¡ãƒ¢ãƒªä½¿ç”¨çŠ¶æ³")
    if has_spill:
        report_lines.append(f"- **ãƒ¡ãƒ¢ãƒªã‚¹ãƒ”ãƒ«**: âŒ ç™ºç”Ÿä¸­ ({spill_gb:.2f}GB)")
        report_lines.append("  - **å¯¾å¿œå¿…è¦**: ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼è¨­å®šã®è¦‹ç›´ã—ã€ã‚¯ã‚¨ãƒªæœ€é©åŒ–")
    else:
        report_lines.append("- **ãƒ¡ãƒ¢ãƒªã‚¹ãƒ”ãƒ«**: âœ… ãªã—")
    report_lines.append("")
    
    # TOP5å‡¦ç†æ™‚é–“ãƒœãƒˆãƒ«ãƒãƒƒã‚¯
    report_lines.append("## 3. TOP5å‡¦ç†æ™‚é–“ãƒœãƒˆãƒ«ãƒãƒƒã‚¯")
    report_lines.append("")
    
    for process in critical_processes:
        severity_icon = "ğŸ”´" if process['severity'] == "CRITICAL" else "ğŸŸ " if process['severity'] == "HIGH" else "ğŸŸ¡"
        report_lines.append(f"### {process['rank']}. {severity_icon} {process['name']}")
        report_lines.append(f"   - **å®Ÿè¡Œæ™‚é–“**: {process['duration_sec']:.1f}ç§’ (å…¨ä½“ã®{process['percentage']:.1f}%)")
        report_lines.append(f"   - **é‡è¦åº¦**: {process['severity']}")
        report_lines.append("")
    
    # Liquid Clusteringæ¨å¥¨äº‹é …
    report_lines.append("## 4. Liquid Clusteringæ¨å¥¨äº‹é …")
    report_lines.append("")
    
    if identified_tables:
        report_lines.append("### å¯¾è±¡ãƒ†ãƒ¼ãƒ–ãƒ«")
        for i, table_name in enumerate(identified_tables, 1):
            # ç¾åœ¨ã®ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°ã‚­ãƒ¼æƒ…å ±ã‚’å–å¾—
            table_details = table_info.get(table_name, {})
            current_keys = table_details.get('current_clustering_keys', [])
            current_keys_str = ', '.join(current_keys) if current_keys else 'è¨­å®šãªã—'
            
            report_lines.append(f"{i}. `{table_name}`")
            report_lines.append(f"   - ç¾åœ¨ã®ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°ã‚­ãƒ¼: `{current_keys_str}`")
        report_lines.append("")
    
    if filter_columns or join_columns or groupby_columns:
        report_lines.append("### æ¨å¥¨ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°ã‚­ãƒ¼")
        
        if filter_columns:
            report_lines.append("**ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼æ¡ä»¶ã‚«ãƒ©ãƒ  (é«˜å„ªå…ˆåº¦)**:")
            for i, col in enumerate(filter_columns[:5], 1):
                expression = col.get('expression', 'Unknown')
                report_lines.append(f"  {i}. `{expression}`")
            report_lines.append("")
        
        if join_columns:
            report_lines.append("**JOINæ¡ä»¶ã‚«ãƒ©ãƒ  (ä¸­å„ªå…ˆåº¦)**:")
            for i, col in enumerate(join_columns[:5], 1):
                expression = col.get('expression', 'Unknown')
                key_type = col.get('key_type', '')
                report_lines.append(f"  {i}. `{expression}` ({key_type})")
            report_lines.append("")
        
        if groupby_columns:
            report_lines.append("**GROUP BYæ¡ä»¶ã‚«ãƒ©ãƒ  (ä¸­å„ªå…ˆåº¦)**:")
            for i, col in enumerate(groupby_columns[:5], 1):
                expression = col.get('expression', 'Unknown')
                report_lines.append(f"  {i}. `{expression}`")
            report_lines.append("")
    
    # å®Ÿè£…SQLä¾‹
    if identified_tables:
        report_lines.append("### å®Ÿè£…SQLä¾‹")
        for table_name in identified_tables[:2]:  # TOP2ãƒ†ãƒ¼ãƒ–ãƒ«ã®ã¿
            # ç¾åœ¨ã®ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°ã‚­ãƒ¼æƒ…å ±ã‚’å–å¾—
            table_details = table_info.get(table_name, {})
            current_keys = table_details.get('current_clustering_keys', [])
            current_keys_str = ', '.join(current_keys) if current_keys else 'è¨­å®šãªã—'
            
            report_lines.append(f"```sql")
            report_lines.append(f"-- {table_name}ãƒ†ãƒ¼ãƒ–ãƒ«ã«Liquid Clusteringã‚’é©ç”¨")
            report_lines.append(f"-- ç¾åœ¨ã®ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°ã‚­ãƒ¼: {current_keys_str}")
            report_lines.append(f"ALTER TABLE {table_name}")
            report_lines.append(f"CLUSTER BY (column1, column2, column3, column4);")
            report_lines.append(f"```")
            report_lines.append("")
    
    # æœ€é©åŒ–æ¨å¥¨ã‚¢ã‚¯ã‚·ãƒ§ãƒ³
    report_lines.append("## 5. æ¨å¥¨æœ€é©åŒ–ã‚¢ã‚¯ã‚·ãƒ§ãƒ³")
    report_lines.append("")
    
    # å„ªå…ˆåº¦åˆ¥ã®æ¨å¥¨äº‹é …
    high_priority_actions = []
    medium_priority_actions = []
    low_priority_actions = []
    
    # CRITICAL/HIGH priorityã‚¢ã‚¯ã‚·ãƒ§ãƒ³
    if not photon_enabled:
        high_priority_actions.append("**Photonã‚¨ãƒ³ã‚¸ãƒ³ã®æœ‰åŠ¹åŒ–** - æœ€å¤§50%ã®æ€§èƒ½å‘ä¸ŠæœŸå¾…")
    
    if has_spill:
        high_priority_actions.append(f"**ãƒ¡ãƒ¢ãƒªã‚¹ãƒ”ãƒ«è§£æ±º** - {spill_gb:.2f}GBã®ã‚¹ãƒ”ãƒ«ã‚’è§£æ¶ˆ")
    
    if has_shuffle_bottleneck:
        high_priority_actions.append("**ã‚·ãƒ£ãƒƒãƒ•ãƒ«æœ€é©åŒ–** - JOINé †åºã¨REPARTITIONé©ç”¨")
    
    # MEDIUMã‚¢ã‚¯ã‚·ãƒ§ãƒ³
    if photon_enabled and photon_utilization < 80:
        medium_priority_actions.append("**Photonåˆ©ç”¨ç‡å‘ä¸Š** - è¨­å®šã®æœ€é©åŒ–")
    
    if has_low_parallelism:
        medium_priority_actions.append("**ä¸¦åˆ—åº¦å‘ä¸Š** - ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼è¨­å®šã®è¦‹ç›´ã—")
    
    if cache_hit_ratio < 50:
        medium_priority_actions.append("**ã‚­ãƒ£ãƒƒã‚·ãƒ¥åŠ¹ç‡æ”¹å–„** - ãƒ‡ãƒ¼ã‚¿ã‚¢ã‚¯ã‚»ã‚¹ãƒ‘ã‚¿ãƒ¼ãƒ³ã®æœ€é©åŒ–")
    
    # Liquid Clustering
    if identified_tables:
        medium_priority_actions.append("**Liquid Clusteringå®Ÿè£…** - ä¸»è¦ãƒ†ãƒ¼ãƒ–ãƒ«ã®ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°")
    
    # LOWã‚¢ã‚¯ã‚·ãƒ§ãƒ³
    if data_selectivity < 50:
        low_priority_actions.append("**WHEREå¥æœ€é©åŒ–** - ãƒ•ã‚£ãƒ«ã‚¿åŠ¹ç‡ã®å‘ä¸Š")
    
    # ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ã®å‡ºåŠ›
    if high_priority_actions:
        report_lines.append("### ğŸš¨ ç·Šæ€¥å¯¾å¿œ (HIGHå„ªå…ˆåº¦)")
        for i, action in enumerate(high_priority_actions, 1):
            report_lines.append(f"{i}. {action}")
        report_lines.append("")
    
    if medium_priority_actions:
        report_lines.append("### âš ï¸ é‡è¦æ”¹å–„ (MEDIUMå„ªå…ˆåº¦)")
        for i, action in enumerate(medium_priority_actions, 1):
            report_lines.append(f"{i}. {action}")
        report_lines.append("")
    
    if low_priority_actions:
        report_lines.append("### ğŸ“ é•·æœŸæœ€é©åŒ– (LOWå„ªå…ˆåº¦)")
        for i, action in enumerate(low_priority_actions, 1):
            report_lines.append(f"{i}. {action}")
        report_lines.append("")
    
    # æœŸå¾…åŠ¹æœ
    report_lines.append("## 6. æœŸå¾…ã•ã‚Œã‚‹ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æ”¹å–„")
    report_lines.append("")
    
    total_improvement_estimate = 0
    improvement_details = []
    
    if not photon_enabled:
        total_improvement_estimate += 40
        improvement_details.append("- **Photonæœ‰åŠ¹åŒ–**: 30-50%ã®å®Ÿè¡Œæ™‚é–“çŸ­ç¸®")
    
    if has_spill:
        total_improvement_estimate += 25
        improvement_details.append(f"- **ã‚¹ãƒ”ãƒ«è§£æ¶ˆ**: 20-30%ã®å®Ÿè¡Œæ™‚é–“çŸ­ç¸® ({spill_gb:.2f}GBã‚¹ãƒ”ãƒ«å‰Šæ¸›)")
    
    if has_shuffle_bottleneck:
        total_improvement_estimate += 20
        improvement_details.append("- **ã‚·ãƒ£ãƒƒãƒ•ãƒ«æœ€é©åŒ–**: 15-25%ã®å®Ÿè¡Œæ™‚é–“çŸ­ç¸®")
    
    if identified_tables:
        total_improvement_estimate += 15
        improvement_details.append("- **Liquid Clustering**: 10-20%ã®å®Ÿè¡Œæ™‚é–“çŸ­ç¸®")
    
    # æ”¹å–„åŠ¹æœã®ä¸Šé™è¨­å®š
    total_improvement_estimate = min(total_improvement_estimate, 80)
    
    if improvement_details:
        for detail in improvement_details:
            report_lines.append(detail)
        report_lines.append("")
        report_lines.append(f"**ç·åˆæ”¹å–„è¦‹è¾¼ã¿**: æœ€å¤§{total_improvement_estimate}%ã®å®Ÿè¡Œæ™‚é–“çŸ­ç¸®")
    else:
        report_lines.append("ç¾åœ¨ã®ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã¯æ¯”è¼ƒçš„è‰¯å¥½ã§ã™ã€‚å¾®ç´°ãªæœ€é©åŒ–ã«ã‚ˆã‚Š5-10%ã®æ”¹å–„ãŒæœŸå¾…ã§ãã¾ã™ã€‚")
    
    report_lines.append("")
    report_lines.append("---")
    report_lines.append(f"*ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ: {timestamp} | åˆ†æã‚¨ãƒ³ã‚¸ãƒ³: Databricks SQL Profiler*")
    
    print("âœ… åŒ…æ‹¬çš„ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹åˆ†æãƒ¬ãƒãƒ¼ãƒˆãŒå®Œæˆã—ã¾ã—ãŸ")
    
    return "\n".join(report_lines)


def _call_databricks_llm(prompt: str) -> str:
    """Databricks Model Serving APIã‚’å‘¼ã³å‡ºã™"""
    try:
        # Databricksãƒˆãƒ¼ã‚¯ãƒ³ã®å–å¾—
        try:
            token = dbutils.notebook.entry_point.getDbutils().notebook().getContext().apiToken().get()
        except Exception:
            token = os.environ.get('DATABRICKS_TOKEN')
            if not token:
                return "âŒ Databricksãƒˆãƒ¼ã‚¯ãƒ³ã®å–å¾—ã«å¤±æ•—ã—ã¾ã—ãŸã€‚ç’°å¢ƒå¤‰æ•°DATABRICKS_TOKENã‚’è¨­å®šã—ã¦ãã ã•ã„ã€‚"
        
        # ãƒ¯ãƒ¼ã‚¯ã‚¹ãƒšãƒ¼ã‚¹URLã®å–å¾—
        try:
            workspace_url = spark.conf.get("spark.databricks.workspaceUrl")
        except Exception:
            workspace_url = dbutils.notebook.entry_point.getDbutils().notebook().getContext().tags().get("browserHostName").get()
        
        config = LLM_CONFIG["databricks"]
        endpoint_url = f"https://{workspace_url}/serving-endpoints/{config['endpoint_name']}/invocations"
        
        headers = {
            "Authorization": f"Bearer {token}",
            "Content-Type": "application/json"
        }
        
        payload = {
            "messages": [{"role": "user", "content": prompt}],
            "max_tokens": config["max_tokens"],
            "temperature": config["temperature"]
        }
        
        # æ‹¡å¼µæ€è€ƒãƒ¢ãƒ¼ãƒ‰ãŒæœ‰åŠ¹ãªå ´åˆã¯è¿½åŠ 
        if config.get("thinking_enabled", False):
            payload["thinking"] = {
                "type": "enabled",
                "budget_tokens": config.get("thinking_budget_tokens", 65536)
            }
        
        # ãƒªãƒˆãƒ©ã‚¤æ©Ÿèƒ½ï¼ˆSQLæœ€é©åŒ–ç”¨ã«å¢—å¼·ï¼‰
        max_retries = 3
        for attempt in range(max_retries):
            try:
                if attempt > 0:
                    print(f"ğŸ”„ ãƒªãƒˆãƒ©ã‚¤ä¸­... (è©¦è¡Œ {attempt + 1}/{max_retries})")
                
                response = requests.post(endpoint_url, headers=headers, json=payload, timeout=300)
                
                if response.status_code == 200:
                    result = response.json()
                    analysis_text = result.get('choices', [{}])[0].get('message', {}).get('content', '')
                    print("âœ… ãƒœãƒˆãƒ«ãƒãƒƒã‚¯åˆ†æãŒå®Œäº†ã—ã¾ã—ãŸ")
                    return analysis_text
                else:
                    error_msg = f"APIã‚¨ãƒ©ãƒ¼: ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹ã‚³ãƒ¼ãƒ‰ {response.status_code}"
                    if response.status_code == 400:
                        # 400ã‚¨ãƒ©ãƒ¼ã®å ´åˆã¯è©³ç´°ãªè§£æ±ºç­–ã‚’æä¾›
                        error_detail = response.text
                        if "maximum tokens" in error_detail.lower():
                            if attempt == max_retries - 1:
                                detailed_error = f"""âŒ {error_msg}

ğŸ”§ ãƒˆãƒ¼ã‚¯ãƒ³åˆ¶é™ã‚¨ãƒ©ãƒ¼ã®è§£æ±ºç­–:
1. LLM_CONFIG["databricks"]["max_tokens"] ã‚’ 65536 (64K) ã«å‰Šæ¸›
2. ã‚ˆã‚Šå˜ç´”ãªã‚¯ã‚¨ãƒªã§å†è©¦è¡Œ
3. æ‰‹å‹•ã§SQLæœ€é©åŒ–ã‚’å®Ÿè¡Œ
4. ã‚¯ã‚¨ãƒªã‚’åˆ†å‰²ã—ã¦æ®µéšçš„ã«æœ€é©åŒ–

ğŸ’¡ æ¨å¥¨è¨­å®š:
LLM_CONFIG["databricks"]["max_tokens"] = 65536
LLM_CONFIG["databricks"]["thinking_budget_tokens"] = 32768

è©³ç´°ã‚¨ãƒ©ãƒ¼: {error_detail}"""
                                print(detailed_error)
                                return detailed_error
                            else:
                                print(f"âš ï¸ {error_msg} (ãƒˆãƒ¼ã‚¯ãƒ³åˆ¶é™) - ãƒªãƒˆãƒ©ã‚¤ã—ã¾ã™...")
                                continue
                    
                    if attempt == max_retries - 1:
                        print(f"âŒ {error_msg}\nãƒ¬ã‚¹ãƒãƒ³ã‚¹: {response.text}")
                        return f"{error_msg}\nãƒ¬ã‚¹ãƒãƒ³ã‚¹: {response.text}"
                    else:
                        print(f"âš ï¸ {error_msg} - ãƒªãƒˆãƒ©ã‚¤ã—ã¾ã™...")
                        continue
                        
            except requests.exceptions.Timeout:
                if attempt == max_retries - 1:
                    timeout_msg = f"""â° ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆã‚¨ãƒ©ãƒ¼: Databricksã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆã®å¿œç­”ãŒ300ç§’ä»¥å†…ã«å®Œäº†ã—ã¾ã›ã‚“ã§ã—ãŸã€‚

ğŸ”§ è§£æ±ºç­–:
1. LLMã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆã®ç¨¼åƒçŠ¶æ³ã‚’ç¢ºèª
2. ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚µã‚¤ã‚ºã‚’å‰Šæ¸›
3. ã‚ˆã‚Šé«˜æ€§èƒ½ãªãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ç”¨
4. æ‰‹å‹•ã§SQLæœ€é©åŒ–ã‚’å®Ÿè¡Œ

ğŸ’¡ æ¨å¥¨ã‚¢ã‚¯ã‚·ãƒ§ãƒ³:
- ã‚¯ã‚¨ãƒªã®è¤‡é›‘åº¦ã‚’ç¢ºèª
- Databricks Model Servingã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆã®ã‚¹ã‚±ãƒ¼ãƒ«ã‚¢ãƒƒãƒ—
- ã‚·ãƒ³ãƒ—ãƒ«ãªã‚¯ã‚¨ãƒªã§ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ"""
                    print(f"âŒ {timeout_msg}")
                    return timeout_msg
                else:
                    print(f"â° ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆç™ºç”Ÿï¼ˆ300ç§’ï¼‰- ãƒªãƒˆãƒ©ã‚¤ã—ã¾ã™... (è©¦è¡Œ {attempt + 1}/{max_retries})")
                    continue
                    
    except Exception as e:
        return f"Databricks APIå‘¼ã³å‡ºã—ã‚¨ãƒ©ãƒ¼: {str(e)}"

def _call_openai_llm(prompt: str) -> str:
    """OpenAI APIã‚’å‘¼ã³å‡ºã™"""
    try:
        config = LLM_CONFIG["openai"]
        api_key = config["api_key"] or os.environ.get('OPENAI_API_KEY')
        
        if not api_key:
            return "âŒ OpenAI APIã‚­ãƒ¼ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚LLM_CONFIG['openai']['api_key']ã¾ãŸã¯ç’°å¢ƒå¤‰æ•°OPENAI_API_KEYã‚’è¨­å®šã—ã¦ãã ã•ã„ã€‚"
        
        headers = {
            "Authorization": f"Bearer {api_key}",
            "Content-Type": "application/json"
        }
        
        payload = {
            "model": config["model"],
            "messages": [{"role": "user", "content": prompt}],
            "max_tokens": config["max_tokens"],
            "temperature": config["temperature"]
        }
        
        response = requests.post("https://api.openai.com/v1/chat/completions", 
                               headers=headers, json=payload, timeout=300)
        
        if response.status_code == 200:
            result = response.json()
            analysis_text = result['choices'][0]['message']['content']
            print("âœ… OpenAIåˆ†æãŒå®Œäº†ã—ã¾ã—ãŸ")
            return analysis_text
        else:
            return f"OpenAI APIã‚¨ãƒ©ãƒ¼: ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹ã‚³ãƒ¼ãƒ‰ {response.status_code}\n{response.text}"
            
    except Exception as e:
        return f"OpenAI APIå‘¼ã³å‡ºã—ã‚¨ãƒ©ãƒ¼: {str(e)}"

def _call_azure_openai_llm(prompt: str) -> str:
    """Azure OpenAI APIã‚’å‘¼ã³å‡ºã™"""
    try:
        config = LLM_CONFIG["azure_openai"]
        api_key = config["api_key"] or os.environ.get('AZURE_OPENAI_API_KEY')
        
        if not api_key or not config["endpoint"] or not config["deployment_name"]:
            return "âŒ Azure OpenAIè¨­å®šãŒä¸å®Œå…¨ã§ã™ã€‚api_keyã€endpointã€deployment_nameã‚’è¨­å®šã—ã¦ãã ã•ã„ã€‚"
        
        endpoint_url = f"{config['endpoint']}/openai/deployments/{config['deployment_name']}/chat/completions?api-version={config['api_version']}"
        
        headers = {
            "api-key": api_key,
            "Content-Type": "application/json"
        }
        
        payload = {
            "messages": [{"role": "user", "content": prompt}],
            "max_tokens": config["max_tokens"],
            "temperature": config["temperature"]
        }
        
        response = requests.post(endpoint_url, headers=headers, json=payload, timeout=300)
        
        if response.status_code == 200:
            result = response.json()
            analysis_text = result['choices'][0]['message']['content']
            print("âœ… Azure OpenAIåˆ†æãŒå®Œäº†ã—ã¾ã—ãŸ")
            return analysis_text
        else:
            return f"Azure OpenAI APIã‚¨ãƒ©ãƒ¼: ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹ã‚³ãƒ¼ãƒ‰ {response.status_code}\n{response.text}"
            
    except Exception as e:
        return f"Azure OpenAI APIå‘¼ã³å‡ºã—ã‚¨ãƒ©ãƒ¼: {str(e)}"

def _call_anthropic_llm(prompt: str) -> str:
    """Anthropic APIã‚’å‘¼ã³å‡ºã™"""
    try:
        config = LLM_CONFIG["anthropic"]
        api_key = config["api_key"] or os.environ.get('ANTHROPIC_API_KEY')
        
        if not api_key:
            return "âŒ Anthropic APIã‚­ãƒ¼ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚LLM_CONFIG['anthropic']['api_key']ã¾ãŸã¯ç’°å¢ƒå¤‰æ•°ANTHROPIC_API_KEYã‚’è¨­å®šã—ã¦ãã ã•ã„ã€‚"
        
        headers = {
            "x-api-key": api_key,
            "Content-Type": "application/json",
            "anthropic-version": "2023-06-01"
        }
        
        payload = {
            "model": config["model"],
            "max_tokens": config["max_tokens"],
            "temperature": config["temperature"],
            "messages": [{"role": "user", "content": prompt}]
        }
        
        response = requests.post("https://api.anthropic.com/v1/messages", 
                               headers=headers, json=payload, timeout=300)
        
        if response.status_code == 200:
            result = response.json()
            analysis_text = result['content'][0]['text']
            print("âœ… Anthropicåˆ†æãŒå®Œäº†ã—ã¾ã—ãŸ")
            return analysis_text
        else:
            return f"Anthropic APIã‚¨ãƒ©ãƒ¼: ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹ã‚³ãƒ¼ãƒ‰ {response.status_code}\n{response.text}"
            
    except Exception as e:
        return f"Anthropic APIå‘¼ã³å‡ºã—ã‚¨ãƒ©ãƒ¼: {str(e)}"

print("âœ… é–¢æ•°å®šç¾©å®Œäº†: analyze_bottlenecks_with_llm")

# COMMAND ----------

# MAGIC %md
# MAGIC ## ğŸ“‹ LLMãƒœãƒˆãƒ«ãƒãƒƒã‚¯åˆ†æå®Ÿè¡Œã®æº–å‚™
# MAGIC
# MAGIC ã“ã®ã‚»ãƒ«ã§ã¯ä»¥ä¸‹ã®å‡¦ç†ã‚’å®Ÿè¡Œã—ã¾ã™ï¼š
# MAGIC - è¨­å®šã•ã‚ŒãŸLLMãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ã®ç¢ºèªã¨è¡¨ç¤º
# MAGIC - åˆ†æé–‹å§‹ã®æº–å‚™ã¨ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸è¡¨ç¤º
# MAGIC - ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆæœ€é©åŒ–ã«ã‚ˆã‚‹å®‰å®šæ€§å‘ä¸Š

# COMMAND ----------

# LLMãƒœãƒˆãƒ«ãƒãƒƒã‚¯åˆ†æå®Ÿè¡Œã®æº–å‚™
provider = LLM_CONFIG["provider"]

print(f"\nğŸ¤– ã€{provider.upper()} LLM ã«ã‚ˆã‚‹ SQLãƒœãƒˆãƒ«ãƒãƒƒã‚¯åˆ†æã‚’é–‹å§‹ã—ã¾ã™ã€‘")
print("=" * 80)

if provider == "databricks":
    endpoint = LLM_CONFIG["databricks"]["endpoint_name"]
    print(f"ğŸ”— Databricks Model Serving ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆ: {endpoint}")
    print("âš ï¸  Model Servingã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆãŒç¨¼åƒä¸­ã§ã‚ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™")
elif provider == "openai":
    model = LLM_CONFIG["openai"]["model"]
    print(f"ğŸ”— OpenAI ãƒ¢ãƒ‡ãƒ«: {model}")
    print("âš ï¸  OpenAI APIã‚­ãƒ¼ãŒå¿…è¦ã§ã™")
elif provider == "azure_openai":
    deployment = LLM_CONFIG["azure_openai"]["deployment_name"]
    print(f"ğŸ¤– Azure OpenAI ({deployment}) ã«ã‚ˆã‚‹ãƒœãƒˆãƒ«ãƒãƒƒã‚¯åˆ†æã‚’é–‹å§‹ã—ã¾ã™...")
    print("âš ï¸  Azure OpenAI APIã‚­ãƒ¼ã¨ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆãŒå¿…è¦ã§ã™")
elif provider == "anthropic":
    model = LLM_CONFIG["anthropic"]["model"]
    print(f"ğŸ¤– Anthropic ({model}) ã«ã‚ˆã‚‹ãƒœãƒˆãƒ«ãƒãƒƒã‚¯åˆ†æã‚’é–‹å§‹ã—ã¾ã™...")
    print("âš ï¸  Anthropic APIã‚­ãƒ¼ãŒå¿…è¦ã§ã™")

print("ğŸ“ åˆ†æãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’ç°¡æ½”åŒ–ã—ã¦ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆãƒªã‚¹ã‚¯ã‚’è»½æ¸›ã—ã¦ã„ã¾ã™...")
print()

# extracted_metricså¤‰æ•°ãŒå®šç¾©ã•ã‚Œã¦ã„ã‚‹ã‹ãƒã‚§ãƒƒã‚¯
try:
    extracted_metrics
    print("âœ… extracted_metricså¤‰æ•°ãŒç¢ºèªã•ã‚Œã¾ã—ãŸ")
    analysis_result = analyze_bottlenecks_with_llm(extracted_metrics)
except NameError:
    print("âŒ extracted_metricså¤‰æ•°ãŒå®šç¾©ã•ã‚Œã¦ã„ã¾ã›ã‚“")
    print("âš ï¸ ã‚»ãƒ«12 (ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ¡ãƒˆãƒªã‚¯ã‚¹æŠ½å‡º) ã‚’å…ˆã«å®Ÿè¡Œã—ã¦ãã ã•ã„")
    print("ğŸ“‹ æ­£ã—ã„å®Ÿè¡Œé †åº: ã‚»ãƒ«11 â†’ ã‚»ãƒ«12 â†’ ã‚»ãƒ«15")
    print("ğŸ”„ ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã®åˆ†æçµæœã‚’è¨­å®šã—ã¾ã™")
    analysis_result = """
ğŸ¤– LLMãƒœãƒˆãƒ«ãƒãƒƒã‚¯åˆ†æçµæœ

âŒ åˆ†æã«å¿…è¦ãªãƒ¡ãƒˆãƒªã‚¯ã‚¹ãƒ‡ãƒ¼ã‚¿ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚

ğŸ“‹ è§£æ±ºæ–¹æ³•:
1. ã‚»ãƒ«11ã§JSONãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã‚€
2. ã‚»ãƒ«12ã§ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚’æŠ½å‡ºã™ã‚‹
3. ã“ã®ã‚»ãƒ«ï¼ˆã‚»ãƒ«15ï¼‰ã‚’å†å®Ÿè¡Œã™ã‚‹

âš ï¸ å…ˆã«ãƒ¡ãƒˆãƒªã‚¯ã‚¹æŠ½å‡ºã‚’å®Œäº†ã—ã¦ã‹ã‚‰åˆ†æã‚’å®Ÿè¡Œã—ã¦ãã ã•ã„ã€‚
"""
except Exception as e:
    print(f"âŒ LLMåˆ†æä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(e)}")
    analysis_result = f"LLMåˆ†æã‚¨ãƒ©ãƒ¼: {str(e)}"

# COMMAND ----------

# MAGIC %md
# MAGIC # ğŸš€ ã‚¯ã‚¨ãƒªãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ«åˆ†æã‚»ã‚¯ã‚·ãƒ§ãƒ³
# MAGIC
# MAGIC **ã“ã“ã‹ã‚‰ãƒ¡ã‚¤ãƒ³ã®åˆ†æå‡¦ç†ãŒé–‹å§‹ã•ã‚Œã¾ã™**
# MAGIC
# MAGIC ğŸ“‹ **å®Ÿè¡Œæ‰‹é †:**
# MAGIC 1. ä¸Šè¨˜ã®ğŸ”§è¨­å®šãƒ»æº–å‚™ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã‚’ã™ã¹ã¦å®Ÿè¡Œã—ã¦ãã ã•ã„
# MAGIC 2. ä»¥ä¸‹ã®ã‚»ãƒ«ã‚’é †ç•ªã«å®Ÿè¡Œã—ã¦åˆ†æã‚’è¡Œã„ã¾ã™
# MAGIC 3. ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ãŸå ´åˆã¯ã€è¨­å®šã‚»ã‚¯ã‚·ãƒ§ãƒ³ã‹ã‚‰å†å®Ÿè¡Œã—ã¦ãã ã•ã„
# MAGIC
# MAGIC âš ï¸ **æ³¨æ„:**
# MAGIC - ğŸ”§è¨­å®šãƒ»æº–å‚™ã‚»ã‚¯ã‚·ãƒ§ãƒ³ â†’ ğŸš€ãƒ¡ã‚¤ãƒ³å‡¦ç†ã‚»ã‚¯ã‚·ãƒ§ãƒ³ â†’ ğŸ”§SQLæœ€é©åŒ–ã‚»ã‚¯ã‚·ãƒ§ãƒ³ ã®é †åºã§å®Ÿè¡Œ
# MAGIC - ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ã®è¨­å®šã¯å¿…ãšæœ€åˆã®ã‚»ãƒ«ã§è¡Œã£ã¦ãã ã•ã„
# MAGIC - LLMã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆã®è¨­å®šã‚’ç¢ºèªã—ã¦ãã ã•ã„

# COMMAND ----------

# MAGIC %md
# MAGIC ## ğŸš€ SQLãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ©ãƒ¼JSONãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿å®Ÿè¡Œ
# MAGIC
# MAGIC ã“ã®ã‚»ãƒ«ã§ã¯ä»¥ä¸‹ã®å‡¦ç†ã‚’å®Ÿè¡Œã—ã¾ã™ï¼š
# MAGIC - è¨­å®šã•ã‚ŒãŸãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ã‹ã‚‰JSONãƒ•ã‚¡ã‚¤ãƒ«ã®èª­ã¿è¾¼ã¿
# MAGIC - ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚ºã¨åŸºæœ¬æƒ…å ±ã®è¡¨ç¤º
# MAGIC - ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°ã¨å‡¦ç†åœæ­¢åˆ¶å¾¡

# COMMAND ----------

print("=" * 80)
print("ğŸš€ Databricks SQLãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ©ãƒ¼åˆ†æãƒ„ãƒ¼ãƒ«")
print("=" * 80)
print(f"ğŸ“ åˆ†æå¯¾è±¡ãƒ•ã‚¡ã‚¤ãƒ«: {JSON_FILE_PATH}")
print()

# ãƒ•ã‚¡ã‚¤ãƒ«å­˜åœ¨ãƒã‚§ãƒƒã‚¯
import os
if not os.path.exists(JSON_FILE_PATH):
    print("âŒ ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“:")
    print(f"   æŒ‡å®šãƒ‘ã‚¹: {JSON_FILE_PATH}")
    print()
    print("ğŸ’¡ ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹è¨­å®šã®ãƒ’ãƒ³ãƒˆ:")
    print("   1. ã‚»ãƒ«2ã§JSON_FILE_PATHå¤‰æ•°ã‚’æ­£ã—ã„ãƒ‘ã‚¹ã«è¨­å®šã—ã¦ãã ã•ã„")
    print("   2. åˆ©ç”¨å¯èƒ½ãªã‚ªãƒ—ã‚·ãƒ§ãƒ³ä¾‹:")
    print("      - /Volumes/main/base/mitsuhiro_vol/ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°å‰ãƒ—ãƒ©ãƒ³ãƒ•ã‚¡ã‚¤ãƒ«.json")
    print("      - /Volumes/main/base/mitsuhiro_vol/nophoton.json")
    print("      - /Volumes/main/base/mitsuhiro_vol/POC1.json")
    print("   3. ãƒ•ã‚¡ã‚¤ãƒ«ãŒDBFS FileStoreã«ã‚ã‚‹å ´åˆ:")
    print("      - /FileStore/shared_uploads/your_username/filename.json")
    print("âš ï¸ å‡¦ç†ã‚’åœæ­¢ã—ã¾ã™ã€‚")
    raise RuntimeError(f"æŒ‡å®šã•ã‚ŒãŸãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {JSON_FILE_PATH}")

# SQLãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ©ãƒ¼JSONãƒ•ã‚¡ã‚¤ãƒ«ã®èª­ã¿è¾¼ã¿
profiler_data = load_profiler_json(JSON_FILE_PATH)
if not profiler_data:
    print("âŒ JSONãƒ•ã‚¡ã‚¤ãƒ«ã®èª­ã¿è¾¼ã¿ã«å¤±æ•—ã—ã¾ã—ãŸã€‚ãƒ•ã‚¡ã‚¤ãƒ«å½¢å¼ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚")
    print("âš ï¸ å‡¦ç†ã‚’åœæ­¢ã—ã¾ã™ã€‚")
    # dbutils.notebook.exit("File loading failed")  # å®‰å…¨ã®ãŸã‚ã‚³ãƒ¡ãƒ³ãƒˆã‚¢ã‚¦ãƒˆ
    raise RuntimeError("JSONãƒ•ã‚¡ã‚¤ãƒ«ã®èª­ã¿è¾¼ã¿ã«å¤±æ•—ã—ã¾ã—ãŸã€‚")

print(f"âœ… ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿å®Œäº†")
print()

# COMMAND ----------

# MAGIC %md
# MAGIC ## ğŸ“Š ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ¡ãƒˆãƒªã‚¯ã‚¹æŠ½å‡ºã¨æ¦‚è¦è¡¨ç¤º
# MAGIC
# MAGIC ã“ã®ã‚»ãƒ«ã§ã¯ä»¥ä¸‹ã®å‡¦ç†ã‚’å®Ÿè¡Œã—ã¾ã™ï¼š
# MAGIC - ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ©ãƒ¼ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã®æŠ½å‡º
# MAGIC - ã‚¯ã‚¨ãƒªåŸºæœ¬æƒ…å ±ã®è¡¨ç¤º
# MAGIC - å…¨ä½“ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æŒ‡æ¨™ã®è¨ˆç®—ã¨è¡¨ç¤º
# MAGIC - Liquid Clusteringã®åˆ†æçµæœè¡¨ç¤º

# COMMAND ----------

# ğŸ“Š ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã®æŠ½å‡º
extracted_metrics = extract_performance_metrics(profiler_data)
print("âœ… ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚’æŠ½å‡ºã—ã¾ã—ãŸ")

# æŠ½å‡ºã•ã‚ŒãŸãƒ¡ãƒˆãƒªã‚¯ã‚¹ã®æ¦‚è¦è¡¨ç¤º
print("\n" + "=" * 50)
print("ğŸ“ˆ æŠ½å‡ºã•ã‚ŒãŸãƒ¡ãƒˆãƒªã‚¯ã‚¹æ¦‚è¦")
print("=" * 50)

query_info = extracted_metrics['query_info']
overall_metrics = extracted_metrics['overall_metrics']
bottleneck_indicators = extracted_metrics['bottleneck_indicators']

print(f"ğŸ†” ã‚¯ã‚¨ãƒªID: {query_info['query_id']}")
print(f"ğŸ“Š ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹: {query_info['status']}")
print(f"ğŸ‘¤ å®Ÿè¡Œãƒ¦ãƒ¼ã‚¶ãƒ¼: {query_info['user']}")
print(f"â±ï¸ å®Ÿè¡Œæ™‚é–“: {overall_metrics['total_time_ms']:,} ms ({overall_metrics['total_time_ms']/1000:.2f} sec)")
print(f"ğŸ’¾ èª­ã¿è¾¼ã¿ãƒ‡ãƒ¼ã‚¿: {overall_metrics['read_bytes']/1024/1024/1024:.2f} GB")
print(f"ğŸ“ˆ å‡ºåŠ›è¡Œæ•°: {overall_metrics['rows_produced_count']:,} è¡Œ")
print(f"ğŸ“‰ èª­ã¿è¾¼ã¿è¡Œæ•°: {overall_metrics['rows_read_count']:,} è¡Œ")
print(f"ğŸ¯ ãƒ•ã‚£ãƒ«ã‚¿ç‡: {bottleneck_indicators.get('data_selectivity', 0):.4f} ({bottleneck_indicators.get('data_selectivity', 0)*100:.2f}%)")
print(f"ğŸ”§ ã‚¹ãƒ†ãƒ¼ã‚¸æ•°: {len(extracted_metrics['stage_metrics'])}")
print(f"ğŸ—ï¸ ãƒãƒ¼ãƒ‰æ•°: {len(extracted_metrics['node_metrics'])}")

# Liquid Clusteringåˆ†æçµæœã®è¡¨ç¤º
liquid_analysis = extracted_metrics['liquid_clustering_analysis']
liquid_summary = liquid_analysis.get('summary', {})
print(f"ğŸ—‚ï¸ Liquid Clusteringå¯¾è±¡ãƒ†ãƒ¼ãƒ–ãƒ«æ•°: {liquid_summary.get('tables_identified', 0)}")
print(f"ğŸ“Š é«˜ã‚¤ãƒ³ãƒ‘ã‚¯ãƒˆãƒ†ãƒ¼ãƒ–ãƒ«æ•°: {liquid_summary.get('high_impact_tables', 0)}")

# COMMAND ----------

# MAGIC %md
# MAGIC ## ğŸ” ãƒœãƒˆãƒ«ãƒãƒƒã‚¯æŒ‡æ¨™è©³ç´°
# MAGIC
# MAGIC ã“ã®ã‚»ãƒ«ã§ã¯ä»¥ä¸‹ã®å‡¦ç†ã‚’å®Ÿè¡Œã—ã¾ã™ï¼š
# MAGIC - Photon ã‚¨ãƒ³ã‚¸ãƒ³ã®åˆ©ç”¨çŠ¶æ³ã¨ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹åˆ†æ
# MAGIC - ã‚·ãƒ£ãƒƒãƒ•ãƒ«æ“ä½œã¨ä¸¦åˆ—åº¦ã®å•é¡Œæ¤œå‡º
# MAGIC - å„ç¨®ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æŒ‡æ¨™ã®è©³ç´°è¡¨ç¤º

# COMMAND ----------

# ğŸ“‹ ãƒœãƒˆãƒ«ãƒãƒƒã‚¯æŒ‡æ¨™ã®è©³ç´°è¡¨ç¤º
print("\n" + "=" * 50)
print("ğŸ” ãƒœãƒˆãƒ«ãƒãƒƒã‚¯æŒ‡æ¨™è©³ç´°")
print("=" * 50)

# Photoné–¢é€£æŒ‡æ¨™
photon_enabled = overall_metrics.get('photon_enabled', False)
photon_utilization_ratio = overall_metrics.get('photon_utilization_ratio', 0)
photon_utilization = min(photon_utilization_ratio * 100, 100.0)  # æœ€å¤§100%ã«åˆ¶é™
photon_emoji = "âœ…" if photon_enabled and photon_utilization > 80 else "âš ï¸" if photon_enabled else "âŒ"

# åˆ©ç”¨ç‡ã«é–¢ã™ã‚‹è©³ç´°æƒ…å ±
if photon_enabled:
    photon_total_ms = overall_metrics.get('photon_total_time_ms', 0)
    task_total_ms = overall_metrics.get('task_total_time_ms', 0)
    print(f"{photon_emoji} Photonã‚¨ãƒ³ã‚¸ãƒ³: æœ‰åŠ¹ (åˆ©ç”¨ç‡: {photon_utilization:.1f}%)")
    print(f"   ğŸ“Š Photonå®Ÿè¡Œæ™‚é–“: {photon_total_ms:,} ms | ã‚¿ã‚¹ã‚¯åˆè¨ˆæ™‚é–“: {task_total_ms:,} ms")
else:
    print(f"{photon_emoji} Photonã‚¨ãƒ³ã‚¸ãƒ³: ç„¡åŠ¹")

# ä¸¦åˆ—åº¦ãƒ»ã‚·ãƒ£ãƒƒãƒ•ãƒ«é–¢é€£æŒ‡æ¨™
shuffle_count = bottleneck_indicators.get('shuffle_operations_count', 0)
has_shuffle_bottleneck = bottleneck_indicators.get('has_shuffle_bottleneck', False)
has_low_parallelism = bottleneck_indicators.get('has_low_parallelism', False)
low_parallelism_count = bottleneck_indicators.get('low_parallelism_stages_count', 0)

shuffle_emoji = "ğŸš¨" if has_shuffle_bottleneck else "âš ï¸" if shuffle_count > 5 else "âœ…"
print(f"{shuffle_emoji} ã‚·ãƒ£ãƒƒãƒ•ãƒ«æ“ä½œ: {shuffle_count}å› ({'ãƒœãƒˆãƒ«ãƒãƒƒã‚¯ã‚ã‚Š' if has_shuffle_bottleneck else 'æ­£å¸¸'})")

parallelism_emoji = "ğŸš¨" if has_low_parallelism else "âœ…"
print(f"{parallelism_emoji} ä¸¦åˆ—åº¦: {'å•é¡Œã‚ã‚Š' if has_low_parallelism else 'é©åˆ‡'} (ä½ä¸¦åˆ—åº¦ã‚¹ãƒ†ãƒ¼ã‚¸: {low_parallelism_count}å€‹)")

print()
print("ğŸ“Š ãã®ä»–ã®æŒ‡æ¨™:")

for key, value in bottleneck_indicators.items():
    # æ–°ã—ãè¿½åŠ ã—ãŸæŒ‡æ¨™ã¯ä¸Šè¨˜ã§è¡¨ç¤ºæ¸ˆã¿ãªã®ã§ã‚¹ã‚­ãƒƒãƒ—
    if key in ['shuffle_operations_count', 'has_shuffle_bottleneck', 'has_low_parallelism', 
               'low_parallelism_stages_count', 'total_shuffle_time_ms', 'shuffle_time_ratio',
               'slowest_shuffle_duration_ms', 'slowest_shuffle_node', 'low_parallelism_details',
               'average_low_parallelism']:
        continue
        
    if 'ratio' in key:
        emoji = "ğŸ“Š" if value < 0.1 else "âš ï¸" if value < 0.3 else "ğŸš¨"
        print(f"{emoji} {key}: {value:.3f} ({value*100:.1f}%)")
    elif 'bytes' in key and key != 'has_spill':
        if value > 0:
            emoji = "ğŸ’¾" if value < 1024*1024*1024 else "âš ï¸"  # 1GBæœªæº€ã¯æ™®é€šã€ä»¥ä¸Šã¯æ³¨æ„
            print(f"{emoji} {key}: {value:,} bytes ({value/1024/1024:.2f} MB)")
    elif key == 'has_spill':
        emoji = "âŒ" if not value else "âš ï¸"
        print(f"{emoji} {key}: {'ã‚ã‚Š' if value else 'ãªã—'}")
    elif 'duration' in key:
        emoji = "â±ï¸"
        print(f"{emoji} {key}: {value:,} ms ({value/1000:.2f} sec)")
    else:
        emoji = "â„¹ï¸"
        print(f"{emoji} {key}: {value}")

print()

# COMMAND ----------

# MAGIC %md
# MAGIC ## ğŸ’¾ ãƒ¡ãƒˆãƒªã‚¯ã‚¹ä¿å­˜ã¨æ™‚é–“æ¶ˆè²»åˆ†æ
# MAGIC
# MAGIC ã“ã®ã‚»ãƒ«ã§ã¯ä»¥ä¸‹ã®å‡¦ç†ã‚’å®Ÿè¡Œã—ã¾ã™ï¼š
# MAGIC - æŠ½å‡ºã—ãŸãƒ¡ãƒˆãƒªã‚¯ã‚¹ã®JSONå½¢å¼ã§ã®ä¿å­˜
# MAGIC - setå‹ã‹ã‚‰listå‹ã¸ã®å¤‰æ›å‡¦ç†
# MAGIC - æœ€ã‚‚æ™‚é–“ãŒã‹ã‹ã£ã¦ã„ã‚‹å‡¦ç†TOP10ã®è©³ç´°åˆ†æ
# MAGIC - ç‰¹å®šãƒ¡ãƒˆãƒªã‚¯ã‚¹ãƒ™ãƒ¼ã‚¹ã‚¹ãƒ”ãƒ«æ¤œå‡ºã¨AQEãƒ™ãƒ¼ã‚¹ã‚¹ã‚­ãƒ¥ãƒ¼åˆ†æ
# MAGIC
# MAGIC ğŸ’¿ **ã‚¹ãƒ”ãƒ«æ¤œå‡ºãƒ­ã‚¸ãƒƒã‚¯**:
# MAGIC - ã‚¿ãƒ¼ã‚²ãƒƒãƒˆãƒ¡ãƒˆãƒªã‚¯ã‚¹: `"Sink - Num bytes spilled to disk due to memory pressure"`
# MAGIC - åˆ¤å®šæ¡ä»¶: ä¸Šè¨˜ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã®å€¤ > 0 ã®å ´åˆã«ã‚¹ãƒ”ãƒ«ã‚ã‚Šã¨åˆ¤å®š
# MAGIC - æ¤œç´¢å¯¾è±¡: detailed_metrics â†’ raw_metrics â†’ key_metrics ã®é †åºã§æ¤œç´¢
# MAGIC
# MAGIC ğŸ¯ **ã‚¹ã‚­ãƒ¥ãƒ¼æ¤œå‡ºãƒ­ã‚¸ãƒƒã‚¯**:
# MAGIC - `AQEShuffleRead - Number of skewed partitions`: AQEãƒ™ãƒ¼ã‚¹ã‚¹ã‚­ãƒ¥ãƒ¼æ¤œå‡º
# MAGIC - åˆ¤å®šæ¡ä»¶: ãƒ¡ãƒˆãƒªã‚¯ã‚¹å€¤ > 0 ã§ã‚¹ã‚­ãƒ¥ãƒ¼åˆ¤å®š
# MAGIC - é‡è¦åº¦: æ¤œå‡ºå€¤ã«åŸºã¥ã„ãŸåˆ¤å®š
# MAGIC - çµ±è¨ˆãƒ™ãƒ¼ã‚¹åˆ¤å®šã¯éæ¨å¥¨ï¼ˆAQEãƒ™ãƒ¼ã‚¹åˆ¤å®šã‚’æ¨å¥¨ï¼‰
# MAGIC
# MAGIC ğŸ’¡ **ãƒ‡ãƒãƒƒã‚°ãƒ¢ãƒ¼ãƒ‰**: ã‚¹ãƒ”ãƒ«ãƒ»ã‚¹ã‚­ãƒ¥ãƒ¼ã®åˆ¤å®šæ ¹æ‹ ã‚’è©³ç´°è¡¨ç¤ºã—ãŸã„å ´åˆ
# MAGIC ```python
# MAGIC import os
# MAGIC os.environ['DEBUG_SPILL_ANALYSIS'] = 'true'   # ç‰¹å®šãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚¹ãƒ”ãƒ«åˆ¤å®šã®è©³ç´°è¡¨ç¤º
# MAGIC os.environ['DEBUG_SKEW_ANALYSIS'] = 'true'    # AQEãƒ™ãƒ¼ã‚¹ã‚¹ã‚­ãƒ¥ãƒ¼åˆ¤å®šã®è©³ç´°è¡¨ç¤º
# MAGIC ```

# COMMAND ----------

# ğŸ› ãƒ‡ãƒãƒƒã‚°ãƒ¢ãƒ¼ãƒ‰è¨­å®šï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
# 
# **ã‚¹ãƒ”ãƒ«ãƒ»ã‚¹ã‚­ãƒ¥ãƒ¼ã®åˆ¤å®šæ ¹æ‹ ã‚’è©³ç´°è¡¨ç¤ºã—ãŸã„å ´åˆã®ã¿å®Ÿè¡Œã—ã¦ãã ã•ã„**
# 
# ğŸ“‹ è¨­å®šå†…å®¹:
# - DEBUG_SPILL_ANALYSIS=true: ç‰¹å®šãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚¹ãƒ”ãƒ«åˆ¤å®šã®è©³ç´°æ ¹æ‹ ã‚’è¡¨ç¤º
# - DEBUG_SKEW_ANALYSIS=true: AQEãƒ™ãƒ¼ã‚¹ã‚¹ã‚­ãƒ¥ãƒ¼åˆ¤å®šã®è©³ç´°æ ¹æ‹ ã‚’è¡¨ç¤º
# 
# ğŸ’¿ ã‚¹ãƒ”ãƒ«ãƒ‡ãƒãƒƒã‚°è¡¨ç¤ºå†…å®¹:
# - ã‚¿ãƒ¼ã‚²ãƒƒãƒˆãƒ¡ãƒˆãƒªã‚¯ã‚¹: "Sink - Num bytes spilled to disk due to memory pressure"
# - å„ãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹ï¼ˆdetailed_metrics, raw_metrics, key_metricsï¼‰ã§ã®æ¤œç´¢çµæœ
# - ãƒ¡ãƒˆãƒªã‚¯ã‚¹ç™ºè¦‹æ™‚ã®å€¤ã¨åˆ¤å®šçµæœ
# - ãã®ä»–ã®ã‚¹ãƒ”ãƒ«é–¢é€£ãƒ¡ãƒˆãƒªã‚¯ã‚¹ä¸€è¦§ï¼ˆå‚è€ƒæƒ…å ±ï¼‰
# 
# ğŸ¯ ã‚¹ã‚­ãƒ¥ãƒ¼ãƒ‡ãƒãƒƒã‚°è¡¨ç¤ºå†…å®¹:
# - AQEShuffleRead - Number of skewed partitions ãƒ¡ãƒˆãƒªã‚¯ã‚¹å€¤
# - AQEãƒ™ãƒ¼ã‚¹ã‚¹ã‚­ãƒ¥ãƒ¼æ¤œå‡ºã®åˆ¤å®šæ ¹æ‹ 
# - æ¤œå‡ºã•ã‚ŒãŸã‚¹ã‚­ãƒ¥ãƒ¼æ•°ã¨é‡è¦åº¦ãƒ¬ãƒ™ãƒ«
# - çµ±è¨ˆãƒ™ãƒ¼ã‚¹åˆ¤å®šã¯éæ¨å¥¨ï¼ˆAQEãƒ™ãƒ¼ã‚¹åˆ¤å®šã‚’æ¨å¥¨ï¼‰

import os

# ç‰¹å®šãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚¹ãƒ”ãƒ«åˆ†æã®ãƒ‡ãƒãƒƒã‚°è¡¨ç¤ºã‚’æœ‰åŠ¹ã«ã™ã‚‹å ´åˆã¯ã‚³ãƒ¡ãƒ³ãƒˆã‚¢ã‚¦ãƒˆã‚’è§£é™¤
# os.environ['DEBUG_SPILL_ANALYSIS'] = 'true'

# AQEãƒ™ãƒ¼ã‚¹ã‚¹ã‚­ãƒ¥ãƒ¼åˆ†æã®ãƒ‡ãƒãƒƒã‚°è¡¨ç¤ºã‚’æœ‰åŠ¹ã«ã™ã‚‹å ´åˆã¯ã‚³ãƒ¡ãƒ³ãƒˆã‚¢ã‚¦ãƒˆã‚’è§£é™¤  
# os.environ['DEBUG_SKEW_ANALYSIS'] = 'true'

print("ğŸ› ãƒ‡ãƒãƒƒã‚°ãƒ¢ãƒ¼ãƒ‰è¨­å®š:")
print(f"   ç‰¹å®šãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚¹ãƒ”ãƒ«åˆ†æãƒ‡ãƒãƒƒã‚°: {os.environ.get('DEBUG_SPILL_ANALYSIS', 'false')}")
print(f"   AQEãƒ™ãƒ¼ã‚¹ã‚¹ã‚­ãƒ¥ãƒ¼åˆ†æãƒ‡ãƒãƒƒã‚°: {os.environ.get('DEBUG_SKEW_ANALYSIS', 'false')}")
print("   â€» 'true'ã«è¨­å®šã™ã‚‹ã¨åˆ¤å®šæ ¹æ‹ ã®è©³ç´°æƒ…å ±ãŒè¡¨ç¤ºã•ã‚Œã¾ã™")
print()
print("ğŸ’¿ ç‰¹å®šãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚¹ãƒ”ãƒ«æ¤œå‡ºåŸºæº–:")
print('   ğŸ¯ ã‚¿ãƒ¼ã‚²ãƒƒãƒˆ: "Sink - Num bytes spilled to disk due to memory pressure"')
print("   âœ… åˆ¤å®šæ¡ä»¶: å€¤ > 0")
print()
print("ğŸ¯ AQEãƒ™ãƒ¼ã‚¹ã‚¹ã‚­ãƒ¥ãƒ¼æ¤œå‡ºåŸºæº–:")
print("   ğŸ“Š AQEShuffleRead - Number of skewed partitions > 0")
print("   ğŸ“Š åˆ¤å®šæ¡ä»¶: ãƒ¡ãƒˆãƒªã‚¯ã‚¹å€¤ > 0")
print("   ğŸ“Š é‡è¦åº¦: æ¤œå‡ºå€¤ã«åŸºã¥ã")

# COMMAND ----------

# MAGIC %md
# MAGIC ## ğŸŒ æœ€ã‚‚æ™‚é–“ãŒã‹ã‹ã£ã¦ã„ã‚‹å‡¦ç†TOP10
# MAGIC
# MAGIC ã“ã®ã‚»ãƒ«ã§ã¯ä»¥ä¸‹ã®å‡¦ç†ã‚’å®Ÿè¡Œã—ã¾ã™ï¼š
# MAGIC - æŠ½å‡ºã—ãŸãƒ¡ãƒˆãƒªã‚¯ã‚¹ã®JSONå½¢å¼ã§ã®ä¿å­˜
# MAGIC - setå‹ã‹ã‚‰listå‹ã¸ã®å¤‰æ›å‡¦ç†
# MAGIC - æœ€ã‚‚æ™‚é–“ãŒã‹ã‹ã£ã¦ã„ã‚‹å‡¦ç†TOP10ã®è©³ç´°åˆ†æ
# MAGIC - ã‚¹ãƒ”ãƒ«æ¤œå‡ºã¨ãƒ‡ãƒ¼ã‚¿ã‚¹ã‚­ãƒ¥ãƒ¼åˆ†æ
# MAGIC - Sparkã‚¹ãƒ†ãƒ¼ã‚¸å®Ÿè¡Œåˆ†æ

# COMMAND ----------

# ğŸ’¾ æŠ½å‡ºã—ãŸãƒ¡ãƒˆãƒªã‚¯ã‚¹ã®JSONãƒ•ã‚¡ã‚¤ãƒ«ä¿å­˜ã¯é™¤å¤–ï¼ˆä¸è¦ï¼‰
def format_thinking_response(response) -> str:
    """
    thinking_enabled: Trueã®å ´åˆã®ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‚’äººé–“ã«èª­ã¿ã‚„ã™ã„å½¢å¼ã«å¤‰æ›
    æ€è€ƒéç¨‹ï¼ˆthinkingï¼‰ã¨ã‚·ã‚°ãƒãƒãƒ£ï¼ˆsignatureï¼‰ç­‰ã®ä¸è¦ãªæƒ…å ±ã¯é™¤å¤–ã—ã€æœ€çµ‚çš„ãªçµè«–ã®ã¿ã‚’è¡¨ç¤º
    JSONæ§‹é€ ã‚„ä¸é©åˆ‡ãªæ–‡å­—åˆ—ã®éœ²å‡ºã‚’é˜²æ­¢
    """
    import re  # reãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆã‚’è¿½åŠ 
    
    if not isinstance(response, list):
        # ãƒªã‚¹ãƒˆã§ãªã„å ´åˆã¯æ–‡å­—åˆ—ã¨ã—ã¦å‡¦ç†ã—ã€ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—
        cleaned_text = clean_response_text(str(response))
        return cleaned_text
    
    # é™¤å¤–ã™ã¹ãã‚­ãƒ¼ã®ãƒªã‚¹ãƒˆï¼ˆæ‹¡å¼µï¼‰
    excluded_keys = {
        'thinking', 'signature', 'metadata', 'id', 'request_id', 
        'timestamp', 'uuid', 'reasoning', 'type', 'model'
    }
    
    formatted_parts = []
    
    for item in response:
        if isinstance(item, dict):
            # æœ€ã‚‚é©åˆ‡ãªãƒ†ã‚­ã‚¹ãƒˆã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’æŠ½å‡º
            content = extract_best_content_from_dict(item, excluded_keys)
            if content:
                cleaned_content = clean_response_text(content)
                if is_valid_content(cleaned_content):
                    formatted_parts.append(cleaned_content)
        else:
            # è¾æ›¸ã§ãªã„å ´åˆã‚‚ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—
            cleaned_content = clean_response_text(str(item))
            if is_valid_content(cleaned_content):
                formatted_parts.append(cleaned_content)
    
    final_result = '\n'.join(formatted_parts)
    
    # æœ€çµ‚çš„ãªå“è³ªãƒã‚§ãƒƒã‚¯ã¨ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—
    final_result = final_quality_check(final_result)
    
    return final_result

def extract_best_content_from_dict(item_dict, excluded_keys):
    """è¾æ›¸ã‹ã‚‰æœ€é©ãªã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’æŠ½å‡º"""
    # å„ªå…ˆé †ä½: text > summary_text > content > message > ãã®ä»–
    priority_keys = ['text', 'summary_text', 'content', 'message', 'response']
    
    for key in priority_keys:
        if key in item_dict and item_dict[key]:
            content = str(item_dict[key])
            # JSONæ§‹é€ ãŒå«ã¾ã‚Œã¦ã„ãªã„ã‹ãƒã‚§ãƒƒã‚¯
            if not looks_like_json_structure(content):
                return content
    
    # å„ªå…ˆã‚­ãƒ¼ã§è¦‹ã¤ã‹ã‚‰ãªã„å ´åˆã€ä»–ã®ã‚­ãƒ¼ã‚’ãƒã‚§ãƒƒã‚¯ï¼ˆé™¤å¤–ã‚­ãƒ¼ä»¥å¤–ï¼‰
    for key, value in item_dict.items():
        if key not in excluded_keys and value and isinstance(value, str):
            if not looks_like_json_structure(value):
                return value
    
    return None

def looks_like_json_structure(text):
    """ãƒ†ã‚­ã‚¹ãƒˆãŒJSONæ§‹é€ ã‚’å«ã‚“ã§ã„ã‚‹ã‹ãƒã‚§ãƒƒã‚¯"""
    json_indicators = [
        "{'type':", '[{\'type\':', '{"type":', '[{"type":',
        "'text':", '"text":', "'summary_text':", '"summary_text":',
        'reasoning', 'metadata', 'signature'
    ]
    text_lower = text.lower()
    return any(indicator.lower() in text_lower for indicator in json_indicators)

def clean_response_text(text):
    """ãƒ¬ã‚¹ãƒãƒ³ã‚¹ãƒ†ã‚­ã‚¹ãƒˆã®ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—"""
    import re
    
    if not text or not isinstance(text, str):
        return ""
    
    # æ”¹è¡Œã‚³ãƒ¼ãƒ‰ã®æ­£è¦åŒ–
    text = text.replace('\\n', '\n').replace('\\t', '\t')
    
    # JSONæ§‹é€ ã®é™¤å»
    
    # å…¸å‹çš„ãªJSONæ§‹é€ ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’é™¤å»
    json_patterns = [
        r"'type':\s*'[^']*'",
        r'"type":\s*"[^"]*"',
        r"\[?\{'type':[^}]*\}[,\]]?",
        r'\[?\{"type":[^}]*\}[,\]]?',
        r"'reasoning':\s*\[[^\]]*\]",
        r'"reasoning":\s*\[[^\]]*\]',
        r"'signature':\s*'[A-Za-z0-9+/=]{50,}'",
        r'"signature":\s*"[A-Za-z0-9+/=]{50,}"'
    ]
    
    for pattern in json_patterns:
        text = re.sub(pattern, '', text, flags=re.IGNORECASE | re.DOTALL)
    
    # ä¸å®Œå…¨ãªJSONãƒ–ãƒ©ã‚±ãƒƒãƒˆã®é™¤å»
    text = re.sub(r'^\s*[\[\{]', '', text)  # å…ˆé ­ã® [ ã‚„ {
    text = re.sub(r'[\]\}]\s*$', '', text)  # æœ«å°¾ã® ] ã‚„ }
    text = re.sub(r'^\s*[,;]\s*', '', text)  # å…ˆé ­ã®ã‚«ãƒ³ãƒã‚„ã‚»ãƒŸã‚³ãƒ­ãƒ³
    
    # é€£ç¶šã™ã‚‹ç©ºç™½ãƒ»æ”¹è¡Œã®æ­£è¦åŒ–
    text = re.sub(r'\n\s*\n\s*\n+', '\n\n', text)  # 3ã¤ä»¥ä¸Šã®é€£ç¶šæ”¹è¡Œã‚’2ã¤ã«
    text = re.sub(r'[ \t]+', ' ', text)  # é€£ç¶šã™ã‚‹ã‚¹ãƒšãƒ¼ã‚¹ãƒ»ã‚¿ãƒ–ã‚’1ã¤ã«
    
    # å‰å¾Œã®ç©ºç™½ã‚’é™¤å»
    text = text.strip()
    
    return text

def is_valid_content(text):
    """ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ãŒæœ‰åŠ¹ã‹ã©ã†ã‹ã‚’ãƒã‚§ãƒƒã‚¯"""
    import re
    
    if not text or len(text.strip()) < 10:
        return False
    
    # ç„¡åŠ¹ãªãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’ãƒã‚§ãƒƒã‚¯
    invalid_patterns = [
        r'^[{\[\'"]*$',  # JSONæ§‹é€ ã®ã¿
        r'^[,;:\s]*$',   # åŒºåˆ‡ã‚Šæ–‡å­—ã®ã¿
        r'^\s*reasoning\s*$',  # reasoningã®ã¿
        r'^\s*metadata\s*$',   # metadataã®ã¿
        r'^[A-Za-z0-9+/=]{50,}$',  # Base64ã£ã½ã„é•·ã„æ–‡å­—åˆ—
    ]
    
    for pattern in invalid_patterns:
        if re.match(pattern, text.strip(), re.IGNORECASE):
            return False
    
    return True

def final_quality_check(text):
    """æœ€çµ‚çš„ãªå“è³ªãƒã‚§ãƒƒã‚¯ã¨ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—"""
    import re  # reãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆã‚’è¿½åŠ 
    
    if not text:
        return "åˆ†æçµæœã®æŠ½å‡ºã«å¤±æ•—ã—ã¾ã—ãŸã€‚"
    
    # è¨€èªã®ä¸€è²«æ€§ãƒã‚§ãƒƒã‚¯ï¼ˆå®‰å…¨ãªå¤‰æ•°ã‚¢ã‚¯ã‚»ã‚¹ï¼‰
    try:
        language = globals().get('OUTPUT_LANGUAGE', 'ja')  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã¯æ—¥æœ¬èª
    except:
        language = 'ja'
    
    if language == 'ja':
        text = ensure_japanese_consistency(text)
    elif language == 'en':
        text = ensure_english_consistency(text)
    
    # æœ€å°é™ã®é•·ã•ãƒã‚§ãƒƒã‚¯
    if len(text.strip()) < 20:
        if language == 'ja':
            return "åˆ†æçµæœãŒä¸å®Œå…¨ã§ã™ã€‚è©³ç´°ãªåˆ†æã‚’å®Ÿè¡Œä¸­ã§ã™ã€‚"
        else:
            return "Analysis result is incomplete. Detailed analysis in progress."
    
    return text

def ensure_japanese_consistency(text):
    """æ—¥æœ¬èªã®ä¸€è²«æ€§ã‚’ç¢ºä¿"""
    import re
    
    # æ˜ã‚‰ã‹ã«ç ´æã—ã¦ã„ã‚‹éƒ¨åˆ†ã‚’é™¤å»
    # ä¾‹: "æ­£caientify="predicate_liquid_referencet1" ã®ã‚ˆã†ãªç ´ææ–‡å­—åˆ—
    text = re.sub(r'[a-zA-Z0-9_="\']{20,}', '', text)
    
    # ä¸å®Œå…¨ãªãƒãƒ¼ã‚¯ãƒ€ã‚¦ãƒ³ã®ä¿®æ­£
    text = re.sub(r'#\s*[^#\n]*["\'>]+[^#\n]*', '', text)  # ç ´æã—ãŸãƒãƒ¼ã‚¯ãƒ€ã‚¦ãƒ³ãƒ˜ãƒƒãƒ€ãƒ¼
    
    # æ„å‘³ä¸æ˜ãªæ–‡å­—åˆ—ãƒ‘ã‚¿ãƒ¼ãƒ³ã®é™¤å»ï¼ˆæ‹¡å¼µï¼‰
    nonsense_patterns = [
        r'addressing_sales_column\d*',
        r'predicate_liquid_reference[a-zA-Z0-9]*',
        r'bottlenars\s+effect',
        r'å®Ÿè£…éä¿å­˜åœ¨',
        r'è£ç¥¨ã®end_by',
        r'riconsistall',
        r'caientify[a-zA-Z0-9="\']*',
        r'iving\s+[a-zA-Z0-9]*',
        r'o\s+Matteré…è³›',
        r'ubsãŒä½ã„åƒ®æ€§',
        r'åˆ°ç”°ãƒ‡ãƒ¼ã‚¿ã®æ–¹åŠ¹æ€§',
        r'ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹.*topic.*é …è¡Œã«è€ƒ',
        r'ï¼»[^ï¼½]*ï¼½">[^<]*',  # ç ´æã—ãŸHTML/XMLè¦ç´ 
        r'\]\s*">\s*$'  # æ–‡æœ«ã®ç ´æã—ãŸã‚¿ã‚°
    ]
    
    for pattern in nonsense_patterns:
        text = re.sub(pattern, '', text, flags=re.IGNORECASE)
    
    # é€£ç¶šã™ã‚‹è¨˜å·ã®é™¤å»
    text = re.sub(r'["\'>]{2,}', '', text)
    text = re.sub(r'[=\'"]{3,}', '', text)
    
    # ç ´æã—ãŸæ—¥æœ¬èªã®ä¿®æ­£ãƒ‘ã‚¿ãƒ¼ãƒ³
    broken_japanese_patterns = [
        (r'ã®æ–¹æ³•å‹•çš„ãŒã‚‰', 'å‹•çš„ãªæ–¹æ³•ã§'),
        (r'æ€è€ƒã«æ²¿ã£ã¦é€²ã‚ã¦ã„ãã¾ã™ã€‚$', 'æ€è€ƒã«æ²¿ã£ã¦åˆ†æã‚’é€²ã‚ã¾ã™ã€‚'),
        (r'ãƒ™ã‚¹ãƒˆãƒ—ãƒ©ã‚¯ãƒ†ã‚£ã‚¹ã«æ²¿ã£ãŸæ”¹å–„ã‚’.*ã¾ã§ã—ã¦ã„ã‚‹ã®', 'ãƒ™ã‚¹ãƒˆãƒ—ãƒ©ã‚¯ãƒ†ã‚£ã‚¹ã«æ²¿ã£ãŸæ”¹å–„ææ¡ˆ'),
    ]
    
    for broken, fixed in broken_japanese_patterns:
        text = re.sub(broken, fixed, text, flags=re.IGNORECASE)
    
    # ç©ºè¡Œã®æ­£è¦åŒ–
    text = re.sub(r'\n\s*\n\s*\n+', '\n\n', text)
    
    return text.strip()

def ensure_english_consistency(text):
    """è‹±èªã®ä¸€è²«æ€§ã‚’ç¢ºä¿"""
    import re
    
    # åŒæ§˜ã®ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—ã‚’è‹±èªç”¨ã«å®Ÿè£…
    text = re.sub(r'[^\x00-\x7F\s]{10,}', '', text)  # éASCIIæ–‡å­—ã®é•·ã„é€£ç¶šã‚’é™¤å»
    
    return text.strip()

def extract_main_content_from_thinking_response(response) -> str:
    """
    thinkingå½¢å¼ã®ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‹ã‚‰ä¸»è¦ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ï¼ˆtextã¾ãŸã¯summary_textï¼‰ã®ã¿ã‚’æŠ½å‡º
    thinkingã€signatureç­‰ã®ä¸è¦ãªæƒ…å ±ã¯é™¤å¤–
    JSONæ§‹é€ ã‚„ç ´æã—ãŸãƒ†ã‚­ã‚¹ãƒˆã®æ··å…¥ã‚’é˜²æ­¢
    """
    if not isinstance(response, list):
        cleaned_text = clean_response_text(str(response))
        return final_quality_check(cleaned_text)
    
    # é™¤å¤–ã™ã¹ãã‚­ãƒ¼
    excluded_keys = {
        'thinking', 'signature', 'metadata', 'id', 'request_id', 
        'timestamp', 'uuid', 'reasoning', 'type', 'model'
    }
    
    for item in response:
        if isinstance(item, dict):
            # æœ€é©ãªã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’æŠ½å‡º
            content = extract_best_content_from_dict(item, excluded_keys)
            if content:
                cleaned_content = clean_response_text(content)
                if is_valid_content(cleaned_content):
                    return final_quality_check(cleaned_content)
    
    # ä¸»è¦ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ãŒè¦‹ã¤ã‹ã‚‰ãªã„å ´åˆã¯å…¨ä½“ã‚’ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆ
    return format_thinking_response(response)

def convert_sets_to_lists(obj):
    """setå‹ã‚’listå‹ã«å¤‰æ›ã—ã¦JSONã‚·ãƒªã‚¢ãƒ©ã‚¤ã‚ºå¯èƒ½ã«ã™ã‚‹"""
    if isinstance(obj, set):
        return list(obj)
    elif isinstance(obj, dict):
        return {key: convert_sets_to_lists(value) for key, value in obj.items()}
    elif isinstance(obj, list):
        return [convert_sets_to_lists(item) for item in obj]
    else:
        return obj

# output_extracted_metrics ã®ç”Ÿæˆã¯é™¤å¤–ï¼ˆä¸è¦ï¼‰

# ğŸŒ æœ€ã‚‚æ™‚é–“ãŒã‹ã‹ã£ã¦ã„ã‚‹å‡¦ç†TOP10
print(f"\nğŸŒ æœ€ã‚‚æ™‚é–“ãŒã‹ã‹ã£ã¦ã„ã‚‹å‡¦ç†TOP10")
print("=" * 80)
print("ğŸ“Š ã‚¢ã‚¤ã‚³ãƒ³èª¬æ˜: â±ï¸æ™‚é–“ ğŸ’¾ãƒ¡ãƒ¢ãƒª ğŸ”¥ğŸŒä¸¦åˆ—åº¦ ğŸ’¿ã‚¹ãƒ”ãƒ« âš–ï¸ã‚¹ã‚­ãƒ¥ãƒ¼")
print('ğŸ’¿ ã‚¹ãƒ”ãƒ«åˆ¤å®š: "Sink - Num bytes spilled to disk due to memory pressure" > 0')
print("ğŸ¯ ã‚¹ã‚­ãƒ¥ãƒ¼åˆ¤å®š: 'AQEShuffleRead - Number of skewed partitions' > 0")

# ãƒãƒ¼ãƒ‰ã‚’å®Ÿè¡Œæ™‚é–“ã§ã‚½ãƒ¼ãƒˆ
sorted_nodes = sorted(extracted_metrics['node_metrics'], 
                     key=lambda x: x['key_metrics'].get('durationMs', 0), 
                     reverse=True)

# æœ€å¤§10å€‹ã®ãƒãƒ¼ãƒ‰ã‚’å‡¦ç†
final_sorted_nodes = sorted_nodes[:10]

if final_sorted_nodes:
    # ğŸš¨ é‡è¦: æ­£ã—ã„å…¨ä½“æ™‚é–“ã®è¨ˆç®—ï¼ˆãƒ‡ã‚°ãƒ¬é˜²æ­¢ï¼‰
    # 1. overall_metricsã‹ã‚‰å…¨ä½“å®Ÿè¡Œæ™‚é–“ã‚’å–å¾—ï¼ˆwall-clock timeï¼‰
    overall_metrics = extracted_metrics.get('overall_metrics', {})
    total_duration = overall_metrics.get('total_time_ms', 0)
    
    # ğŸš¨ ä¸¦åˆ—å®Ÿè¡Œå•é¡Œã®ä¿®æ­£: task_total_time_msã‚’å„ªå…ˆä½¿ç”¨
    task_total_time_ms = overall_metrics.get('task_total_time_ms', 0)
    
    if task_total_time_ms > 0:
        total_duration = task_total_time_ms
        print(f"âœ… ã‚³ãƒ³ã‚½ãƒ¼ãƒ«è¡¨ç¤º: ä¸¦åˆ—å®Ÿè¡Œå¯¾å¿œ - task_total_time_msä½¿ç”¨: {total_duration:,} ms ({total_duration/3600000:.1f}æ™‚é–“)")
    elif total_duration <= 0:
        # execution_time_msã‚’æ¬¡ã®å„ªå…ˆåº¦ã§ä½¿ç”¨
        execution_time_ms = overall_metrics.get('execution_time_ms', 0)
        if execution_time_ms > 0:
            total_duration = execution_time_ms
            print(f"âš ï¸ ã‚³ãƒ³ã‚½ãƒ¼ãƒ«è¡¨ç¤º: task_total_time_msåˆ©ç”¨ä¸å¯ã€execution_time_msä½¿ç”¨: {total_duration} ms")
        else:
            # æœ€çµ‚ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
            max_node_time = max([node['key_metrics'].get('durationMs', 0) for node in sorted_nodes], default=1)
            total_duration = int(max_node_time * 1.2)
            print(f"âš ï¸ ã‚³ãƒ³ã‚½ãƒ¼ãƒ«è¡¨ç¤º: æœ€çµ‚ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ - æ¨å®šæ™‚é–“ä½¿ç”¨: {total_duration} ms")
    
    print(f"ğŸ“Š ç´¯ç©ã‚¿ã‚¹ã‚¯å®Ÿè¡Œæ™‚é–“ï¼ˆä¸¦åˆ—ï¼‰: {total_duration:,} ms ({total_duration/3600000:.1f} æ™‚é–“)")
    print(f"ğŸ“ˆ TOP10åˆè¨ˆæ™‚é–“ï¼ˆä¸¦åˆ—å®Ÿè¡Œï¼‰: {sum(node['key_metrics'].get('durationMs', 0) for node in final_sorted_nodes):,} ms")

    print()
    
    for i, node in enumerate(final_sorted_nodes):
        rows_num = node['key_metrics'].get('rowsNum', 0)
        duration_ms = node['key_metrics'].get('durationMs', 0)
        memory_mb = node['key_metrics'].get('peakMemoryBytes', 0) / 1024 / 1024
        
        # ğŸš¨ é‡è¦: æ­£ã—ã„ãƒ‘ãƒ¼ã‚»ãƒ³ãƒ†ãƒ¼ã‚¸è¨ˆç®—ï¼ˆãƒ‡ã‚°ãƒ¬é˜²æ­¢ï¼‰
        # wall-clock timeã«å¯¾ã™ã‚‹å„ãƒãƒ¼ãƒ‰ã®å®Ÿè¡Œæ™‚é–“ã®å‰²åˆ
        time_percentage = min((duration_ms / max(total_duration, 1)) * 100, 100.0)
        
        # æ™‚é–“ã®é‡è¦åº¦ã«åŸºã¥ã„ã¦ã‚¢ã‚¤ã‚³ãƒ³ã‚’é¸æŠ
        if duration_ms >= 10000:  # 10ç§’ä»¥ä¸Š
            time_icon = "ï¿½"
            severity = "CRITICAL"
        elif duration_ms >= 5000:  # 5ç§’ä»¥ä¸Š
            time_icon = "ğŸŸ "
            severity = "HIGH"
        elif duration_ms >= 1000:  # 1ç§’ä»¥ä¸Š
            time_icon = "ğŸŸ¡"
            severity = "MEDIUM"
        else:
            time_icon = "ï¿½"
            severity = "LOW"
        
        # ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ã®ã‚¢ã‚¤ã‚³ãƒ³
        memory_icon = "ï¿½" if memory_mb < 100 else "âš ï¸" if memory_mb < 1000 else "ğŸš¨"
        
        # ã‚ˆã‚Šæ„å‘³ã®ã‚ã‚‹ãƒãƒ¼ãƒ‰åã‚’å–å¾—
        raw_node_name = node['name']
        node_name = get_meaningful_node_name(node, extracted_metrics)
        short_name = node_name[:100] + "..." if len(node_name) > 100 else node_name
        
        # ä¸¦åˆ—åº¦æƒ…å ±ã®å–å¾—ï¼ˆä¿®æ­£ç‰ˆ: è¤‡æ•°ã®Tasks totalãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚’å–å¾—ï¼‰
        parallelism_data = extract_parallelism_metrics(node)
        
        # å¾“æ¥ã®å˜ä¸€å€¤ï¼ˆäº’æ›æ€§ã®ãŸã‚ï¼‰
        num_tasks = parallelism_data.get('tasks_total', 0)
        
        # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: Sink - Tasks totalã¾ãŸã¯Source - Tasks totalãŒã‚ã‚‹å ´åˆ
        if num_tasks == 0:
            if parallelism_data.get('sink_tasks_total', 0) > 0:
                num_tasks = parallelism_data.get('sink_tasks_total', 0)
            elif parallelism_data.get('source_tasks_total', 0) > 0:
                num_tasks = parallelism_data.get('source_tasks_total', 0)
        
        # ãƒ‡ã‚£ã‚¹ã‚¯ã‚¹ãƒ”ãƒ«ã‚¢ã‚¦ãƒˆã®æ¤œå‡ºï¼ˆãƒ¡ãƒ¢ãƒªãƒ—ãƒ¬ãƒƒã‚·ãƒ£ãƒ¼ã«ã‚ˆã‚‹ã‚¹ãƒ”ãƒ«ãƒ¡ãƒˆãƒªã‚¯ã‚¹å¯¾å¿œæ”¹å–„ç‰ˆï¼‰
        spill_detected = False
        spill_bytes = 0
        spill_details = []
        
        # ã‚¹ãƒ”ãƒ«æ¤œå‡ºã‚¿ãƒ¼ã‚²ãƒƒãƒˆãƒ¡ãƒˆãƒªã‚¯ã‚¹åãƒªã‚¹ãƒˆï¼ˆæ­£ç¢ºãªãƒ¡ãƒˆãƒªã‚¯ã‚¹åã®ã¿ï¼‰
        exact_spill_metrics = [
            "Num bytes spilled to disk due to memory pressure",
            "Sink - Num bytes spilled to disk due to memory pressure",
            "Sink/Num bytes spilled to disk due to memory pressure"
        ]
        
        # 1. detailed_metricsã‹ã‚‰æ­£ç¢ºãªãƒ¡ãƒˆãƒªã‚¯ã‚¹åã§æ¤œç´¢
        detailed_metrics = node.get('detailed_metrics', {})
        for metric_key, metric_info in detailed_metrics.items():
            metric_value = metric_info.get('value', 0)
            metric_label = metric_info.get('label', '')
            
            # æ­£ç¢ºãªãƒ¡ãƒˆãƒªã‚¯ã‚¹åã§ã®ã¿ãƒãƒƒãƒãƒ³ã‚°
            if (metric_key in exact_spill_metrics or metric_label in exact_spill_metrics) and metric_value > 0:
                spill_detected = True
                spill_bytes = max(spill_bytes, metric_value)  # æœ€å¤§å€¤ã‚’ä½¿ç”¨
                spill_details.append({
                    'metric_name': metric_key,
                    'value': metric_value,
                    'label': metric_label,
                    'source': 'detailed_metrics',
                    'matched_field': 'key' if metric_key in exact_spill_metrics else 'label',
                    'matched_pattern': metric_key if metric_key in exact_spill_metrics else metric_label
                })
                break  # æœ€åˆã«è¦‹ã¤ã‹ã£ãŸã‚¹ãƒ”ãƒ«ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚’ä½¿ç”¨
        
        # 2. detailed_metricsã§è¦‹ã¤ã‹ã‚‰ãªã„å ´åˆã€ç”Ÿãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‹ã‚‰æ­£ç¢ºãªãƒ¡ãƒˆãƒªã‚¯ã‚¹åã§æ¤œç´¢
        if not spill_detected:
            raw_metrics = node.get('metrics', [])
            for metric in raw_metrics:
                metric_key = metric.get('key', '')
                metric_label = metric.get('label', '')
                metric_value = metric.get('value', 0)
                
                # æ­£ç¢ºãªãƒ¡ãƒˆãƒªã‚¯ã‚¹åã§ã®ã¿ãƒãƒƒãƒãƒ³ã‚°
                if (metric_key in exact_spill_metrics or metric_label in exact_spill_metrics) and metric_value > 0:
                    spill_detected = True
                    spill_bytes = max(spill_bytes, metric_value)  # æœ€å¤§å€¤ã‚’ä½¿ç”¨
                    spill_details.append({
                        'metric_name': metric_key,
                        'value': metric_value,
                        'label': metric_label,
                        'source': 'raw_metrics',
                        'matched_field': 'key' if metric_key in exact_spill_metrics else 'label',
                        'matched_pattern': metric_key if metric_key in exact_spill_metrics else metric_label
                    })
                    break  # æœ€åˆã«è¦‹ã¤ã‹ã£ãŸã‚¹ãƒ”ãƒ«ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚’ä½¿ç”¨
        
        # 3. key_metricsã‹ã‚‰æ­£ç¢ºãªãƒ¡ãƒˆãƒªã‚¯ã‚¹åã§æ¤œç´¢
        if not spill_detected:
            key_metrics = node.get('key_metrics', {})
            for exact_metric in exact_spill_metrics:
                if exact_metric in key_metrics and key_metrics[exact_metric] > 0:
                    spill_detected = True
                    spill_bytes = max(spill_bytes, key_metrics[exact_metric])  # æœ€å¤§å€¤ã‚’ä½¿ç”¨
                    spill_details.append({
                        'metric_name': f"key_metrics.{exact_metric}",
                        'value': key_metrics[exact_metric],
                        'label': f"Key metric: {exact_metric}",
                        'source': 'key_metrics',
                        'matched_field': 'key',
                        'matched_pattern': exact_metric
                    })
                    break
        
        # ãƒ‡ãƒ¼ã‚¿ã‚¹ã‚­ãƒ¥ãƒ¼ã®æ¤œå‡ºï¼ˆAQEãƒ™ãƒ¼ã‚¹ã®ç²¾å¯†åˆ¤å®šï¼‰
        skew_detected = False
        skew_details = []
        skewed_partitions = 0  # ã‚¹ã‚­ãƒ¥ãƒ¼ãƒ‘ãƒ¼ãƒ†ã‚£ã‚·ãƒ§ãƒ³æ•°
        
        # AQEãƒ™ãƒ¼ã‚¹ã‚¹ã‚­ãƒ¥ãƒ¼æ¤œå‡º: "AQEShuffleRead - Number of skewed partitions" > 0
        target_aqe_metrics = [
            "AQEShuffleRead - Number of skewed partitions",
            "AQEShuffleRead - Number of skewed partition splits"
        ]
        
        aqe_skew_value = 0
        aqe_split_value = 0
        aqe_metric_name = ""
        aqe_split_metric_name = ""
        
        # 1. detailed_metricsã§æ¤œç´¢
        detailed_metrics = node.get('detailed_metrics', {})
        for metric_key, metric_info in detailed_metrics.items():
            if metric_key == "AQEShuffleRead - Number of skewed partitions":
                aqe_skew_value = metric_info.get('value', 0)
                aqe_metric_name = metric_key
            elif metric_key == "AQEShuffleRead - Number of skewed partition splits":
                aqe_split_value = metric_info.get('value', 0)
                aqe_split_metric_name = metric_key
            elif metric_info.get('label', '') == "AQEShuffleRead - Number of skewed partitions":
                aqe_skew_value = metric_info.get('value', 0)
                aqe_metric_name = metric_info.get('label', '')
            elif metric_info.get('label', '') == "AQEShuffleRead - Number of skewed partition splits":
                aqe_split_value = metric_info.get('value', 0)
                aqe_split_metric_name = metric_info.get('label', '')
        
        # 2. raw_metricsã§æ¤œç´¢ï¼ˆãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼‰
        if aqe_skew_value == 0 or aqe_split_value == 0:
            raw_metrics = node.get('metrics', [])
            if isinstance(raw_metrics, list):
                for raw_metric in raw_metrics:
                    if isinstance(raw_metric, dict):
                        # 'label'ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã‚’æœ€åˆã«ãƒã‚§ãƒƒã‚¯
                        raw_metric_label = raw_metric.get('label', '')
                        if raw_metric_label == "AQEShuffleRead - Number of skewed partitions" and aqe_skew_value == 0:
                            aqe_skew_value = raw_metric.get('value', 0)
                            aqe_metric_name = raw_metric_label
                        elif raw_metric_label == "AQEShuffleRead - Number of skewed partition splits" and aqe_split_value == 0:
                            aqe_split_value = raw_metric.get('value', 0)
                            aqe_split_metric_name = raw_metric_label
                        
                        # 'key'ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã‚‚ãƒã‚§ãƒƒã‚¯
                        raw_metric_key = raw_metric.get('key', '')
                        if raw_metric_key == "AQEShuffleRead - Number of skewed partitions" and aqe_skew_value == 0:
                            aqe_skew_value = raw_metric.get('value', 0)
                            aqe_metric_name = raw_metric_key
                        elif raw_metric_key == "AQEShuffleRead - Number of skewed partition splits" and aqe_split_value == 0:
                            aqe_split_value = raw_metric.get('value', 0)
                            aqe_split_metric_name = raw_metric_key
                        
                        # 'metricName'ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã‚‚ãƒã‚§ãƒƒã‚¯ï¼ˆå¾“æ¥ã®äº’æ›æ€§ï¼‰
                        raw_metric_name = raw_metric.get('metricName', '')
                        if raw_metric_name == "AQEShuffleRead - Number of skewed partitions" and aqe_skew_value == 0:
                            aqe_skew_value = raw_metric.get('value', 0)
                            aqe_metric_name = raw_metric_name
                        elif raw_metric_name == "AQEShuffleRead - Number of skewed partition splits" and aqe_split_value == 0:
                            aqe_split_value = raw_metric.get('value', 0)
                            aqe_split_metric_name = raw_metric_name
        
        # 3. key_metricsã§æ¤œç´¢ï¼ˆãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼‰
        if aqe_skew_value == 0 or aqe_split_value == 0:
            key_metrics = node.get('key_metrics', {})
            for key_metric_name, key_metric_value in key_metrics.items():
                if "AQEShuffleRead - Number of skewed partitions" in key_metric_name and aqe_skew_value == 0:
                    aqe_skew_value = key_metric_value
                    aqe_metric_name = key_metric_name
                elif "AQEShuffleRead - Number of skewed partition splits" in key_metric_name and aqe_split_value == 0:
                    aqe_split_value = key_metric_value
                    aqe_split_metric_name = key_metric_name
        
        # AQEã‚¹ã‚­ãƒ¥ãƒ¼åˆ¤å®š
        if aqe_skew_value > 0:
            skew_detected = True
            skewed_partitions = aqe_skew_value  # ã‚¹ã‚­ãƒ¥ãƒ¼ãƒ‘ãƒ¼ãƒ†ã‚£ã‚·ãƒ§ãƒ³æ•°ã‚’è¨­å®š
            severity_level = "é«˜" if aqe_skew_value >= 5 else "ä¸­"
            
            # åŸºæœ¬çš„ãªAQEã‚¹ã‚­ãƒ¥ãƒ¼æ¤œå‡ºæƒ…å ±
            description = f'AQEã‚¹ã‚­ãƒ¥ãƒ¼æ¤œå‡º: {aqe_metric_name} = {aqe_skew_value} > åŸºæº–å€¤ 0 [é‡è¦åº¦:{severity_level}]'
            
            # splitå€¤ã‚‚å–å¾—ã§ããŸå ´åˆã€è©³ç´°æƒ…å ±ã‚’è¿½åŠ 
            if aqe_split_value > 0:
                description += f' | AQEæ¤œå‡ºè©³ç´°: SparkãŒè‡ªå‹•çš„ã«{aqe_skew_value}å€‹ã®ã‚¹ã‚­ãƒ¥ãƒ¼ãƒ‘ãƒ¼ãƒ†ã‚£ã‚·ãƒ§ãƒ³ã‚’æ¤œå‡º'
                description += f' | AQEè‡ªå‹•å¯¾å¿œ: SparkãŒè‡ªå‹•çš„ã«{aqe_split_value}å€‹ã®ãƒ‘ãƒ¼ãƒ†ã‚£ã‚·ãƒ§ãƒ³ã«åˆ†å‰²'
            
            skew_details.append({
                'type': 'aqe_skew',
                'value': aqe_skew_value,
                'split_value': aqe_split_value,
                'threshold': 0,
                'metric_name': aqe_metric_name,
                'split_metric_name': aqe_split_metric_name,
                'severity': severity_level,
                'description': description
            })
        
        # AQEãƒ™ãƒ¼ã‚¹ã‚¹ã‚­ãƒ¥ãƒ¼æ¤œå‡ºã®ã¿ä½¿ç”¨ï¼ˆã‚¹ãƒ”ãƒ«ãƒ™ãƒ¼ã‚¹åˆ¤å®šã¯å‰Šé™¤ï¼‰
        # ç†ç”±: AQEShuffleRead - Number of skewed partitions ãŒæ­£ç¢ºãªã‚¹ã‚­ãƒ¥ãƒ¼åˆ¤å®šåŸºæº–
        
        # ä¸¦åˆ—åº¦ã‚¢ã‚¤ã‚³ãƒ³
        parallelism_icon = "ğŸ”¥" if num_tasks >= 10 else "âš ï¸" if num_tasks >= 5 else "ğŸŒ"
        # ã‚¹ãƒ”ãƒ«ã‚¢ã‚¤ã‚³ãƒ³
        spill_icon = "ğŸ’¿" if spill_detected else "âœ…"
        # ã‚¹ã‚­ãƒ¥ãƒ¼ã‚¢ã‚¤ã‚³ãƒ³
        skew_icon = "âš–ï¸" if skew_detected else "âœ…"
        
        print(f"{i+1:2d}. {time_icon}{memory_icon}{parallelism_icon}{spill_icon}{skew_icon} [{severity:8}] {short_name}")
        print(f"    â±ï¸  å®Ÿè¡Œæ™‚é–“: {duration_ms:>8,} ms ({duration_ms/1000:>6.1f} sec) - ç´¯ç©æ™‚é–“ã® {time_percentage:>5.1f}%")
        print(f"    ğŸ“Š å‡¦ç†è¡Œæ•°: {rows_num:>8,} è¡Œ")
        print(f"    ğŸ’¾ ãƒ”ãƒ¼ã‚¯ãƒ¡ãƒ¢ãƒª: {memory_mb:>6.1f} MB")
        # è¤‡æ•°ã®Tasks totalãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚’è¡¨ç¤º
        parallelism_display = []
        for task_metric in parallelism_data.get('all_tasks_metrics', []):
            parallelism_display.append(f"{task_metric['name']}: {task_metric['value']}")
        
        if parallelism_display:
            print(f"    ğŸ”§ ä¸¦åˆ—åº¦: {' | '.join(parallelism_display)}")
        else:
            print(f"    ğŸ”§ ä¸¦åˆ—åº¦: {num_tasks:>3d} ã‚¿ã‚¹ã‚¯")
        
        # ã‚¹ã‚­ãƒ¥ãƒ¼åˆ¤å®šï¼ˆAQEã‚¹ã‚­ãƒ¥ãƒ¼æ¤œå‡ºã¨AQEShuffleReadå¹³å‡ãƒ‘ãƒ¼ãƒ†ã‚£ã‚·ãƒ§ãƒ³ã‚µã‚¤ã‚ºã®ä¸¡æ–¹ã‚’è€ƒæ…®ï¼‰
        aqe_shuffle_skew_warning = parallelism_data.get('aqe_shuffle_skew_warning', False)
        
        if skew_detected:
            skew_status = "AQEã§æ¤œå‡ºãƒ»å¯¾å¿œæ¸ˆ"
        elif aqe_shuffle_skew_warning:
            skew_status = "æ½œåœ¨çš„ãªã‚¹ã‚­ãƒ¥ãƒ¼ã®å¯èƒ½æ€§ã‚ã‚Š"
        else:
            skew_status = "ãªã—"
        
        print(f"    ğŸ’¿ ã‚¹ãƒ”ãƒ«: {'ã‚ã‚Š' if spill_detected else 'ãªã—'} | âš–ï¸ ã‚¹ã‚­ãƒ¥ãƒ¼: {skew_status}")
        
        # AQEShuffleReadãƒ¡ãƒˆãƒªã‚¯ã‚¹ã®è¡¨ç¤º
        aqe_shuffle_metrics = parallelism_data.get('aqe_shuffle_metrics', [])
        if aqe_shuffle_metrics:
            aqe_display = []
            for aqe_metric in aqe_shuffle_metrics:
                if aqe_metric['name'] == "AQEShuffleRead - Number of partitions":
                    aqe_display.append(f"ãƒ‘ãƒ¼ãƒ†ã‚£ã‚·ãƒ§ãƒ³æ•°: {aqe_metric['value']}")
                elif aqe_metric['name'] == "AQEShuffleRead - Partition data size":
                    aqe_display.append(f"ãƒ‡ãƒ¼ã‚¿ã‚µã‚¤ã‚º: {aqe_metric['value']:,} bytes")
            
            if aqe_display:
                print(f"    ğŸ”„ AQEShuffleRead: {' | '.join(aqe_display)}")
                
                # å¹³å‡ãƒ‘ãƒ¼ãƒ†ã‚£ã‚·ãƒ§ãƒ³ã‚µã‚¤ã‚ºã¨è­¦å‘Šè¡¨ç¤º
                avg_partition_size = parallelism_data.get('aqe_shuffle_avg_partition_size', 0)
                if avg_partition_size > 0:
                    avg_size_mb = avg_partition_size / (1024 * 1024)
                    print(f"    ğŸ“Š å¹³å‡ãƒ‘ãƒ¼ãƒ†ã‚£ã‚·ãƒ§ãƒ³ã‚µã‚¤ã‚º: {avg_size_mb:.2f} MB")
                    
                    # 512MBä»¥ä¸Šã®å ´åˆã«è­¦å‘Š
                    if parallelism_data.get('aqe_shuffle_skew_warning', False):
                        print(f"    âš ï¸  ã€è­¦å‘Šã€‘ å¹³å‡ãƒ‘ãƒ¼ãƒ†ã‚£ã‚·ãƒ§ãƒ³ã‚µã‚¤ã‚ºãŒ512MBä»¥ä¸Š - æ½œåœ¨çš„ãªã‚¹ã‚­ãƒ¥ãƒ¼ã®å¯èƒ½æ€§ã‚ã‚Š")
        
        # åŠ¹ç‡æ€§æŒ‡æ¨™ï¼ˆè¡Œ/ç§’ï¼‰ã‚’è¨ˆç®—
        if duration_ms > 0:
            rows_per_sec = (rows_num * 1000) / duration_ms
            print(f"    ğŸš€ å‡¦ç†åŠ¹ç‡: {rows_per_sec:>8,.0f} è¡Œ/ç§’")
        
# ãƒ•ã‚£ãƒ«ã‚¿ç‡è¡¨ç¤ºï¼ˆãƒ‡ãƒãƒƒã‚°æ©Ÿèƒ½ä»˜ãï¼‰
        filter_result = calculate_filter_rate(node)
        filter_display = format_filter_rate_display(filter_result)
        if filter_display:
            print(f"    {filter_display}")
        else:
            # ãƒ‡ãƒãƒƒã‚°æƒ…å ±ï¼šãªãœãƒ•ã‚£ãƒ«ã‚¿ç‡ãŒè¡¨ç¤ºã•ã‚Œãªã„ã‹ã‚’ç¢ºèª
            if filter_result["has_filter_metrics"]:
                print(f"    ğŸ“‚ ãƒ•ã‚£ãƒ«ã‚¿ç‡: {filter_result['filter_rate']:.1%} (èª­ã¿è¾¼ã¿: {filter_result['files_read_bytes']/(1024*1024*1024):.2f}GB, ãƒ—ãƒ«ãƒ¼ãƒ³: {filter_result['files_pruned_bytes']/(1024*1024*1024):.2f}GB)")
            else:
                # ãƒ¡ãƒˆãƒªã‚¯ã‚¹æ¤œç´¢ã®ãƒ‡ãƒãƒƒã‚°
                debug_info = []
                detailed_metrics = node.get('detailed_metrics', {})
                for metric_key, metric_info in detailed_metrics.items():
                    metric_label = metric_info.get('label', '')
                    if 'file' in metric_label.lower() and ('read' in metric_label.lower() or 'prun' in metric_label.lower()):
                        debug_info.append(f"{metric_label}: {metric_info.get('value', 0)}")
                
                if debug_info:
                    print(f"    ğŸ“‚ ãƒ•ã‚£ãƒ«ã‚¿é–¢é€£ãƒ¡ãƒˆãƒªã‚¯ã‚¹æ¤œå‡º: {', '.join(debug_info[:2])}")
                # else:
                #     print(f"    ğŸ“‚ ãƒ•ã‚£ãƒ«ã‚¿ç‡: ãƒ¡ãƒˆãƒªã‚¯ã‚¹æœªæ¤œå‡º")
        
        # ã‚¹ãƒ”ãƒ«è©³ç´°æƒ…å ±ï¼ˆã‚·ãƒ³ãƒ—ãƒ«è¡¨ç¤ºï¼‰
        spill_display = ""
        if spill_detected and spill_bytes > 0:
            spill_mb = spill_bytes / 1024 / 1024
            if spill_mb >= 1024:  # GBå˜ä½
                spill_display = f"{spill_mb/1024:.2f} GB"
            else:  # MBå˜ä½
                spill_display = f"{spill_mb:.1f} MB"
            print(f"    ğŸ’¿ ã‚¹ãƒ”ãƒ«: {spill_display}")
        
        # Shuffleãƒãƒ¼ãƒ‰ã®å ´åˆã¯å¸¸ã«Shuffle attributesã‚’è¡¨ç¤º
        if "shuffle" in short_name.lower():
            shuffle_attributes = extract_shuffle_attributes(node)
            if shuffle_attributes:
                print(f"    ğŸ”„ Shuffleå±æ€§: {', '.join(shuffle_attributes)}")
                
                # REPARTITIONãƒ’ãƒ³ãƒˆã®ææ¡ˆï¼ˆã‚¹ãƒ”ãƒ«ãŒæ¤œå‡ºã•ã‚ŒãŸå ´åˆã®ã¿ï¼‰
                if spill_detected and spill_bytes > 0 and spill_display:
                    suggested_partitions = max(num_tasks * 2, 200)  # æœ€å°200ãƒ‘ãƒ¼ãƒ†ã‚£ã‚·ãƒ§ãƒ³
                    
                    # Shuffleå±æ€§ã§æ¤œå‡ºã•ã‚ŒãŸã‚«ãƒ©ãƒ ã‚’å…¨ã¦ä½¿ç”¨ï¼ˆå®Œå…¨ä¸€è‡´ï¼‰
                    repartition_columns = ", ".join(shuffle_attributes)
                    
                    print(f"    ğŸ’¡ æœ€é©åŒ–ææ¡ˆ: REPARTITION({suggested_partitions}, {repartition_columns})")
                    print(f"       ç†ç”±: ã‚¹ãƒ”ãƒ«({spill_display})ã‚’æ”¹å–„ã™ã‚‹ãŸã‚")
                    print(f"       å¯¾è±¡: Shuffleå±æ€§å…¨{len(shuffle_attributes)}ã‚«ãƒ©ãƒ ã‚’å®Œå…¨ä½¿ç”¨")
            else:
                print(f"    ğŸ”„ Shuffleå±æ€§: è¨­å®šãªã—")
        
        # ã‚¹ã‚­ãƒ£ãƒ³ãƒãƒ¼ãƒ‰ã®å ´åˆã¯ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°ã‚­ãƒ¼ã‚’è¡¨ç¤º
        if "scan" in short_name.lower():
            cluster_attributes = extract_cluster_attributes(node)
            if cluster_attributes:
                print(f"    ğŸ“Š ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°ã‚­ãƒ¼: {', '.join(cluster_attributes)}")
            else:
                print(f"    ğŸ“Š ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°ã‚­ãƒ¼: è¨­å®šãªã—")

        
        # ã‚¹ã‚­ãƒ¥ãƒ¼è©³ç´°æƒ…å ±ï¼ˆç°¡ç•¥è¡¨ç¤ºï¼‰
        if skew_detected and skewed_partitions > 0:
            print(f"    âš–ï¸ ã‚¹ã‚­ãƒ¥ãƒ¼è©³ç´°: {skewed_partitions} å€‹ã®ã‚¹ã‚­ãƒ¥ãƒ¼ãƒ‘ãƒ¼ãƒ†ã‚£ã‚·ãƒ§ãƒ³")
        
        # ãƒãƒ¼ãƒ‰IDã‚‚è¡¨ç¤º
        print(f"    ğŸ†” ãƒãƒ¼ãƒ‰ID: {node.get('node_id', node.get('id', 'N/A'))}")
        print()
        
else:
    print("âš ï¸ ãƒãƒ¼ãƒ‰ãƒ¡ãƒˆãƒªã‚¯ã‚¹ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ")

print()

# ğŸ”¥ Sparkã‚¹ãƒ†ãƒ¼ã‚¸å®Ÿè¡Œåˆ†æ
if extracted_metrics['stage_metrics']:
    print("\nğŸ”¥ Sparkã‚¹ãƒ†ãƒ¼ã‚¸å®Ÿè¡Œåˆ†æ")
    print("=" * 60)
    
    stage_metrics = extracted_metrics['stage_metrics']
    total_stages = len(stage_metrics)
    completed_stages = len([s for s in stage_metrics if s.get('status') == 'COMPLETE'])
    failed_stages = len([s for s in stage_metrics if s.get('num_failed_tasks', 0) > 0])
    
    print(f"ğŸ“Š ã‚¹ãƒ†ãƒ¼ã‚¸æ¦‚è¦: å…¨{total_stages}ã‚¹ãƒ†ãƒ¼ã‚¸ (å®Œäº†:{completed_stages}, å¤±æ•—ã‚¿ã‚¹ã‚¯ã‚ã‚Š:{failed_stages})")
    print()
    
    # ã‚¹ãƒ†ãƒ¼ã‚¸ã‚’å®Ÿè¡Œæ™‚é–“ã§ã‚½ãƒ¼ãƒˆ
    sorted_stages = sorted(stage_metrics, key=lambda x: x.get('duration_ms', 0), reverse=True)
    
    print("â±ï¸ ã‚¹ãƒ†ãƒ¼ã‚¸å®Ÿè¡Œæ™‚é–“ãƒ©ãƒ³ã‚­ãƒ³ã‚°:")
    print("-" * 60)
    
    for i, stage in enumerate(sorted_stages[:5]):  # TOP5ã‚¹ãƒ†ãƒ¼ã‚¸ã®ã¿è¡¨ç¤º
        stage_id = stage.get('stage_id', 'N/A')
        status = stage.get('status', 'UNKNOWN')
        duration_ms = stage.get('duration_ms', 0)
        num_tasks = stage.get('num_tasks', 0)
        failed_tasks = stage.get('num_failed_tasks', 0)
        complete_tasks = stage.get('num_complete_tasks', 0)
        
        # ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹ã«å¿œã˜ãŸã‚¢ã‚¤ã‚³ãƒ³
        if status == 'COMPLETE' and failed_tasks == 0:
            status_icon = "âœ…"
        elif failed_tasks > 0:
            status_icon = "âš ï¸"
        else:
            status_icon = "â“"
        
        # ä¸¦åˆ—åº¦ã‚¢ã‚¤ã‚³ãƒ³
        parallelism_icon = "ğŸ”¥" if num_tasks >= 10 else "âš ï¸" if num_tasks >= 5 else "ğŸŒ"
        
        # å®Ÿè¡Œæ™‚é–“ã®é‡è¦åº¦
        if duration_ms >= 10000:
            time_icon = "ğŸ”´"
            severity = "CRITICAL"
        elif duration_ms >= 5000:
            time_icon = "ğŸŸ "
            severity = "HIGH"
        elif duration_ms >= 1000:
            time_icon = "ğŸŸ¡"
            severity = "MEDIUM"
        else:
            time_icon = "ğŸŸ¢"
            severity = "LOW"
        
        print(f"{i+1}. {status_icon}{parallelism_icon}{time_icon} ã‚¹ãƒ†ãƒ¼ã‚¸ {stage_id} [{severity:8}]")
        print(f"   â±ï¸ å®Ÿè¡Œæ™‚é–“: {duration_ms:,} ms ({duration_ms/1000:.1f} sec)")
        print(f"   ğŸ”§ ã‚¿ã‚¹ã‚¯: {complete_tasks}/{num_tasks} å®Œäº† (å¤±æ•—: {failed_tasks})")
        
        # ã‚¿ã‚¹ã‚¯ã‚ãŸã‚Šã®å¹³å‡æ™‚é–“
        if num_tasks > 0:
            avg_task_time = duration_ms / num_tasks
            print(f"   ğŸ“Š å¹³å‡ã‚¿ã‚¹ã‚¯æ™‚é–“: {avg_task_time:.1f} ms")
        
        # åŠ¹ç‡æ€§è©•ä¾¡
        if num_tasks > 0:
            task_efficiency = "é«˜åŠ¹ç‡" if num_tasks >= 10 and failed_tasks == 0 else "è¦æ”¹å–„" if failed_tasks > 0 else "æ¨™æº–"
            print(f"   ğŸ¯ åŠ¹ç‡æ€§: {task_efficiency}")
        
        print()
    
    if len(sorted_stages) > 5:
        print(f"... ä»– {len(sorted_stages) - 5} ã‚¹ãƒ†ãƒ¼ã‚¸")
    
    # å•é¡Œã®ã‚ã‚‹ã‚¹ãƒ†ãƒ¼ã‚¸ã®ãƒã‚¤ãƒ©ã‚¤ãƒˆ
    problematic_stages = [s for s in stage_metrics if s.get('num_failed_tasks', 0) > 0 or s.get('duration_ms', 0) > 30000]
    if problematic_stages:
        print("\nğŸš¨ æ³¨æ„ãŒå¿…è¦ãªã‚¹ãƒ†ãƒ¼ã‚¸:")
        print("-" * 40)
        for stage in problematic_stages[:3]:
            stage_id = stage.get('stage_id', 'N/A')
            duration_sec = stage.get('duration_ms', 0) / 1000
            failed_tasks = stage.get('num_failed_tasks', 0)
            
            issues = []
            if failed_tasks > 0:
                issues.append(f"å¤±æ•—ã‚¿ã‚¹ã‚¯{failed_tasks}å€‹")
            if duration_sec > 30:
                issues.append(f"é•·æ™‚é–“å®Ÿè¡Œ({duration_sec:.1f}sec)")
            
            print(f"   âš ï¸ ã‚¹ãƒ†ãƒ¼ã‚¸ {stage_id}: {', '.join(issues)}")
    
    
    print()
else:
    print("\nğŸ”¥ Sparkã‚¹ãƒ†ãƒ¼ã‚¸å®Ÿè¡Œåˆ†æ")
    print("=" * 60)
    print("âš ï¸ ã‚¹ãƒ†ãƒ¼ã‚¸ãƒ¡ãƒˆãƒªã‚¯ã‚¹ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ")
    print()

print()

# COMMAND ----------

# MAGIC %md
# MAGIC ## ğŸ—‚ï¸ Liquid Clusteringåˆ†æçµæœã®è©³ç´°è¡¨ç¤º
# MAGIC
# MAGIC ã“ã®ã‚»ãƒ«ã§ã¯ä»¥ä¸‹ã®å‡¦ç†ã‚’å®Ÿè¡Œã—ã¾ã™ï¼š
# MAGIC - ãƒ†ãƒ¼ãƒ–ãƒ«åˆ¥æ¨å¥¨ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°ã‚«ãƒ©ãƒ ã®è©³ç´°è¡¨ç¤º
# MAGIC - ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹å‘ä¸Šè¦‹è¾¼ã¿ã®åˆ†æ
# MAGIC - ã‚«ãƒ©ãƒ ä½¿ç”¨ãƒ‘ã‚¿ãƒ¼ãƒ³ã®è©³ç´°åˆ†æ
# MAGIC - ãƒ—ãƒƒã‚·ãƒ¥ãƒ€ã‚¦ãƒ³ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼æƒ…å ±ã®è¡¨ç¤º
# MAGIC - SQLå®Ÿè£…ä¾‹ã®æç¤º

# COMMAND ----------

# ğŸ—‚ï¸ LLMã«ã‚ˆã‚‹Liquid Clusteringåˆ†æçµæœã®è©³ç´°è¡¨ç¤º
print("\n" + "=" * 50)
print("ğŸ¤– LLM Liquid Clusteringæ¨å¥¨åˆ†æ")
print("=" * 50)

# LLMãƒ™ãƒ¼ã‚¹ã®Liquid Clusteringåˆ†æã‚’å®Ÿè¡Œ
liquid_analysis = extracted_metrics['liquid_clustering_analysis']

# LLMåˆ†æçµæœã‚’è¡¨ç¤º
print("\nğŸ¤– LLMåˆ†æçµæœ:")
print("=" * 50)
llm_analysis = liquid_analysis.get('llm_analysis', '')
if llm_analysis:
    print(llm_analysis)
else:
    print("âŒ LLMåˆ†æçµæœãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")

# æŠ½å‡ºãƒ‡ãƒ¼ã‚¿ã®æ¦‚è¦ã‚’è¡¨ç¤º
extracted_data = liquid_analysis.get('extracted_data', {})
metadata_summary = extracted_data.get('metadata_summary', {})

print(f"\nğŸ“Š æŠ½å‡ºãƒ‡ãƒ¼ã‚¿æ¦‚è¦:")
print(f"   ğŸ” ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼æ¡ä»¶: {metadata_summary.get('filter_expressions_count', 0)}å€‹")
print(f"   ğŸ”— JOINæ¡ä»¶: {metadata_summary.get('join_expressions_count', 0)}å€‹")
print(f"   ğŸ“Š GROUP BYæ¡ä»¶: {metadata_summary.get('groupby_expressions_count', 0)}å€‹")
print(f"   ğŸ“ˆ é›†ç´„é–¢æ•°: {metadata_summary.get('aggregate_expressions_count', 0)}å€‹")
print(f"   ğŸ·ï¸ è­˜åˆ¥ãƒ†ãƒ¼ãƒ–ãƒ«: {metadata_summary.get('tables_identified', 0)}å€‹")
print(f"   ğŸ“‚ ã‚¹ã‚­ãƒ£ãƒ³ãƒãƒ¼ãƒ‰: {metadata_summary.get('scan_nodes_count', 0)}å€‹")

# ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã®è¡¨ç¤º
performance_context = liquid_analysis.get('performance_context', {})
print(f"\nâš¡ ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æƒ…å ±:")
print(f"   â±ï¸ å®Ÿè¡Œæ™‚é–“: {performance_context.get('total_time_sec', 0):.1f}ç§’")
print(f"   ğŸ’¾ ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿: {performance_context.get('read_gb', 0):.2f}GB")
print(f"   ğŸ“Š å‡ºåŠ›è¡Œæ•°: {performance_context.get('rows_produced', 0):,}è¡Œ")
print(f"   ğŸ¯ ãƒ•ã‚£ãƒ«ã‚¿ç‡: {performance_context.get('data_selectivity', 0):.4f}")

# åˆ†æçµæœã‚’ãƒ•ã‚¡ã‚¤ãƒ«ã«å‡ºåŠ›
print(f"\nğŸ’¾ åˆ†æçµæœã‚’ãƒ•ã‚¡ã‚¤ãƒ«ã«å‡ºåŠ›ä¸­...")
try:
    saved_files = save_liquid_clustering_analysis(liquid_analysis, "/tmp")
    
    if "error" in saved_files:
        print(f"âŒ ãƒ•ã‚¡ã‚¤ãƒ«å‡ºåŠ›ã‚¨ãƒ©ãƒ¼: {saved_files['error']}")
    else:
        print(f"âœ… ãƒ•ã‚¡ã‚¤ãƒ«å‡ºåŠ›å®Œäº†:")
        for file_type, file_path in saved_files.items():
            if file_type == "json":
                print(f"   ğŸ“„ JSONè©³ç´°ãƒ‡ãƒ¼ã‚¿: {file_path}")
            elif file_type == "markdown":
                print(f"   ğŸ“ Markdownãƒ¬ãƒãƒ¼ãƒˆ: {file_path}")
            elif file_type == "sql":
                print(f"   ğŸ”§ SQLå®Ÿè£…ä¾‹: {file_path}")
                
except Exception as e:
    print(f"âŒ ãƒ•ã‚¡ã‚¤ãƒ«å‡ºåŠ›ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(e)}")

# ã‚µãƒãƒªãƒ¼æƒ…å ±
summary = liquid_analysis.get('summary', {})
print(f"\nğŸ“‹ åˆ†æã‚µãƒãƒªãƒ¼:")
print(f"   ğŸ”¬ åˆ†ææ–¹æ³•: {summary.get('analysis_method', 'Unknown')}")
print(f"   ğŸ¤– LLMãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼: {summary.get('llm_provider', 'Unknown')}")
print(f"   ğŸ“Š å¯¾è±¡ãƒ†ãƒ¼ãƒ–ãƒ«æ•°: {summary.get('tables_identified', 0)}")
print(f"   ğŸ“ˆ æŠ½å‡ºã‚«ãƒ©ãƒ æ•°: ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼({summary.get('total_filter_columns', 0)}) + JOIN({summary.get('total_join_columns', 0)}) + GROUP BY({summary.get('total_groupby_columns', 0)})")

print()

# COMMAND ----------

# ğŸ¤– è¨­å®šã•ã‚ŒãŸLLMã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆã‚’ä½¿ç”¨ã—ã¦ãƒœãƒˆãƒ«ãƒãƒƒã‚¯åˆ†æ
provider = LLM_CONFIG["provider"]
if provider == "databricks":
    endpoint_name = LLM_CONFIG["databricks"]["endpoint_name"]
    print(f"ğŸ¤– Databricks Model Serving ({endpoint_name}) ã«ã‚ˆã‚‹ãƒœãƒˆãƒ«ãƒãƒƒã‚¯åˆ†æã‚’é–‹å§‹ã—ã¾ã™...")
    print(f"âš ï¸  Model Servingã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆ '{endpoint_name}' ãŒå¿…è¦ã§ã™")
elif provider == "openai":
    model = LLM_CONFIG["openai"]["model"]
    print(f"ğŸ¤– OpenAI ({model}) ã«ã‚ˆã‚‹ãƒœãƒˆãƒ«ãƒãƒƒã‚¯åˆ†æã‚’é–‹å§‹ã—ã¾ã™...")
    print("âš ï¸  OpenAI APIã‚­ãƒ¼ãŒå¿…è¦ã§ã™")
elif provider == "azure_openai":
    deployment = LLM_CONFIG["azure_openai"]["deployment_name"]
    print(f"ğŸ¤– Azure OpenAI ({deployment}) ã«ã‚ˆã‚‹ãƒœãƒˆãƒ«ãƒãƒƒã‚¯åˆ†æã‚’é–‹å§‹ã—ã¾ã™...")
    print("âš ï¸  Azure OpenAI APIã‚­ãƒ¼ã¨ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆãŒå¿…è¦ã§ã™")
elif provider == "anthropic":
    model = LLM_CONFIG["anthropic"]["model"]
    print(f"ğŸ¤– Anthropic ({model}) ã«ã‚ˆã‚‹ãƒœãƒˆãƒ«ãƒãƒƒã‚¯åˆ†æã‚’é–‹å§‹ã—ã¾ã™...")
    print("âš ï¸  Anthropic APIã‚­ãƒ¼ãŒå¿…è¦ã§ã™")

print("ğŸ“ åˆ†æãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’ç°¡æ½”åŒ–ã—ã¦ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆãƒªã‚¹ã‚¯ã‚’è»½æ¸›ã—ã¦ã„ã¾ã™...")
print()

analysis_result = analyze_bottlenecks_with_llm(extracted_metrics)

# COMMAND ----------

# MAGIC %md
# MAGIC ## ğŸ¯ LLMãƒœãƒˆãƒ«ãƒãƒƒã‚¯åˆ†æçµæœã®è¡¨ç¤º
# MAGIC
# MAGIC ã“ã®ã‚»ãƒ«ã§ã¯ä»¥ä¸‹ã®å‡¦ç†ã‚’å®Ÿè¡Œã—ã¾ã™ï¼š
# MAGIC - è¨­å®šã•ã‚ŒãŸLLMãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ã«ã‚ˆã‚‹è©³ç´°åˆ†æçµæœã®è¡¨ç¤º
# MAGIC - ãƒœãƒˆãƒ«ãƒãƒƒã‚¯ç‰¹å®šã¨æ”¹å–„ææ¡ˆã®å¯è¦–åŒ–
# MAGIC - åˆ†æçµæœã®æ•´å½¢ã¨èª­ã¿ã‚„ã™ã„è¡¨ç¤º

# COMMAND ----------

# ğŸ“Š åˆ†æçµæœã®è¡¨ç¤º
print("\n" + "=" * 80)
print(f"ğŸ¯ ã€{provider.upper()} LLM ã«ã‚ˆã‚‹ SQLãƒœãƒˆãƒ«ãƒãƒƒã‚¯åˆ†æçµæœã€‘")
print("=" * 80)
print()
print(analysis_result)
print()
print("=" * 80)

# COMMAND ----------

# MAGIC %md
# MAGIC ## ğŸ’¾ åˆ†æçµæœã®ä¿å­˜ã¨å®Œäº†ã‚µãƒãƒªãƒ¼
# MAGIC
# MAGIC ã“ã®ã‚»ãƒ«ã§ã¯ä»¥ä¸‹ã®å‡¦ç†ã‚’å®Ÿè¡Œã—ã¾ã™ï¼š
# MAGIC - LLMåˆ†æçµæœã®ãƒ†ã‚­ã‚¹ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ã¸ã®ä¿å­˜
# MAGIC - åˆ†æå¯¾è±¡ã®åŸºæœ¬æƒ…å ±ã®è¨˜éŒ²
# MAGIC - å…¨ä½“å‡¦ç†ã®å®Œäº†ã‚µãƒãƒªãƒ¼è¡¨ç¤º
# MAGIC - å‡ºåŠ›ãƒ•ã‚¡ã‚¤ãƒ«ã®ä¸€è¦§è¡¨ç¤º

# COMMAND ----------

# ğŸ’¾ åˆ†æçµæœã®ä¿å­˜ã¨å®Œäº†ã‚µãƒãƒªãƒ¼
from datetime import datetime
# output_bottleneck_analysis_result_XXX.txtãƒ•ã‚¡ã‚¤ãƒ«ã®å‡ºåŠ›ã¯å»ƒæ­¢ï¼ˆoptimization_reportã«çµ±åˆï¼‰

# æœ€çµ‚çš„ãªã‚µãƒãƒªãƒ¼
print("\n" + "ğŸ‰" * 20)
print("ğŸ ã€å‡¦ç†å®Œäº†ã‚µãƒãƒªãƒ¼ã€‘")
print("ğŸ‰" * 20)
print("âœ… SQLãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ©ãƒ¼JSONãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿å®Œäº†")
print(f"âœ… ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ¡ãƒˆãƒªã‚¯ã‚¹æŠ½å‡ºå®Œäº†")

# LLMãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼æƒ…å ±ã®å‹•çš„è¡¨ç¤º
try:
    current_provider = LLM_CONFIG.get('provider', 'unknown')
    provider_display_names = {
        'databricks': f"Databricks ({LLM_CONFIG.get('databricks', {}).get('endpoint_name', 'Model Serving')})",
        'openai': f"OpenAI ({LLM_CONFIG.get('openai', {}).get('model', 'GPT-4')})",
        'azure_openai': f"Azure OpenAI ({LLM_CONFIG.get('azure_openai', {}).get('deployment_name', 'GPT-4')})",
        'anthropic': f"Anthropic ({LLM_CONFIG.get('anthropic', {}).get('model', 'Claude')})"
    }
    provider_display = provider_display_names.get(current_provider, f"{current_provider}ï¼ˆæœªçŸ¥ã®ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ï¼‰")
    print(f"âœ… {provider_display}ã«ã‚ˆã‚‹ãƒœãƒˆãƒ«ãƒãƒƒã‚¯åˆ†æå®Œäº†")
except Exception as e:
    print("âœ… LLMã«ã‚ˆã‚‹ãƒœãƒˆãƒ«ãƒãƒƒã‚¯åˆ†æå®Œäº†")

print("âœ… åˆ†æçµæœã¯å¾Œã§optimization_reportã«çµ±åˆã•ã‚Œã¾ã™")
print()
print("ğŸš€ åˆ†æå®Œäº†ï¼çµæœã‚’ç¢ºèªã—ã¦ã‚¯ã‚¨ãƒªæœ€é©åŒ–ã«ãŠå½¹ç«‹ã¦ãã ã•ã„ã€‚")
print("ğŸ‰" * 20)

# COMMAND ----------

# MAGIC %md
# MAGIC # ğŸ”§ SQLæœ€é©åŒ–æ©Ÿèƒ½ã‚»ã‚¯ã‚·ãƒ§ãƒ³
# MAGIC
# MAGIC **ã“ã®ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã§ã¯SQLã‚¯ã‚¨ãƒªã®æœ€é©åŒ–ã‚’è¡Œã„ã¾ã™**
# MAGIC
# MAGIC ğŸ“‹ **æœ€é©åŒ–ãƒ—ãƒ­ã‚»ã‚¹:**
# MAGIC - ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ©ãƒ¼ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰ã‚ªãƒªã‚¸ãƒŠãƒ«ã‚¯ã‚¨ãƒªã®æŠ½å‡º
# MAGIC - LLMã«ã‚ˆã‚‹ã‚¯ã‚¨ãƒªæœ€é©åŒ–ã®å®Ÿè¡Œ
# MAGIC - æœ€é©åŒ–çµæœã®ãƒ•ã‚¡ã‚¤ãƒ«ç”Ÿæˆ
# MAGIC - ãƒ†ã‚¹ãƒˆå®Ÿè¡Œã®æº–å‚™
# MAGIC
# MAGIC âš ï¸ **å‰ææ¡ä»¶:** ãƒ¡ã‚¤ãƒ³å‡¦ç†ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã‚’å®Œäº†ã—ã¦ã‹ã‚‰å®Ÿè¡Œã—ã¦ãã ã•ã„

# COMMAND ----------

# MAGIC %md
# MAGIC ## ğŸ”§ SQLæœ€é©åŒ–é–¢é€£é–¢æ•°å®šç¾©
# MAGIC
# MAGIC ã“ã®ã‚»ãƒ«ã§ã¯ä»¥ä¸‹ã®é–¢æ•°ã‚’å®šç¾©ã—ã¾ã™ï¼š
# MAGIC - `extract_original_query_from_profiler_data`: ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ©ãƒ¼ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰ã‚ªãƒªã‚¸ãƒŠãƒ«ã‚¯ã‚¨ãƒªã‚’æŠ½å‡º
# MAGIC - `generate_optimized_query_with_llm`: LLMåˆ†æçµæœã«åŸºã¥ãã‚¯ã‚¨ãƒªæœ€é©åŒ–
# MAGIC - `save_optimized_sql_files`: æœ€é©åŒ–çµæœã®å„ç¨®ãƒ•ã‚¡ã‚¤ãƒ«ä¿å­˜

# COMMAND ----------

def extract_original_query_from_profiler_data(profiler_data: Dict[str, Any]) -> str:
    """
    ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ©ãƒ¼ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰ã‚ªãƒªã‚¸ãƒŠãƒ«ã‚¯ã‚¨ãƒªã‚’æŠ½å‡º
    """
    
    # è¤‡æ•°ã®å ´æ‰€ã‹ã‚‰SQLã‚¯ã‚¨ãƒªã‚’æ¢ã™
    query_candidates = []
    
    # 1. query.queryText ã‹ã‚‰æŠ½å‡º
    if 'query' in profiler_data and 'queryText' in profiler_data['query']:
        query_text = profiler_data['query']['queryText']
        if query_text and query_text.strip():
            query_candidates.append(query_text.strip())
    
    # 2. metadata ã‹ã‚‰æŠ½å‡º
    if 'metadata' in profiler_data:
        metadata = profiler_data['metadata']
        for key, value in metadata.items():
            if 'sql' in key.lower() or 'query' in key.lower():
                if isinstance(value, str) and value.strip():
                    query_candidates.append(value.strip())
    
    # 3. graphs ã® metadata ã‹ã‚‰æŠ½å‡º
    if 'graphs' in profiler_data:
        for graph in profiler_data['graphs']:
            nodes = graph.get('nodes', [])
            for node in nodes:
                node_metadata = node.get('metadata', [])
                for meta in node_metadata:
                    if meta.get('key', '').upper() in ['SQL', 'QUERY', 'SQL_TEXT']:
                        value = meta.get('value', '')
                        if value and value.strip():
                            query_candidates.append(value.strip())
    
    # æœ€ã‚‚é•·ã„ã‚¯ã‚¨ãƒªã‚’é¸æŠï¼ˆé€šå¸¸ã€æœ€ã‚‚å®Œå…¨ãªã‚¯ã‚¨ãƒªï¼‰
    if query_candidates:
        original_query = max(query_candidates, key=len)
        return original_query
    
    return ""

def extract_table_size_estimates_from_plan(profiler_data: Dict[str, Any]) -> Dict[str, Dict[str, Any]]:
    """
    å®Ÿè¡Œãƒ—ãƒ©ãƒ³ã‹ã‚‰ãƒ†ãƒ¼ãƒ–ãƒ«ã”ã¨ã®æ¨å®šã‚µã‚¤ã‚ºæƒ…å ±ã‚’æŠ½å‡º
    
    æ³¨æ„: Databricksã‚¯ã‚¨ãƒªãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ«ã«ã¯ estimatedSizeInBytes ãŒå«ã¾ã‚Œã¦ã„ãªã„ãŸã‚ã€
    ã“ã®æ©Ÿèƒ½ã¯ç¾åœ¨ç„¡åŠ¹åŒ–ã•ã‚Œã¦ã„ã¾ã™ã€‚ãƒ¡ãƒˆãƒªã‚¯ã‚¹ãƒ™ãƒ¼ã‚¹ã®æ¨å®šã‚’ä½¿ç”¨ã—ã¦ãã ã•ã„ã€‚
    
    Args:
        profiler_data: ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ©ãƒ¼ãƒ‡ãƒ¼ã‚¿
        
    Returns:
        Dict: ç©ºã®è¾æ›¸ï¼ˆæ©Ÿèƒ½ç„¡åŠ¹åŒ–ï¼‰
    """
    # Databricksã‚¯ã‚¨ãƒªãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ«ã«estimatedSizeInBytesãŒå«ã¾ã‚Œã¦ã„ãªã„ãŸã‚ç„¡åŠ¹åŒ–
    return {}

def extract_table_name_from_scan_node(node: Dict[str, Any]) -> str:
    """
    ã‚¹ã‚­ãƒ£ãƒ³ãƒãƒ¼ãƒ‰ã‹ã‚‰ãƒ†ãƒ¼ãƒ–ãƒ«åã‚’æŠ½å‡º
    
    Args:
        node: å®Ÿè¡Œãƒ—ãƒ©ãƒ³ã®ãƒãƒ¼ãƒ‰
        
    Returns:
        str: ãƒ†ãƒ¼ãƒ–ãƒ«å
    """
    try:
        # è¤‡æ•°ã®æ–¹æ³•ã§ãƒ†ãƒ¼ãƒ–ãƒ«åã‚’æŠ½å‡ºã‚’è©¦è¡Œ
        
        # 1. node outputã‹ã‚‰ã®æŠ½å‡º
        output = node.get("output", "")
        if output:
            # ãƒ‘ã‚¿ãƒ¼ãƒ³: [col1#123, col2#456] table_name
            import re
            table_match = re.search(r'\]\s+([a-zA-Z_][a-zA-Z0-9_]*(?:\.[a-zA-Z_][a-zA-Z0-9_]*)*)', output)
            if table_match:
                return table_match.group(1)
        
        # 2. nodeè©³ç´°ã‹ã‚‰ã®æŠ½å‡º
        details = node.get("details", "")
        if details:
            # ãƒ‘ã‚¿ãƒ¼ãƒ³: Location: /path/to/table/name
            location_match = re.search(r'Location:.*?([a-zA-Z_][a-zA-Z0-9_]*)', details)
            if location_match:
                return location_match.group(1)
            
            # ãƒ‘ã‚¿ãƒ¼ãƒ³: Table: database.table_name
            table_match = re.search(r'Table:\s+([a-zA-Z_][a-zA-Z0-9_]*(?:\.[a-zA-Z_][a-zA-Z0-9_]*)*)', details)
            if table_match:
                return table_match.group(1)
        
        # 3. ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰ã®æŠ½å‡º
        metadata = node.get("metadata", [])
        for meta in metadata:
            if meta.get("key") == "table" or meta.get("key") == "relation":
                values = meta.get("values", [])
                if values:
                    return str(values[0])
        
        # 4. nodeåã‹ã‚‰ã®æ¨æ¸¬ï¼ˆæœ€å¾Œã®æ‰‹æ®µï¼‰
        node_name = node.get("nodeName", "")
        if "delta" in node_name.lower():
            # Delta Scan ã®å ´åˆã€è©³ç´°æƒ…å ±ã‹ã‚‰æŠ½å‡º
            pass
    
    except Exception as e:
        print(f"âš ï¸ ãƒ†ãƒ¼ãƒ–ãƒ«åæŠ½å‡ºã§ã‚¨ãƒ©ãƒ¼: {str(e)}")
    
    return None

def extract_broadcast_table_names(profiler_data: Dict[str, Any], broadcast_nodes: list) -> Dict[str, Any]:
    """
    BROADCASTãƒãƒ¼ãƒ‰ã‹ã‚‰é–¢é€£ã™ã‚‹ãƒ†ãƒ¼ãƒ–ãƒ«åã‚’æŠ½å‡º
    """
    broadcast_table_info = {
        "broadcast_tables": [],
        "broadcast_table_mapping": {},
        "broadcast_nodes_with_tables": []
    }
    
    # å®Ÿè¡Œãƒ—ãƒ©ãƒ³ã®ã‚°ãƒ©ãƒ•æƒ…å ±ã‚’å–å¾—
    graphs = profiler_data.get('graphs', [])
    if not graphs:
        return broadcast_table_info
    
    # å…¨ãƒãƒ¼ãƒ‰ã‚’åé›†
    all_nodes = []
    for graph in graphs:
        nodes = graph.get('nodes', [])
        all_nodes.extend(nodes)
    
    # ã‚¨ãƒƒã‚¸æƒ…å ±ã‚’åé›†ï¼ˆãƒãƒ¼ãƒ‰é–“ã®é–¢ä¿‚ï¼‰
    all_edges = []
    for graph in graphs:
        edges = graph.get('edges', [])
        all_edges.extend(edges)
    
    # å„BROADCASTãƒãƒ¼ãƒ‰ã«ã¤ã„ã¦é–¢é€£ã™ã‚‹ãƒ†ãƒ¼ãƒ–ãƒ«ã‚’ç‰¹å®š
    for broadcast_node in broadcast_nodes:
        broadcast_node_id = broadcast_node.get('node_id', '')
        broadcast_node_name = broadcast_node.get('node_name', '')
        
        # BROADCASTãƒãƒ¼ãƒ‰ã‹ã‚‰ç›´æ¥ãƒ†ãƒ¼ãƒ–ãƒ«åã‚’æŠ½å‡º
        table_names = set()
        
        # 1. ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰ãƒ†ãƒ¼ãƒ–ãƒ«åã‚’æŠ½å‡º
        metadata = broadcast_node.get('metadata', [])
        for meta in metadata:
            key = meta.get('key', '')
            value = meta.get('value', '')
            values = meta.get('values', [])
            
            # ãƒ†ãƒ¼ãƒ–ãƒ«åã‚’ç¤ºã™ã‚­ãƒ¼ã‚’ãƒã‚§ãƒƒã‚¯
            if key in ['SCAN_IDENTIFIER', 'TABLE_NAME', 'RELATION']:
                if value:
                    table_names.add(value)
                table_names.update(values)
        
        # 2. ãƒãƒ¼ãƒ‰åã‹ã‚‰ãƒ†ãƒ¼ãƒ–ãƒ«åã‚’æ¨å®š
        if 'SCAN' in broadcast_node_name:
            # "Broadcast Scan delta orders" â†’ "orders"
            import re
            table_match = re.search(r'SCAN\s+(?:DELTA|PARQUET|JSON|CSV)?\s*([a-zA-Z_][a-zA-Z0-9_]*(?:\.[a-zA-Z_][a-zA-Z0-9_]*)*)', broadcast_node_name, re.IGNORECASE)
            if table_match:
                table_names.add(table_match.group(1))
        
        # 3. ã‚¨ãƒƒã‚¸æƒ…å ±ã‹ã‚‰é–¢é€£ã™ã‚‹ã‚¹ã‚­ãƒ£ãƒ³ãƒãƒ¼ãƒ‰ã‚’ç‰¹å®š
        for edge in all_edges:
            source_id = edge.get('source', '')
            target_id = edge.get('target', '')
            
            # BROADCASTãƒãƒ¼ãƒ‰ã«å…¥åŠ›ã•ã‚Œã‚‹ãƒãƒ¼ãƒ‰ã‚’æ¤œç´¢
            if target_id == broadcast_node_id:
                # å…¥åŠ›ãƒãƒ¼ãƒ‰ãŒã‚¹ã‚­ãƒ£ãƒ³ãƒãƒ¼ãƒ‰ã‹ãƒã‚§ãƒƒã‚¯
                for node in all_nodes:
                    if node.get('id', '') == source_id:
                        node_name = node.get('name', '').upper()
                        if any(keyword in node_name for keyword in ['SCAN', 'FILESCAN']):
                            # ã‚¹ã‚­ãƒ£ãƒ³ãƒãƒ¼ãƒ‰ã‹ã‚‰ãƒ†ãƒ¼ãƒ–ãƒ«åã‚’æŠ½å‡º
                            scan_table_name = extract_table_name_from_scan_node(node)
                            if scan_table_name:
                                table_names.add(scan_table_name)
        
        # 4. åŒã˜ã‚°ãƒ©ãƒ•å†…ã®ã‚¹ã‚­ãƒ£ãƒ³ãƒãƒ¼ãƒ‰ã¨ã®é–¢é€£ä»˜ã‘
        for node in all_nodes:
            node_name = node.get('name', '').upper()
            if any(keyword in node_name for keyword in ['SCAN', 'FILESCAN']):
                # ã‚¹ã‚­ãƒ£ãƒ³ãƒãƒ¼ãƒ‰ã®åå‰ãŒBROADCASTãƒãƒ¼ãƒ‰åã«å«ã¾ã‚Œã‚‹ã‹ãƒã‚§ãƒƒã‚¯
                scan_table_name = extract_table_name_from_scan_node(node)
                if scan_table_name:
                    # ãƒ†ãƒ¼ãƒ–ãƒ«åã®éƒ¨åˆ†ä¸€è‡´ã‚’ãƒã‚§ãƒƒã‚¯
                    if any(part in broadcast_node_name for part in scan_table_name.split('.') if len(part) > 2):
                        table_names.add(scan_table_name)
        
        # çµæœã‚’è¨˜éŒ²
        table_names_list = list(table_names)
        if table_names_list:
            broadcast_table_info["broadcast_tables"].extend(table_names_list)
            broadcast_table_info["broadcast_table_mapping"][broadcast_node_id] = table_names_list
            
            # BROADCASTãƒãƒ¼ãƒ‰æƒ…å ±ã‚’æ‹¡å¼µ
            enhanced_broadcast_node = broadcast_node.copy()
            enhanced_broadcast_node["associated_tables"] = table_names_list
            enhanced_broadcast_node["table_count"] = len(table_names_list)
            broadcast_table_info["broadcast_nodes_with_tables"].append(enhanced_broadcast_node)
    
    # é‡è¤‡ã‚’é™¤å»
    broadcast_table_info["broadcast_tables"] = list(set(broadcast_table_info["broadcast_tables"]))
    
    return broadcast_table_info

def extract_execution_plan_info(profiler_data: Dict[str, Any]) -> Dict[str, Any]:
    """
    JSONãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‹ã‚‰å®Ÿè¡Œãƒ—ãƒ©ãƒ³æƒ…å ±ã‚’æŠ½å‡º
    """
    plan_info = {
        "broadcast_nodes": [],
        "join_nodes": [],
        "scan_nodes": [],
        "shuffle_nodes": [],
        "aggregate_nodes": [],
        "plan_summary": {},
        "broadcast_already_applied": False,
        "join_strategies": [],
        "table_scan_details": {},
        "broadcast_table_info": {}
    }
    
    # ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ©ãƒ¼ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰å®Ÿè¡Œã‚°ãƒ©ãƒ•æƒ…å ±ã‚’å–å¾—
    graphs = profiler_data.get('graphs', [])
    if not graphs:
        return plan_info
    
    # ã™ã¹ã¦ã®ã‚°ãƒ©ãƒ•ã‹ã‚‰ãƒãƒ¼ãƒ‰ã‚’åé›†
    all_nodes = []
    for graph_index, graph in enumerate(graphs):
        nodes = graph.get('nodes', [])
        for node in nodes:
            node['graph_index'] = graph_index
            all_nodes.append(node)
    
    # ãƒãƒ¼ãƒ‰åˆ†æ
    for node in all_nodes:
        node_name = node.get('name', '').upper()
        node_tag = node.get('tag', '').upper()
        node_metadata = node.get('metadata', [])
        
        # BROADCASTãƒãƒ¼ãƒ‰ã®æ¤œå‡º
        if 'BROADCAST' in node_name or 'BROADCAST' in node_tag:
            plan_info["broadcast_already_applied"] = True
            broadcast_info = {
                "node_name": node_name,
                "node_tag": node_tag,
                "node_id": node.get('id', ''),
                "metadata": []
            }
            
            # BROADCASTã«é–¢é€£ã™ã‚‹ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚’æŠ½å‡º
            for meta in node_metadata:
                key = meta.get('key', '')
                value = meta.get('value', '')
                values = meta.get('values', [])
                
                if any(keyword in key.upper() for keyword in ['BROADCAST', 'BUILD', 'PROBE']):
                    broadcast_info["metadata"].append({
                        "key": key,
                        "value": value,
                        "values": values
                    })
            
            plan_info["broadcast_nodes"].append(broadcast_info)
        
        # JOINãƒãƒ¼ãƒ‰ã®æ¤œå‡ºã¨æˆ¦ç•¥åˆ†æ
        elif any(keyword in node_name for keyword in ['JOIN', 'HASH']):
            join_info = {
                "node_name": node_name,
                "node_tag": node_tag,
                "node_id": node.get('id', ''),
                "join_strategy": "unknown",
                "join_keys": [],
                "join_type": "unknown"
            }
            
            # JOINæˆ¦ç•¥ã®ç‰¹å®š
            if 'BROADCAST' in node_name:
                join_info["join_strategy"] = "broadcast_hash_join"
            elif 'SORT' in node_name and 'MERGE' in node_name:
                join_info["join_strategy"] = "sort_merge_join"
            elif 'HASH' in node_name:
                join_info["join_strategy"] = "shuffle_hash_join"
            elif 'NESTED' in node_name:
                join_info["join_strategy"] = "broadcast_nested_loop_join"
            
            # JOINã‚¿ã‚¤ãƒ—ã®ç‰¹å®š
            if 'INNER' in node_name:
                join_info["join_type"] = "inner"
            elif 'LEFT' in node_name:
                join_info["join_type"] = "left"
            elif 'RIGHT' in node_name:
                join_info["join_type"] = "right"
            elif 'OUTER' in node_name:
                join_info["join_type"] = "outer"
            
            # JOINæ¡ä»¶ã®æŠ½å‡º
            for meta in node_metadata:
                key = meta.get('key', '')
                values = meta.get('values', [])
                
                if key in ['LEFT_KEYS', 'RIGHT_KEYS']:
                    join_info["join_keys"].extend(values)
            
            plan_info["join_nodes"].append(join_info)
            plan_info["join_strategies"].append(join_info["join_strategy"])
        
        # ã‚¹ã‚­ãƒ£ãƒ³ãƒãƒ¼ãƒ‰ã®è©³ç´°åˆ†æ
        elif any(keyword in node_name for keyword in ['SCAN', 'FILESCAN']):
            scan_info = {
                "node_name": node_name,
                "node_tag": node_tag,
                "node_id": node.get('id', ''),
                "table_name": "unknown",
                "file_format": "unknown",
                "pushed_filters": [],
                "output_columns": []
            }
            
            # ãƒ†ãƒ¼ãƒ–ãƒ«åã¨ãƒ•ã‚¡ã‚¤ãƒ«å½¢å¼ã®æŠ½å‡º
            for meta in node_metadata:
                key = meta.get('key', '')
                value = meta.get('value', '')
                values = meta.get('values', [])
                
                if key == 'SCAN_IDENTIFIER':
                    scan_info["table_name"] = value
                elif key == 'OUTPUT':
                    scan_info["output_columns"] = values
                elif key == 'PUSHED_FILTERS' or key == 'FILTERS':
                    scan_info["pushed_filters"] = values
            
            # ãƒ•ã‚¡ã‚¤ãƒ«å½¢å¼ã®æ¨å®š
            if 'DELTA' in node_name:
                scan_info["file_format"] = "delta"
            elif 'PARQUET' in node_name:
                scan_info["file_format"] = "parquet"
            elif 'JSON' in node_name:
                scan_info["file_format"] = "json"
            elif 'CSV' in node_name:
                scan_info["file_format"] = "csv"
            
            plan_info["scan_nodes"].append(scan_info)
            plan_info["table_scan_details"][scan_info["table_name"]] = scan_info
        
        # ã‚·ãƒ£ãƒƒãƒ•ãƒ«ãƒãƒ¼ãƒ‰ã®æ¤œå‡º
        elif any(keyword in node_name for keyword in ['SHUFFLE', 'EXCHANGE']):
            shuffle_info = {
                "node_name": node_name,
                "node_tag": node_tag,
                "node_id": node.get('id', ''),
                "partition_keys": []
            }
            
            # ãƒ‘ãƒ¼ãƒ†ã‚£ã‚·ãƒ§ãƒ³æƒ…å ±ã®æŠ½å‡º
            for meta in node_metadata:
                key = meta.get('key', '')
                values = meta.get('values', [])
                
                if key in ['PARTITION_EXPRESSIONS', 'PARTITION_KEYS']:
                    shuffle_info["partition_keys"] = values
            
            plan_info["shuffle_nodes"].append(shuffle_info)
        
        # é›†ç´„ãƒãƒ¼ãƒ‰ã®æ¤œå‡º
        elif any(keyword in node_name for keyword in ['AGGREGATE', 'GROUP']):
            agg_info = {
                "node_name": node_name,
                "node_tag": node_tag,
                "node_id": node.get('id', ''),
                "group_keys": [],
                "aggregate_expressions": []
            }
            
            # é›†ç´„æƒ…å ±ã®æŠ½å‡º
            for meta in node_metadata:
                key = meta.get('key', '')
                values = meta.get('values', [])
                
                if key == 'GROUPING_EXPRESSIONS':
                    agg_info["group_keys"] = values
                elif key == 'AGGREGATE_EXPRESSIONS':
                    agg_info["aggregate_expressions"] = values
            
            plan_info["aggregate_nodes"].append(agg_info)
    
    # ãƒ—ãƒ©ãƒ³ã‚µãƒãƒªãƒ¼ã®ç”Ÿæˆ
    plan_info["plan_summary"] = {
        "total_nodes": len(all_nodes),
        "broadcast_nodes_count": len(plan_info["broadcast_nodes"]),
        "join_nodes_count": len(plan_info["join_nodes"]),
        "scan_nodes_count": len(plan_info["scan_nodes"]),
        "shuffle_nodes_count": len(plan_info["shuffle_nodes"]),
        "aggregate_nodes_count": len(plan_info["aggregate_nodes"]),
        "unique_join_strategies": list(set(plan_info["join_strategies"])),
        "has_broadcast_joins": plan_info["broadcast_already_applied"],
        "tables_scanned": len(plan_info["table_scan_details"])
    }
    
    # BROADCASTãƒ†ãƒ¼ãƒ–ãƒ«æƒ…å ±ã‚’æŠ½å‡º
    if plan_info["broadcast_nodes"]:
        broadcast_table_info = extract_broadcast_table_names(profiler_data, plan_info["broadcast_nodes"])
        plan_info["broadcast_table_info"] = broadcast_table_info
        
        # ãƒ—ãƒ©ãƒ³ã‚µãƒãƒªãƒ¼ã«BROADCASTãƒ†ãƒ¼ãƒ–ãƒ«æƒ…å ±ã‚’è¿½åŠ 
        plan_info["plan_summary"]["broadcast_tables"] = broadcast_table_info["broadcast_tables"]
        plan_info["plan_summary"]["broadcast_table_count"] = len(broadcast_table_info["broadcast_tables"])
    
    # å®Ÿè¡Œãƒ—ãƒ©ãƒ³ã‹ã‚‰ã®ãƒ†ãƒ¼ãƒ–ãƒ«ã‚µã‚¤ã‚ºæ¨å®šæƒ…å ±ã‚’è¿½åŠ ï¼ˆestimatedSizeInBytesåˆ©ç”¨ä¸å¯ã®ãŸã‚ç„¡åŠ¹åŒ–ï¼‰
    plan_info["table_size_estimates"] = {}  # extract_table_size_estimates_from_plan(profiler_data)
    
    return plan_info

def get_spark_broadcast_threshold() -> float:
    """
    Sparkã®å®Ÿéš›ã®broadcasté–¾å€¤è¨­å®šã‚’å–å¾—
    """
    try:
        # Sparkã®è¨­å®šå€¤ã‚’å–å¾—
        threshold_bytes = spark.conf.get("spark.databricks.optimizer.autoBroadcastJoinThreshold", "31457280")  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ30MB
        threshold_mb = float(threshold_bytes) / 1024 / 1024
        return threshold_mb
    except:
        # å–å¾—ã§ããªã„å ´åˆã¯æ¨™æº–çš„ãª30MBã‚’è¿”ã™
        return 30.0

def estimate_uncompressed_size(compressed_size_mb: float, file_format: str = "parquet") -> float:
    """
    åœ§ç¸®ã‚µã‚¤ã‚ºã‹ã‚‰éåœ§ç¸®ã‚µã‚¤ã‚ºã‚’æ¨å®šï¼ˆ3.0å€å›ºå®šï¼‰
    
    æ³¨æ„: å®Ÿéš›ã®estimatedSizeInBytesãŒåˆ©ç”¨ã§ããªã„ãŸã‚ã€
    ä¿å®ˆçš„ãª3.0å€åœ§ç¸®ç‡ã§çµ±ä¸€ã—ã¦æ¨å®šã—ã¾ã™ã€‚
    """
    # ä¿å®ˆçš„ãª3.0å€åœ§ç¸®ç‡ã§çµ±ä¸€ï¼ˆestimatedSizeInBytesåˆ©ç”¨ä¸å¯ã®ãŸã‚ï¼‰
    compression_ratio = 3.0
    
    return compressed_size_mb * compression_ratio

def analyze_broadcast_feasibility(metrics: Dict[str, Any], original_query: str, plan_info: Dict[str, Any] = None) -> Dict[str, Any]:
    """
    BROADCASTãƒ’ãƒ³ãƒˆã®é©ç”¨å¯èƒ½æ€§ã‚’åˆ†æï¼ˆæ­£ç¢ºãª30MBé–¾å€¤é©ç”¨ï¼‰
    """
    broadcast_analysis = {
        "is_join_query": False,
        "broadcast_candidates": [],
        "recommendations": [],
        "feasibility": "not_applicable",
        "reasoning": [],
        "spark_threshold_mb": get_spark_broadcast_threshold(),
        "compression_analysis": {},
        "detailed_size_analysis": [],
        "execution_plan_analysis": {},
        "existing_broadcast_nodes": [],
        "already_optimized": False,
        "broadcast_applied_tables": []
    }
    
    # ã‚¯ã‚¨ãƒªã«JOINãŒå«ã¾ã‚Œã¦ã„ã‚‹ã‹ãƒã‚§ãƒƒã‚¯
    query_upper = original_query.upper()
    join_types = ['JOIN', 'INNER JOIN', 'LEFT JOIN', 'RIGHT JOIN', 'LEFT OUTER JOIN', 'RIGHT OUTER JOIN', 'SEMI JOIN', 'ANTI JOIN']
    has_join = any(join_type in query_upper for join_type in join_types)
    
    if not has_join:
        broadcast_analysis["reasoning"].append("JOINã‚¯ã‚¨ãƒªã§ã¯ãªã„ãŸã‚ã€BROADCASTãƒ’ãƒ³ãƒˆã¯é©ç”¨ä¸å¯")
        return broadcast_analysis
    
    broadcast_analysis["is_join_query"] = True
    broadcast_analysis["reasoning"].append(f"Spark BROADCASTé–¾å€¤: {broadcast_analysis['spark_threshold_mb']:.1f}MBï¼ˆéåœ§ç¸®ï¼‰")
    
    # å®Ÿè¡Œãƒ—ãƒ©ãƒ³æƒ…å ±ã®åˆ†æ
    if plan_info:
        plan_summary = plan_info.get("plan_summary", {})
        broadcast_nodes = plan_info.get("broadcast_nodes", [])
        join_nodes = plan_info.get("join_nodes", [])
        table_scan_details = plan_info.get("table_scan_details", {})
        table_size_estimates = plan_info.get("table_size_estimates", {})
        
        # æ—¢å­˜ã®BROADCASTé©ç”¨çŠ¶æ³ã®è¨˜éŒ²
        broadcast_analysis["existing_broadcast_nodes"] = broadcast_nodes
        broadcast_analysis["already_optimized"] = len(broadcast_nodes) > 0
        
        # ãƒ—ãƒ©ãƒ³åˆ†æçµæœã®è¨˜éŒ²
        broadcast_analysis["execution_plan_analysis"] = {
            "has_broadcast_joins": plan_summary.get("has_broadcast_joins", False),
            "unique_join_strategies": plan_summary.get("unique_join_strategies", []),
            "broadcast_nodes_count": len(broadcast_nodes),
            "join_nodes_count": len(join_nodes),
            "scan_nodes_count": plan_summary.get("scan_nodes_count", 0),
            "shuffle_nodes_count": plan_summary.get("shuffle_nodes_count", 0),
            "tables_in_plan": list(table_scan_details.keys())
        }
        
        # æ—¢ã«BROADCASTãŒé©ç”¨ã•ã‚Œã¦ã„ã‚‹å ´åˆã®è©³ç´°è¨˜éŒ²
        if broadcast_nodes:
            broadcast_analysis["reasoning"].append(f"âœ… å®Ÿè¡Œãƒ—ãƒ©ãƒ³ã§æ—¢ã«BROADCAST JOINãŒé©ç”¨æ¸ˆã¿: {len(broadcast_nodes)}å€‹ã®ãƒãƒ¼ãƒ‰")
            
            # BROADCASTãƒ†ãƒ¼ãƒ–ãƒ«æƒ…å ±ã‚’å–å¾—
            broadcast_table_info = plan_info.get("broadcast_table_info", {})
            broadcast_tables = broadcast_table_info.get("broadcast_tables", [])
            
            if broadcast_tables:
                broadcast_analysis["reasoning"].append(f"ğŸ“‹ BROADCASTã•ã‚Œã¦ã„ã‚‹ãƒ†ãƒ¼ãƒ–ãƒ«: {', '.join(broadcast_tables)}")
                broadcast_analysis["broadcast_applied_tables"] = broadcast_tables
                
                # å„BROADCASTãƒãƒ¼ãƒ‰ã®è©³ç´°
                broadcast_nodes_with_tables = broadcast_table_info.get("broadcast_nodes_with_tables", [])
                for i, node in enumerate(broadcast_nodes_with_tables[:3]):  # æœ€å¤§3å€‹ã¾ã§è¡¨ç¤º
                    node_name_short = node['node_name'][:50] + "..." if len(node['node_name']) > 50 else node['node_name']
                    associated_tables = node.get('associated_tables', [])
                    if associated_tables:
                        broadcast_analysis["reasoning"].append(f"  â€¢ BROADCAST Node {i+1}: {node_name_short}")
                        broadcast_analysis["reasoning"].append(f"    â””â”€ ãƒ†ãƒ¼ãƒ–ãƒ«: {', '.join(associated_tables)}")
                    else:
                        broadcast_analysis["reasoning"].append(f"  â€¢ BROADCAST Node {i+1}: {node_name_short} (ãƒ†ãƒ¼ãƒ–ãƒ«åæœªç‰¹å®š)")
            else:
                # BROADCASTãƒãƒ¼ãƒ‰ã¯å­˜åœ¨ã™ã‚‹ãŒãƒ†ãƒ¼ãƒ–ãƒ«åãŒç‰¹å®šã§ããªã„å ´åˆ
                for i, node in enumerate(broadcast_nodes[:3]):  # æœ€å¤§3å€‹ã¾ã§è¡¨ç¤º
                    broadcast_analysis["reasoning"].append(f"  â€¢ BROADCAST Node {i+1}: {node['node_name'][:50]}... (ãƒ†ãƒ¼ãƒ–ãƒ«åè§£æä¸­)")
        else:
            # BROADCASTæœªé©ç”¨ã ãŒã€JOINãŒå­˜åœ¨ã™ã‚‹å ´åˆ
            if join_nodes:
                join_strategies = set(node["join_strategy"] for node in join_nodes)
                broadcast_analysis["reasoning"].append(f"ğŸ” ç¾åœ¨ã®JOINæˆ¦ç•¥: {', '.join(join_strategies)}")
                broadcast_analysis["reasoning"].append("ğŸ’¡ BROADCASTæœ€é©åŒ–ã®æ©Ÿä¼šã‚’æ¤œè¨ä¸­...")
    else:
        broadcast_analysis["reasoning"].append("âš ï¸ å®Ÿè¡Œãƒ—ãƒ©ãƒ³æƒ…å ±ãŒåˆ©ç”¨ã§ãã¾ã›ã‚“ - ãƒ¡ãƒˆãƒªã‚¯ã‚¹æ¨å®šã«åŸºã¥ãåˆ†æã‚’å®Ÿè¡Œ")
    
    # ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‹ã‚‰ãƒ†ãƒ¼ãƒ–ãƒ«ã‚µã‚¤ã‚ºæƒ…å ±ã‚’å–å¾—
    overall_metrics = metrics.get('overall_metrics', {})
    node_metrics = metrics.get('node_metrics', [])
    
    # ã‚¹ã‚­ãƒ£ãƒ³ãƒãƒ¼ãƒ‰ã‹ã‚‰ãƒ†ãƒ¼ãƒ–ãƒ«æƒ…å ±ã‚’æŠ½å‡º
    scan_nodes = []
    total_compressed_bytes = 0
    total_rows_all_tables = 0
    
    for node in node_metrics:
        node_name = node.get('name', '').upper()
        if any(keyword in node_name for keyword in ['SCAN', 'FILESCAN', 'PARQUET', 'DELTA']):
            key_metrics = node.get('key_metrics', {})
            rows_num = key_metrics.get('rowsNum', 0)
            duration_ms = key_metrics.get('durationMs', 0)
            
            # ãƒ•ã‚¡ã‚¤ãƒ«å½¢å¼ã®æ¨å®šï¼ˆãƒ—ãƒ©ãƒ³æƒ…å ±ã‚’å„ªå…ˆï¼‰
            file_format = "parquet"  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ
            table_name_from_plan = "unknown"
            
            # ãƒ—ãƒ©ãƒ³æƒ…å ±ã‹ã‚‰ãƒ†ãƒ¼ãƒ–ãƒ«åã¨ãƒ•ã‚¡ã‚¤ãƒ«å½¢å¼ã‚’å–å¾—
            if plan_info and plan_info.get("table_scan_details"):
                # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰è©³ç´°ãªãƒ†ãƒ¼ãƒ–ãƒ«åã‚’æŠ½å‡º
                node_metadata = node.get('metadata', [])
                for meta in node_metadata:
                    meta_key = meta.get('key', '')
                    meta_value = meta.get('value', '')
                    if meta_key in ['SCAN_IDENTIFIER', 'SCAN_TABLE', 'TABLE_NAME'] and meta_value:
                        # ãƒ—ãƒ©ãƒ³ã®è©³ç´°ã¨ç…§åˆ
                        for plan_table, scan_detail in plan_info["table_scan_details"].items():
                            if meta_value in plan_table or plan_table in meta_value:
                                table_name_from_plan = plan_table
                                if scan_detail["file_format"] != "unknown":
                                    file_format = scan_detail["file_format"]
                                break
                        break
            
            # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: ãƒãƒ¼ãƒ‰åã‹ã‚‰ãƒ•ã‚¡ã‚¤ãƒ«å½¢å¼ã‚’æ¨å®š
            if file_format == "parquet":  # ã¾ã ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã®å ´åˆ
                if "DELTA" in node_name:
                    file_format = "delta"
                elif "PARQUET" in node_name:
                    file_format = "parquet"
                elif "JSON" in node_name:
                    file_format = "json"
                elif "CSV" in node_name:
                    file_format = "csv"
            
            # ãƒ¡ãƒˆãƒªã‚¯ã‚¹ãƒ™ãƒ¼ã‚¹æ¨å®šã®ã¿ä½¿ç”¨ï¼ˆestimatedSizeInBytesåˆ©ç”¨ä¸å¯ã®ãŸã‚ï¼‰
            estimated_compressed_mb = 0
            estimated_uncompressed_mb = 0
            size_source = "metrics_estimation"
            
            # ãƒ¡ãƒˆãƒªã‚¯ã‚¹ãƒ™ãƒ¼ã‚¹æ¨å®š
            total_read_bytes = overall_metrics.get('read_bytes', 0)
            total_rows = overall_metrics.get('rows_read_count', 0)
            
            if total_rows > 0 and total_read_bytes > 0 and rows_num > 0:
                # å…¨ä½“ã®èª­ã¿è¾¼ã¿é‡ã‹ã‚‰ã“ã®ãƒ†ãƒ¼ãƒ–ãƒ«ã®å‰²åˆã‚’è¨ˆç®—
                table_ratio = rows_num / total_rows
                estimated_compressed_bytes = total_read_bytes * table_ratio
                estimated_compressed_mb = estimated_compressed_bytes / 1024 / 1024
                 
                # éåœ§ç¸®ã‚µã‚¤ã‚ºã‚’æ¨å®š
                estimated_uncompressed_mb = estimate_uncompressed_size(estimated_compressed_mb, file_format)
            else:
                # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: è¡Œæ•°ãƒ™ãƒ¼ã‚¹ã®æ¨å®šï¼ˆä¿å®ˆçš„ï¼‰
                # å¹³å‡è¡Œã‚µã‚¤ã‚ºã‚’æ¨å®šï¼ˆéåœ§ç¸®ï¼‰
                if total_rows > 0 and total_read_bytes > 0:
                    # å…¨ä½“ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰åœ§ç¸®å¾Œã®å¹³å‡è¡Œã‚µã‚¤ã‚ºã‚’è¨ˆç®—
                    compressed_avg_row_size = total_read_bytes / total_rows
                    # åœ§ç¸®ç‡ã‚’è€ƒæ…®ã—ã¦éåœ§ç¸®ã‚µã‚¤ã‚ºã‚’æ¨å®š
                    uncompressed_avg_row_size = compressed_avg_row_size * estimate_uncompressed_size(1.0, file_format)
                else:
                    # å®Œå…¨ãªãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: ä¸€èˆ¬çš„ãªéåœ§ç¸®è¡Œã‚µã‚¤ã‚ºï¼ˆ1KBï¼‰
                    uncompressed_avg_row_size = 1024
                
                estimated_compressed_mb = (rows_num * compressed_avg_row_size) / 1024 / 1024 if 'compressed_avg_row_size' in locals() else 0
                estimated_uncompressed_mb = (rows_num * uncompressed_avg_row_size) / 1024 / 1024
            
            # æ—¢å­˜ã®BROADCASTé©ç”¨çŠ¶æ³ã‚’ãƒã‚§ãƒƒã‚¯
            is_already_broadcasted = False
            if plan_info and plan_info.get("broadcast_nodes"):
                for broadcast_node in plan_info["broadcast_nodes"]:
                    # ãƒ†ãƒ¼ãƒ–ãƒ«åã®éƒ¨åˆ†ä¸€è‡´ã‚’ãƒã‚§ãƒƒã‚¯
                    broadcast_node_name = broadcast_node["node_name"]
                    if (table_name_from_plan != "unknown" and 
                        any(part in broadcast_node_name for part in table_name_from_plan.split('.') if len(part) > 3)):
                        is_already_broadcasted = True
                        break
                    # ãƒãƒ¼ãƒ‰åã§ã®ç…§åˆ
                    elif any(part in broadcast_node_name for part in node_name.split() if len(part) > 3):
                        is_already_broadcasted = True
                        break

            scan_info = {
                "node_name": node_name,
                "table_name_from_plan": table_name_from_plan,
                "rows": rows_num,
                "duration_ms": duration_ms,
                "estimated_compressed_mb": estimated_compressed_mb,
                "estimated_uncompressed_mb": estimated_uncompressed_mb,
                "file_format": file_format,
                "compression_ratio": 3.0,  # å›ºå®š3.0å€åœ§ç¸®ç‡
                "node_id": node.get('node_id', ''),
                "is_already_broadcasted": is_already_broadcasted,
                "size_estimation_source": size_source,
                "size_confidence": "medium"  # ãƒ¡ãƒˆãƒªã‚¯ã‚¹ãƒ™ãƒ¼ã‚¹æ¨å®šã®ãŸã‚ä¸­ç¨‹åº¦ä¿¡é ¼åº¦
            }
            scan_nodes.append(scan_info)
            
            total_compressed_bytes += estimated_compressed_bytes if 'estimated_compressed_bytes' in locals() else 0
            total_rows_all_tables += rows_num
    
    # BROADCASTå€™è£œã®åˆ¤å®šï¼ˆ30MBé–¾å€¤ä½¿ç”¨ï¼‰
    broadcast_threshold_mb = broadcast_analysis["spark_threshold_mb"]  # å®Ÿéš›ã®Sparkè¨­å®šå€¤
    broadcast_safe_mb = broadcast_threshold_mb * 0.8  # å®‰å…¨ãƒãƒ¼ã‚¸ãƒ³ï¼ˆ80%ï¼‰
    broadcast_max_mb = broadcast_threshold_mb * 10    # æ˜ã‚‰ã‹ã«å¤§ãã™ãã‚‹é–¾å€¤
    
    small_tables = []
    large_tables = []
    marginal_tables = []
    
    # åœ§ç¸®åˆ†æã®è¨˜éŒ²
    broadcast_analysis["compression_analysis"] = {
        "total_compressed_gb": total_compressed_bytes / 1024 / 1024 / 1024 if total_compressed_bytes > 0 else 0,
        "total_rows": total_rows_all_tables,
        "avg_compression_ratio": 0
    }
    
    for scan in scan_nodes:
        uncompressed_size_mb = scan["estimated_uncompressed_mb"]
        compressed_size_mb = scan["estimated_compressed_mb"]
        
        # è©³ç´°ã‚µã‚¤ã‚ºåˆ†æã®è¨˜éŒ²
        table_display_name = scan.get("table_name_from_plan", scan["node_name"])
        is_already_broadcasted = scan.get("is_already_broadcasted", False)
        
        size_analysis = {
            "table": table_display_name,
            "node_name": scan["node_name"],
            "rows": scan["rows"],
            "compressed_mb": compressed_size_mb,
            "uncompressed_mb": uncompressed_size_mb,
            "file_format": scan["file_format"],
            "compression_ratio": scan["compression_ratio"],
            "broadcast_decision": "",
            "decision_reasoning": "",
            "is_already_broadcasted": is_already_broadcasted
        }
        
        # 30MBé–¾å€¤ã§ã®åˆ¤å®šï¼ˆéåœ§ç¸®ã‚µã‚¤ã‚ºï¼‰- æ—¢å­˜é©ç”¨çŠ¶æ³ã‚’è€ƒæ…®
        if is_already_broadcasted:
            # æ—¢ã«BROADCASTãŒé©ç”¨æ¸ˆã¿
            small_tables.append(scan)  # çµ±è¨ˆç›®çš„ã§è¨˜éŒ²
            size_analysis["broadcast_decision"] = "already_applied"
            size_analysis["decision_reasoning"] = f"æ—¢ã«BROADCASTé©ç”¨æ¸ˆã¿ï¼ˆæ¨å®šã‚µã‚¤ã‚º: éåœ§ç¸®{uncompressed_size_mb:.1f}MBï¼‰"
            broadcast_analysis["broadcast_candidates"].append({
                "table": table_display_name,
                "estimated_uncompressed_mb": uncompressed_size_mb,
                "estimated_compressed_mb": compressed_size_mb,
                "rows": scan["rows"],
                "file_format": scan["file_format"],
                "compression_ratio": scan["compression_ratio"],
                "broadcast_feasible": True,
                "confidence": "confirmed",
                "status": "already_applied",
                "reasoning": f"å®Ÿè¡Œãƒ—ãƒ©ãƒ³ã§æ—¢ã«BROADCASTé©ç”¨ç¢ºèªæ¸ˆã¿ï¼ˆæ¨å®šã‚µã‚¤ã‚º: éåœ§ç¸®{uncompressed_size_mb:.1f}MBã€ãƒ¡ãƒˆãƒªã‚¯ã‚¹ãƒ™ãƒ¼ã‚¹æ¨å®šï¼‰"
            })
        elif uncompressed_size_mb <= broadcast_safe_mb and scan["rows"] > 0:
            # å®‰å…¨ãƒãƒ¼ã‚¸ãƒ³å†…ï¼ˆ24MBä»¥ä¸‹ï¼‰- å¼·ãæ¨å¥¨
            small_tables.append(scan)
            size_analysis["broadcast_decision"] = "strongly_recommended"
            size_analysis["decision_reasoning"] = f"éåœ§ç¸®{uncompressed_size_mb:.1f}MB â‰¤ å®‰å…¨é–¾å€¤{broadcast_safe_mb:.1f}MB"
            broadcast_analysis["broadcast_candidates"].append({
                "table": table_display_name,
                "estimated_uncompressed_mb": uncompressed_size_mb,
                "estimated_compressed_mb": compressed_size_mb,
                "rows": scan["rows"],
                "file_format": scan["file_format"],
                "compression_ratio": scan["compression_ratio"],
                "broadcast_feasible": True,
                "confidence": "high",
                "status": "new_recommendation",
                "reasoning": f"éåœ§ç¸®æ¨å®šã‚µã‚¤ã‚º {uncompressed_size_mb:.1f}MBï¼ˆå®‰å…¨é–¾å€¤ {broadcast_safe_mb:.1f}MB ä»¥ä¸‹ï¼‰ã§BROADCASTå¼·ãæ¨å¥¨ï¼ˆãƒ¡ãƒˆãƒªã‚¯ã‚¹ãƒ™ãƒ¼ã‚¹æ¨å®šã€3.0å€åœ§ç¸®ç‡ï¼‰"
            })
        elif uncompressed_size_mb <= broadcast_threshold_mb and scan["rows"] > 0:
            # é–¾å€¤å†…ã ãŒå®‰å…¨ãƒãƒ¼ã‚¸ãƒ³ã¯è¶…éï¼ˆ24-30MBï¼‰- æ¡ä»¶ä»˜ãæ¨å¥¨
            marginal_tables.append(scan)
            size_analysis["broadcast_decision"] = "conditionally_recommended"
            size_analysis["decision_reasoning"] = f"éåœ§ç¸®{uncompressed_size_mb:.1f}MB â‰¤ é–¾å€¤{broadcast_threshold_mb:.1f}MBï¼ˆå®‰å…¨ãƒãƒ¼ã‚¸ãƒ³è¶…éï¼‰"
            broadcast_analysis["broadcast_candidates"].append({
                "table": table_display_name,
                "estimated_uncompressed_mb": uncompressed_size_mb,
                "estimated_compressed_mb": compressed_size_mb,
                "rows": scan["rows"],
                "file_format": scan["file_format"],
                "compression_ratio": scan["compression_ratio"],
                "broadcast_feasible": True,
                "confidence": "medium",
                "status": "new_recommendation",
                "reasoning": f"éåœ§ç¸®æ¨å®šã‚µã‚¤ã‚º {uncompressed_size_mb:.1f}MBï¼ˆé–¾å€¤ {broadcast_threshold_mb:.1f}MB ä»¥ä¸‹ã ãŒå®‰å…¨ãƒãƒ¼ã‚¸ãƒ³ {broadcast_safe_mb:.1f}MB è¶…éï¼‰ã§æ¡ä»¶ä»˜ãBROADCASTæ¨å¥¨ï¼ˆãƒ¡ãƒˆãƒªã‚¯ã‚¹ãƒ™ãƒ¼ã‚¹æ¨å®šã€3.0å€åœ§ç¸®ç‡ï¼‰"
            })
        elif uncompressed_size_mb > broadcast_max_mb:
            # æ˜ã‚‰ã‹ã«å¤§ãã™ãã‚‹ï¼ˆ300MBè¶…ï¼‰
            large_tables.append(scan)
            size_analysis["broadcast_decision"] = "not_recommended"
            size_analysis["decision_reasoning"] = f"éåœ§ç¸®{uncompressed_size_mb:.1f}MB > æœ€å¤§é–¾å€¤{broadcast_max_mb:.1f}MB"
            broadcast_analysis["reasoning"].append(f"ãƒ†ãƒ¼ãƒ–ãƒ« {table_display_name}: éåœ§ç¸®{uncompressed_size_mb:.1f}MB - BROADCASTä¸å¯ï¼ˆ>{broadcast_max_mb:.1f}MBï¼‰")
        else:
            # ä¸­é–“ã‚µã‚¤ã‚ºã®ãƒ†ãƒ¼ãƒ–ãƒ«ï¼ˆ30-300MBï¼‰
            large_tables.append(scan)
            size_analysis["broadcast_decision"] = "not_recommended"
            size_analysis["decision_reasoning"] = f"éåœ§ç¸®{uncompressed_size_mb:.1f}MB > é–¾å€¤{broadcast_threshold_mb:.1f}MB"
            broadcast_analysis["reasoning"].append(f"ãƒ†ãƒ¼ãƒ–ãƒ« {table_display_name}: éåœ§ç¸®{uncompressed_size_mb:.1f}MB - BROADCASTéæ¨å¥¨ï¼ˆ>{broadcast_threshold_mb:.1f}MBé–¾å€¤ï¼‰")
        
        broadcast_analysis["detailed_size_analysis"].append(size_analysis)
    
    # åœ§ç¸®åˆ†æã‚µãƒãƒªãƒ¼ã®æ›´æ–°
    if scan_nodes:
        total_uncompressed_mb = sum(scan["estimated_uncompressed_mb"] for scan in scan_nodes)
        total_compressed_mb = sum(scan["estimated_compressed_mb"] for scan in scan_nodes)
        if total_compressed_mb > 0:
            broadcast_analysis["compression_analysis"]["avg_compression_ratio"] = total_uncompressed_mb / total_compressed_mb
        broadcast_analysis["compression_analysis"]["total_uncompressed_mb"] = total_uncompressed_mb
        broadcast_analysis["compression_analysis"]["total_compressed_mb"] = total_compressed_mb
    
    # ç·ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿é‡ã¨ã®æ•´åˆæ€§ãƒã‚§ãƒƒã‚¯ï¼ˆåœ§ç¸®ãƒ™ãƒ¼ã‚¹ï¼‰
    total_read_gb = overall_metrics.get('read_bytes', 0) / 1024 / 1024 / 1024
    estimated_total_compressed_mb = sum(scan["estimated_compressed_mb"] for scan in scan_nodes)
    
    if estimated_total_compressed_mb > 0:
        size_ratio = (total_read_gb * 1024) / estimated_total_compressed_mb
        if size_ratio > 3 or size_ratio < 0.3:
            broadcast_analysis["reasoning"].append(f"æ¨å®šåœ§ç¸®ã‚µã‚¤ã‚º({estimated_total_compressed_mb:.1f}MB)ã¨å®Ÿèª­ã¿è¾¼ã¿é‡({total_read_gb:.1f}GB)ã«ä¹–é›¢ã‚ã‚Š - ã‚µã‚¤ã‚ºæ¨å®šã«æ³¨æ„")
        else:
            broadcast_analysis["reasoning"].append(f"ã‚µã‚¤ã‚ºæ¨å®šæ•´åˆæ€§: æ¨å®šåœ§ç¸®{estimated_total_compressed_mb:.1f}MB vs å®Ÿéš›{total_read_gb:.1f}GBï¼ˆæ¯”ç‡:{size_ratio:.2f}ï¼‰")
    
    # BROADCASTæ¨å¥¨äº‹é …ã®ç”Ÿæˆï¼ˆ30MBé–¾å€¤å¯¾å¿œã€æ—¢å­˜ã®BROADCASTé©ç”¨çŠ¶æ³ã‚’è€ƒæ…®ï¼‰
    total_broadcast_candidates = len(small_tables) + len(marginal_tables)
    total_tables = len(scan_nodes)
    
    if small_tables or marginal_tables:
        if large_tables:
            # æ—¢å­˜ã®BROADCASTé©ç”¨çŠ¶æ³ã‚’è€ƒæ…®ã—ãŸåˆ¤å®š
            if broadcast_analysis["already_optimized"]:
                broadcast_analysis["feasibility"] = "already_optimized_with_improvements"
                broadcast_analysis["recommendations"] = [
                    f"âœ… æ—¢ã«BROADCAST JOINé©ç”¨æ¸ˆã¿ - è¿½åŠ æ”¹å–„ã®æ¤œè¨",
                    f"ğŸ¯ è¿½åŠ æœ€é©åŒ–ãƒ†ãƒ¼ãƒ–ãƒ«: {total_broadcast_candidates}å€‹ï¼ˆå…¨{total_tables}å€‹ä¸­ï¼‰",
                    f"  âœ… å¼·ãæ¨å¥¨: {len(small_tables)}å€‹ï¼ˆå®‰å…¨é–¾å€¤{broadcast_safe_mb:.1f}MBä»¥ä¸‹ï¼‰",
                    f"  âš ï¸ æ¡ä»¶ä»˜ãæ¨å¥¨: {len(marginal_tables)}å€‹ï¼ˆé–¾å€¤{broadcast_threshold_mb:.1f}MBä»¥ä¸‹ã€è¦æ³¨æ„ï¼‰",
                    f"  âŒ éæ¨å¥¨: {len(large_tables)}å€‹ï¼ˆé–¾å€¤è¶…éï¼‰"
                ]
            else:
                broadcast_analysis["feasibility"] = "recommended"
                broadcast_analysis["recommendations"] = [
                    f"ğŸ¯ BROADCASTæ¨å¥¨ãƒ†ãƒ¼ãƒ–ãƒ«: {total_broadcast_candidates}å€‹ï¼ˆå…¨{total_tables}å€‹ä¸­ï¼‰",
                    f"  âœ… å¼·ãæ¨å¥¨: {len(small_tables)}å€‹ï¼ˆå®‰å…¨é–¾å€¤{broadcast_safe_mb:.1f}MBä»¥ä¸‹ï¼‰",
                    f"  âš ï¸ æ¡ä»¶ä»˜ãæ¨å¥¨: {len(marginal_tables)}å€‹ï¼ˆé–¾å€¤{broadcast_threshold_mb:.1f}MBä»¥ä¸‹ã€è¦æ³¨æ„ï¼‰",
                    f"  âŒ éæ¨å¥¨: {len(large_tables)}å€‹ï¼ˆé–¾å€¤è¶…éï¼‰"
                ]
        else:
            # å…¨ãƒ†ãƒ¼ãƒ–ãƒ«ãŒå°ã•ã„å ´åˆ
            if broadcast_analysis["already_optimized"]:
                broadcast_analysis["feasibility"] = "already_optimized_complete"
                broadcast_analysis["recommendations"] = [
                    f"âœ… æ—¢ã«BROADCAST JOINé©ç”¨æ¸ˆã¿ - æœ€é©åŒ–å®Œäº†",
                    f"ğŸ¯ å…¨ãƒ†ãƒ¼ãƒ–ãƒ«ï¼ˆ{total_tables}å€‹ï¼‰ãŒBROADCASTé–¾å€¤ä»¥ä¸‹ã§é©åˆ‡ã«å‡¦ç†æ¸ˆã¿",
                    f"  âœ… å¼·ãæ¨å¥¨: {len(small_tables)}å€‹",
                    f"  âš ï¸ æ¡ä»¶ä»˜ãæ¨å¥¨: {len(marginal_tables)}å€‹",
                    "ğŸ“‹ ç¾åœ¨ã®è¨­å®šãŒæœ€é©ã§ã™"
                ]
            else:
                broadcast_analysis["feasibility"] = "all_small"
                broadcast_analysis["recommendations"] = [
                    f"ğŸ¯ å…¨ãƒ†ãƒ¼ãƒ–ãƒ«ï¼ˆ{total_tables}å€‹ï¼‰ãŒBROADCASTé–¾å€¤ä»¥ä¸‹",
                    f"  âœ… å¼·ãæ¨å¥¨: {len(small_tables)}å€‹",
                    f"  âš ï¸ æ¡ä»¶ä»˜ãæ¨å¥¨: {len(marginal_tables)}å€‹",
                    "ğŸ“‹ æœ€å°ãƒ†ãƒ¼ãƒ–ãƒ«ã‚’å„ªå…ˆçš„ã«BROADCASTã™ã‚‹ã“ã¨ã‚’æ¨å¥¨"
                ]
        
        # å…·ä½“çš„ãªBROADCASTå€™è£œã®è©³ç´°
        for small_table in small_tables:
            broadcast_analysis["recommendations"].append(
                f"ğŸ”¹ BROADCAST({small_table['node_name']}) - éåœ§ç¸®{small_table['estimated_uncompressed_mb']:.1f}MBï¼ˆåœ§ç¸®{small_table['estimated_compressed_mb']:.1f}MBã€{small_table['file_format']}ã€åœ§ç¸®ç‡{small_table['compression_ratio']:.1f}xï¼‰"
            )
        
        for marginal_table in marginal_tables:
            broadcast_analysis["recommendations"].append(
                f"ğŸ”¸ BROADCAST({marginal_table['node_name']}) - éåœ§ç¸®{marginal_table['estimated_uncompressed_mb']:.1f}MBï¼ˆæ¡ä»¶ä»˜ãã€ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡è¦æ³¨æ„ï¼‰"
            )
            
    elif large_tables:
        broadcast_analysis["feasibility"] = "not_recommended"
        broadcast_analysis["recommendations"] = [
            f"âŒ å…¨ãƒ†ãƒ¼ãƒ–ãƒ«ï¼ˆ{len(large_tables)}å€‹ï¼‰ãŒ30MBé–¾å€¤è¶…éã®ãŸã‚BROADCASTéæ¨å¥¨",
            f"ğŸ“Š æœ€å°ãƒ†ãƒ¼ãƒ–ãƒ«ã§ã‚‚éåœ§ç¸®{min(scan['estimated_uncompressed_mb'] for scan in large_tables):.1f}MB",
            "ğŸ”§ ä»£æ›¿æœ€é©åŒ–æ‰‹æ³•ã‚’æ¨å¥¨:",
            "  â€¢ Liquid Clusteringå®Ÿè£…",
            "  â€¢ ãƒ‡ãƒ¼ã‚¿ãƒ‘ãƒ¼ãƒ†ã‚£ã‚·ãƒ§ãƒ‹ãƒ³ã‚°",
            "  â€¢ ã‚¯ã‚¨ãƒªæœ€é©åŒ–ï¼ˆãƒ•ã‚£ãƒ«ã‚¿ãƒ¼ãƒ—ãƒƒã‚·ãƒ¥ãƒ€ã‚¦ãƒ³ç­‰ï¼‰",
            "  â€¢ spark.databricks.optimizer.autoBroadcastJoinThresholdè¨­å®šå€¤ã®èª¿æ•´æ¤œè¨"
        ]
    else:
        broadcast_analysis["feasibility"] = "insufficient_data"
        broadcast_analysis["recommendations"] = [
            "âš ï¸ ãƒ†ãƒ¼ãƒ–ãƒ«ã‚µã‚¤ã‚ºæƒ…å ±ãŒä¸è¶³ã—ã¦ã„ã‚‹ãŸã‚ã€æ‰‹å‹•ã§ã®ã‚µã‚¤ã‚ºç¢ºèªãŒå¿…è¦",
            "ğŸ“‹ ä»¥ä¸‹ã®ã‚³ãƒãƒ³ãƒ‰ã§ãƒ†ãƒ¼ãƒ–ãƒ«ã‚µã‚¤ã‚ºã‚’ç¢ºèª:",
            "  â€¢ DESCRIBE DETAIL table_name",
            "  â€¢ SELECT COUNT(*) FROM table_name",
            "  â€¢ SHOW TABLE EXTENDED LIKE 'table_name'"
        ]
    
    # 30MBé–¾å€¤ã«ãƒ’ãƒƒãƒˆã™ã‚‹ç‰¹åˆ¥ãªã‚±ãƒ¼ã‚¹åˆ†æï¼ˆsmall_tables + marginal_tables ã‚’è€ƒæ…®ï¼‰
    all_30mb_candidates = small_tables + marginal_tables  # 30MBä»¥ä¸‹ã®å…¨å€™è£œ
    
    if all_30mb_candidates:
        broadcast_analysis["30mb_hit_analysis"] = {
            "has_30mb_candidates": True,
            "candidate_count": len(all_30mb_candidates),
            "small_tables_count": len(small_tables),  # 24MBä»¥ä¸‹ï¼ˆå¼·ãæ¨å¥¨ï¼‰
            "marginal_tables_count": len(marginal_tables),  # 24-30MBï¼ˆæ¡ä»¶ä»˜ãæ¨å¥¨ï¼‰
            "smallest_table_mb": min(scan["estimated_uncompressed_mb"] for scan in all_30mb_candidates),
            "largest_candidate_mb": max(scan["estimated_uncompressed_mb"] for scan in all_30mb_candidates),
            "total_candidate_size_mb": sum(scan["estimated_uncompressed_mb"] for scan in all_30mb_candidates),
            "recommended_broadcast_table": all_30mb_candidates[0]["node_name"] if all_30mb_candidates else None,
            "memory_impact_estimation": f"{sum(scan['estimated_uncompressed_mb'] for scan in all_30mb_candidates):.1f}MB ãŒãƒ¯ãƒ¼ã‚«ãƒ¼ãƒãƒ¼ãƒ‰ã«ãƒ–ãƒ­ãƒ¼ãƒ‰ã‚­ãƒ£ã‚¹ãƒˆ"
        }
        
        # æœ€é©ãªBROADCASTå€™è£œã®ç‰¹å®šï¼ˆå…¨30MBå€™è£œã‹ã‚‰é¸æŠï¼‰
        if len(all_30mb_candidates) > 1:
            optimal_candidate = min(all_30mb_candidates, key=lambda x: x["estimated_uncompressed_mb"])
            broadcast_analysis["30mb_hit_analysis"]["optimal_candidate"] = {
                "table": optimal_candidate["node_name"],
                "size_mb": optimal_candidate["estimated_uncompressed_mb"],
                "rows": optimal_candidate["rows"],
                "reasoning": f"æœ€å°ã‚µã‚¤ã‚º{optimal_candidate['estimated_uncompressed_mb']:.1f}MBã§æœ€ã‚‚åŠ¹ç‡çš„"
            }
        
        # 30MBé–¾å€¤å†…ã®è©³ç´°åˆ†é¡æƒ…å ±ã‚’è¿½åŠ 
        broadcast_analysis["30mb_hit_analysis"]["size_classification"] = {
            "safe_zone_tables": len(small_tables),  # 0-24MBï¼ˆå®‰å…¨ãƒãƒ¼ã‚¸ãƒ³å†…ï¼‰
            "caution_zone_tables": len(marginal_tables),  # 24-30MBï¼ˆè¦æ³¨æ„ï¼‰
            "safe_zone_description": "24MBä»¥ä¸‹ï¼ˆå¼·ãæ¨å¥¨ã€å®‰å…¨ãƒãƒ¼ã‚¸ãƒ³å†…ï¼‰",
            "caution_zone_description": "24-30MBï¼ˆæ¡ä»¶ä»˜ãæ¨å¥¨ã€ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡è¦æ³¨æ„ï¼‰"
        }
    else:
        broadcast_analysis["30mb_hit_analysis"] = {
            "has_30mb_candidates": False,
            "reason": f"å…¨ãƒ†ãƒ¼ãƒ–ãƒ«ãŒ30MBé–¾å€¤ã‚’è¶…éï¼ˆæœ€å°: {min(scan['estimated_uncompressed_mb'] for scan in scan_nodes):.1f}MBï¼‰" if scan_nodes else "ãƒ†ãƒ¼ãƒ–ãƒ«æƒ…å ±ãªã—"
        }
    
    return broadcast_analysis

def generate_optimized_query_with_llm(original_query: str, analysis_result: str, metrics: Dict[str, Any]) -> str:
    """
    ã‚»ãƒ«33ã®è©³ç´°ãƒœãƒˆãƒ«ãƒãƒƒã‚¯åˆ†æçµæœã«åŸºã¥ã„ã¦SQLã‚¯ã‚¨ãƒªã‚’æœ€é©åŒ–ï¼ˆå‡¦ç†é€Ÿåº¦é‡è¦–ï¼‰
    EXPLAINå®Ÿè¡Œãƒ•ãƒ©ã‚°ãŒYã®å ´åˆã¯ã€EXPLAINçµæœãƒ•ã‚¡ã‚¤ãƒ«ã‚‚æ´»ç”¨
    """
    
    # EXPLAINçµæœãƒ•ã‚¡ã‚¤ãƒ«ã®èª­ã¿è¾¼ã¿ï¼ˆEXPLAIN_ENABLEDãŒYã®å ´åˆï¼‰
    explain_content = ""
    physical_plan = ""
    photon_explanation = ""
    
    explain_enabled = globals().get('EXPLAIN_ENABLED', 'N')
    if explain_enabled.upper() == 'Y':
        # æœ€æ–°ã®EXPLAINçµæœãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ¤œç´¢
        import glob
        import os
        
        explain_files = glob.glob("output_explain_plan_*.txt")
        if explain_files:
            # æœ€æ–°ã®ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å–å¾—
            latest_explain_file = max(explain_files, key=os.path.getctime)
            try:
                with open(latest_explain_file, 'r', encoding='utf-8') as f:
                    explain_content = f.read()
                    print(f"âœ… EXPLAINçµæœãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã¿: {latest_explain_file}")
                
                # Physical Planã®æŠ½å‡º
                if "== Physical Plan ==" in explain_content:
                    physical_plan_start = explain_content.find("== Physical Plan ==")
                    physical_plan_end = explain_content.find("== Photon", physical_plan_start)
                    if physical_plan_end == -1:
                        physical_plan_end = len(explain_content)
                    physical_plan = explain_content[physical_plan_start:physical_plan_end].strip()
                    print(f"ğŸ“Š Physical Planæƒ…å ±ã‚’æŠ½å‡º: {len(physical_plan)} æ–‡å­—")
                
                # Photon Explanationã®æŠ½å‡º
                if "== Photon Explanation ==" in explain_content:
                    photon_start = explain_content.find("== Photon Explanation ==")
                    photon_explanation = explain_content[photon_start:].strip()
                    print(f"ğŸš€ Photon Explanationæƒ…å ±ã‚’æŠ½å‡º: {len(photon_explanation)} æ–‡å­—")
                    
            except Exception as e:
                print(f"âš ï¸ EXPLAINçµæœãƒ•ã‚¡ã‚¤ãƒ«ã®èª­ã¿è¾¼ã¿ã«å¤±æ•—: {str(e)}")
                explain_content = ""
        else:
            print("âš ï¸ EXPLAINçµæœãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
    
    # å®Ÿè¡Œãƒ—ãƒ©ãƒ³æƒ…å ±ã®æŠ½å‡ºï¼ˆãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‹ã‚‰ï¼‰
    profiler_data = metrics.get('raw_profiler_data', {})
    plan_info = None
    if profiler_data:
        plan_info = extract_execution_plan_info(profiler_data)
    
    # BROADCASTé©ç”¨å¯èƒ½æ€§ã®åˆ†æï¼ˆãƒ—ãƒ©ãƒ³æƒ…å ±ã‚’å«ã‚€ï¼‰
    broadcast_analysis = analyze_broadcast_feasibility(metrics, original_query, plan_info)
    
    # ãƒ—ãƒ©ãƒ³æƒ…å ±ã‚’ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã«è¿½åŠ ï¼ˆãƒ•ã‚¡ã‚¤ãƒ«å‡ºåŠ›ã§ä½¿ç”¨ï¼‰
    if plan_info:
        metrics['execution_plan_info'] = plan_info
    
    # ğŸš€ ã‚»ãƒ«33ã‚¹ã‚¿ã‚¤ãƒ«ã®è©³ç´°ãƒœãƒˆãƒ«ãƒãƒƒã‚¯åˆ†æã‚’å®Ÿè¡Œ
    detailed_bottleneck = extract_detailed_bottleneck_analysis(metrics)
    
    # æœ€é©åŒ–ã®ãŸã‚ã®ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆæƒ…å ±ã‚’æº–å‚™ï¼ˆè©³ç´°ç‰ˆï¼‰
    optimization_context = []
    performance_critical_issues = []
    
    # åŸºæœ¬çš„ãªãƒœãƒˆãƒ«ãƒãƒƒã‚¯æƒ…å ±ã®æŠ½å‡º
    bottlenecks = metrics.get('bottleneck_indicators', {})
    
    if bottlenecks.get('has_spill', False):
        spill_gb = bottlenecks.get('spill_bytes', 0) / 1024 / 1024 / 1024
        optimization_context.append(f"ã‚¹ãƒ”ãƒ«ç™ºç”Ÿ: {spill_gb:.1f}GB - ãƒ¡ãƒ¢ãƒªåŠ¹ç‡ã®æ”¹å–„ãŒå¿…è¦")
    
    if bottlenecks.get('has_shuffle_bottleneck', False):
        optimization_context.append("ã‚·ãƒ£ãƒƒãƒ•ãƒ«ãƒœãƒˆãƒ«ãƒãƒƒã‚¯ - JOINã¨GROUP BYã®æœ€é©åŒ–ãŒå¿…è¦")
    
    if bottlenecks.get('cache_hit_ratio', 0) < 0.5:
        optimization_context.append("ã‚­ãƒ£ãƒƒã‚·ãƒ¥åŠ¹ç‡ä½ä¸‹ - ãƒ‡ãƒ¼ã‚¿ã‚¢ã‚¯ã‚»ã‚¹ãƒ‘ã‚¿ãƒ¼ãƒ³ã®æœ€é©åŒ–ãŒå¿…è¦")
    
    # ğŸ¯ è©³ç´°ãƒœãƒˆãƒ«ãƒãƒƒã‚¯åˆ†æçµæœã‹ã‚‰ã®è¿½åŠ æƒ…å ±
    if detailed_bottleneck["spill_analysis"]["total_spill_gb"] > 0:
        total_spill = detailed_bottleneck["spill_analysis"]["total_spill_gb"]
        spill_nodes_count = len(detailed_bottleneck["spill_analysis"]["spill_nodes"])
        performance_critical_issues.append(f"ğŸš¨ CRITICAL: åˆè¨ˆ{total_spill:.1f}GBã®ã‚¹ãƒ”ãƒ«ãŒ{spill_nodes_count}å€‹ã®ãƒãƒ¼ãƒ‰ã§ç™ºç”Ÿ")
        
        # æœ€ã‚‚é‡è¦ãªã‚¹ãƒ”ãƒ«ãƒãƒ¼ãƒ‰ã‚’ç‰¹å®š
        if detailed_bottleneck["spill_analysis"]["spill_nodes"]:
            top_spill_node = max(detailed_bottleneck["spill_analysis"]["spill_nodes"], key=lambda x: x["spill_gb"])
            performance_critical_issues.append(f"   æœ€å¤§ã‚¹ãƒ”ãƒ«ãƒãƒ¼ãƒ‰: {top_spill_node['node_name']} ({top_spill_node['spill_gb']:.2f}GB)")
    
    if detailed_bottleneck["skew_analysis"]["total_skewed_partitions"] > 0:
        total_skew = detailed_bottleneck["skew_analysis"]["total_skewed_partitions"]
        skewed_nodes_count = len(detailed_bottleneck["skew_analysis"]["skewed_nodes"])
        performance_critical_issues.append(f"âš–ï¸ ãƒ‡ãƒ¼ã‚¿ã‚¹ã‚­ãƒ¥ãƒ¼: {total_skew}å€‹ã®ã‚¹ã‚­ãƒ¥ãƒ¼ãƒ‘ãƒ¼ãƒ†ã‚£ã‚·ãƒ§ãƒ³ãŒ{skewed_nodes_count}å€‹ã®ãƒãƒ¼ãƒ‰ã§æ¤œå‡º")
    
    # TOP3ãƒœãƒˆãƒ«ãƒãƒƒã‚¯ãƒãƒ¼ãƒ‰ã®è©³ç´°åˆ†æ
    top3_bottlenecks = detailed_bottleneck["top_bottleneck_nodes"][:3]
    performance_critical_issues.append("ğŸ“Š TOP3å‡¦ç†æ™‚é–“ãƒœãƒˆãƒ«ãƒãƒƒã‚¯:")
    for node in top3_bottlenecks:
        severity_icon = "ğŸ”´" if node["severity"] == "CRITICAL" else "ğŸŸ " if node["severity"] == "HIGH" else "ğŸŸ¡"
        performance_critical_issues.append(f"   {severity_icon} #{node['rank']}: {node['node_name'][:60]}...")
        performance_critical_issues.append(f"      å®Ÿè¡Œæ™‚é–“: {node['duration_ms']:,}ms ({node['time_percentage']:.1f}%) | ãƒ¡ãƒ¢ãƒª: {node['memory_mb']:.1f}MB")
        if node["spill_detected"]:
            performance_critical_issues.append(f"      ğŸ’¿ ã‚¹ãƒ”ãƒ«: {node['spill_gb']:.2f}GB - ç·Šæ€¥å¯¾å¿œå¿…è¦")
        if node["skew_detected"]:
            performance_critical_issues.append(f"      âš–ï¸ ã‚¹ã‚­ãƒ¥ãƒ¼: {node['skewed_partitions']}ãƒ‘ãƒ¼ãƒ†ã‚£ã‚·ãƒ§ãƒ³ - ãƒ‡ãƒ¼ã‚¿åˆ†æ•£æ”¹å–„å¿…è¦")
    
    # ğŸ”„ REPARTITIONãƒ’ãƒ³ãƒˆã®è©³ç´°ç”Ÿæˆï¼ˆã‚¹ãƒ”ãƒ«æ¤œå‡ºæ™‚ã®ã¿ï¼‰
    repartition_hints = []
    if detailed_bottleneck["shuffle_optimization_hints"]:
        repartition_hints.append("ğŸ”„ REPARTITIONãƒ’ãƒ³ãƒˆï¼ˆã‚¹ãƒ”ãƒ«æ¤œå‡ºæ™‚ã®ã¿ï¼‰:")
        for hint in detailed_bottleneck["shuffle_optimization_hints"]:
            priority_icon = "ğŸš¨" if hint["priority"] == "HIGH" else "ğŸ“ˆ"
            repartition_hints.append(f"   {priority_icon} ãƒãƒ¼ãƒ‰ID {hint['node_id']}: {hint['suggested_sql']}")
            repartition_hints.append(f"      å±æ€§: {', '.join(hint['attributes'])}")
            repartition_hints.append(f"      ç†ç”±: {hint['reason']}")
            repartition_hints.append(f"      åŠ¹æœ: {hint['estimated_improvement']}")
            
            # ã‚¯ã‚¨ãƒªã¸ã®é©ç”¨æ–¹æ³•ã®å…·ä½“çš„ãªææ¡ˆ
            main_attr = hint['attributes'][0]
            if 'GROUP BY' in original_query.upper():
                repartition_hints.append(f"      é©ç”¨ææ¡ˆ: GROUP BYå‰ã«REPARTITION({hint['suggested_sql'].split('(')[1]}")
            elif 'JOIN' in original_query.upper():
                repartition_hints.append(f"      é©ç”¨ææ¡ˆ: JOINå‰ã®ãƒ†ãƒ¼ãƒ–ãƒ«ã‚’{hint['suggested_sql']}ã§ãƒªãƒ‘ãƒ¼ãƒ†ã‚£ã‚·ãƒ§ãƒ³")
    
    # ğŸ“Š å‡¦ç†é€Ÿåº¦é‡è¦–ã®æœ€é©åŒ–æ¨å¥¨äº‹é …
    speed_optimization_recommendations = []
    for rec in detailed_bottleneck["performance_recommendations"]:
        priority_icon = "ğŸš¨" if rec["priority"] == "CRITICAL" else "âš ï¸" if rec["priority"] == "HIGH" else "ğŸ“"
        speed_optimization_recommendations.append(f"{priority_icon} {rec['type'].upper()}: {rec['description']}")
    
    # Liquid Clusteringæ¨å¥¨æƒ…å ±ï¼ˆLLMãƒ™ãƒ¼ã‚¹å¯¾å¿œï¼‰
    liquid_analysis = metrics.get('liquid_clustering_analysis', {})
    extracted_data = liquid_analysis.get('extracted_data', {})
    table_info = extracted_data.get('table_info', {})
    
    clustering_recommendations = []
    if table_info:
        for table_name in list(table_info.keys())[:3]:  # ä¸Šä½3ãƒ†ãƒ¼ãƒ–ãƒ«
            clustering_recommendations.append(f"ãƒ†ãƒ¼ãƒ–ãƒ« {table_name}: LLMåˆ†æã«ã‚ˆã‚‹æ¨å¥¨ã‚«ãƒ©ãƒ ã§ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°æ¨å¥¨")
    
    # æœ€é©åŒ–ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã®ä½œæˆï¼ˆç°¡æ½”ç‰ˆã§ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆå›é¿ï¼‰
    
    # åˆ†æçµæœã‚’ç°¡æ½”åŒ–ï¼ˆ128Kåˆ¶é™å†…ã§æœ€å¤§åŠ¹ç‡åŒ–ï¼‰
    analysis_summary = ""
    if isinstance(analysis_result, str) and len(analysis_result) > 2000:
        # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆå®¹é‡ã®ç¢ºä¿ã®ãŸã‚ã€åˆ†æçµæœã¯è¦ç‚¹ã®ã¿ã«åœ§ç¸®
        analysis_summary = analysis_result[:2000] + "...[è¦ç´„ï¼šä¸»è¦ãƒœãƒˆãƒ«ãƒãƒƒã‚¯ã®ã¿ä¿æŒ]"
    else:
        analysis_summary = str(analysis_result)
    
    # ãƒœãƒˆãƒ«ãƒãƒƒã‚¯æƒ…å ±ã®ç°¡æ½”åŒ–
    bottleneck_summary = "ã€".join(optimization_context[:3]) if optimization_context else "ç‰¹ã«ãªã—"
    
    # Liquid Clusteringæ¨å¥¨ã®ç°¡æ½”åŒ–
    clustering_summary = "ã€".join(clustering_recommendations[:2]) if clustering_recommendations else "ç‰¹ã«ãªã—"
    
    # BROADCASTåˆ†æçµæœã®ã‚µãƒãƒªãƒ¼ä½œæˆï¼ˆ30MBé–¾å€¤å¯¾å¿œï¼‰
    broadcast_summary = []
    if broadcast_analysis["is_join_query"]:
        # æ—¢å­˜ã®BROADCASTé©ç”¨çŠ¶æ³ã‚’æœ€åˆã«è¡¨ç¤º
        if broadcast_analysis["already_optimized"]:
            existing_broadcast_count = len(broadcast_analysis["existing_broadcast_nodes"])
            broadcast_summary.append(f"âœ… æ—¢ã«BROADCAST JOINé©ç”¨æ¸ˆã¿: {existing_broadcast_count}å€‹ã®ãƒãƒ¼ãƒ‰")
            
            # BROADCASTã•ã‚Œã¦ã„ã‚‹ãƒ†ãƒ¼ãƒ–ãƒ«ä¸€è¦§ã‚’è¡¨ç¤º
            broadcast_applied_tables = broadcast_analysis.get("broadcast_applied_tables", [])
            if broadcast_applied_tables:
                broadcast_summary.append(f"ğŸ“‹ BROADCASTã•ã‚Œã¦ã„ã‚‹ãƒ†ãƒ¼ãƒ–ãƒ«: {', '.join(broadcast_applied_tables)}")
            
            # æ—¢å­˜ã®BROADCASTãƒãƒ¼ãƒ‰ã®è©³ç´°ã‚’è¡¨ç¤ºï¼ˆæœ€å¤§3å€‹ï¼‰
            for i, node in enumerate(broadcast_analysis["existing_broadcast_nodes"][:3]):
                node_name_short = node["node_name"][:50] + "..." if len(node["node_name"]) > 50 else node["node_name"]
                broadcast_summary.append(f"  ğŸ”¹ BROADCAST Node {i+1}: {node_name_short}")
            
            # å®Ÿè¡Œãƒ—ãƒ©ãƒ³åˆ†æã‹ã‚‰ã®JOINæˆ¦ç•¥æƒ…å ±
            plan_analysis = broadcast_analysis.get("execution_plan_analysis", {})
            if plan_analysis.get("unique_join_strategies"):
                broadcast_summary.append(f"ğŸ” æ¤œå‡ºã•ã‚ŒãŸJOINæˆ¦ç•¥: {', '.join(plan_analysis['unique_join_strategies'])}")
        else:
            broadcast_summary.append("ğŸ” BROADCAST JOINæœªé©ç”¨ - æœ€é©åŒ–ã®æ©Ÿä¼šã‚’æ¤œè¨ä¸­")
        
        broadcast_summary.append(f"ğŸ¯ BROADCASTé©ç”¨å¯èƒ½æ€§: {broadcast_analysis['feasibility']}")
        broadcast_summary.append(f"âš–ï¸ Sparké–¾å€¤: {broadcast_analysis['spark_threshold_mb']:.1f}MBï¼ˆéåœ§ç¸®ï¼‰")
        
        # 30MBä»¥ä¸‹ã®å€™è£œãŒã‚ã‚‹å ´åˆ
        if broadcast_analysis["30mb_hit_analysis"]["has_30mb_candidates"]:
            hit_analysis = broadcast_analysis["30mb_hit_analysis"]
            broadcast_summary.append(f"âœ… 30MBé–¾å€¤ãƒ’ãƒƒãƒˆ: {hit_analysis['candidate_count']}å€‹ã®ãƒ†ãƒ¼ãƒ–ãƒ«ãŒæ¡ä»¶é©åˆ")
            broadcast_summary.append(f"ğŸ“Š å€™è£œã‚µã‚¤ã‚ºç¯„å›²: {hit_analysis['smallest_table_mb']:.1f}MB - {hit_analysis['largest_candidate_mb']:.1f}MB")
            
            if "optimal_candidate" in hit_analysis:
                optimal = hit_analysis["optimal_candidate"]
                broadcast_summary.append(f"ğŸ† æœ€é©å€™è£œ: {optimal['table']} ({optimal['size_mb']:.1f}MB)")
        else:
            broadcast_summary.append(f"âŒ 30MBé–¾å€¤ãƒ’ãƒƒãƒˆãªã—: {broadcast_analysis['30mb_hit_analysis']['reason']}")
        
        # BROADCASTå€™è£œã®è©³ç´°ï¼ˆæœ€å¤§3å€‹ï¼‰
        if broadcast_analysis["broadcast_candidates"]:
            broadcast_summary.append("ğŸ“‹ BROADCASTå€™è£œè©³ç´°:")
            for i, candidate in enumerate(broadcast_analysis["broadcast_candidates"][:3]):
                confidence_icon = "ğŸ”¹" if candidate['confidence'] == 'high' else "ğŸ”¸"
                # æ—¢ã«BROADCASTæ¸ˆã¿ã‹ã©ã†ã‹ã‚’è¡¨ç¤º
                already_broadcasted = candidate.get('is_already_broadcasted', False)
                status_icon = "âœ…" if already_broadcasted else "ğŸ’¡"
                status_text = "æ—¢ã«é©ç”¨æ¸ˆã¿" if already_broadcasted else "é©ç”¨æ¨å¥¨"
                
                broadcast_summary.append(
                    f"  {confidence_icon} {candidate['table']}: éåœ§ç¸®{candidate['estimated_uncompressed_mb']:.1f}MB "
                    f"(åœ§ç¸®{candidate['estimated_compressed_mb']:.1f}MB, {candidate['file_format']}, "
                    f"åœ§ç¸®ç‡{candidate['compression_ratio']:.1f}x) {status_icon} {status_text}"
                )
        
        # æ—¢å­˜ã®BROADCASTé©ç”¨çŠ¶æ³ã‚’è€ƒæ…®ã—ãŸæ¨å¥¨ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸
        if broadcast_analysis["already_optimized"]:
            if broadcast_analysis["feasibility"] in ["recommended", "all_small"]:
                broadcast_summary.append("ğŸ’¡ è¿½åŠ æœ€é©åŒ–: å®Ÿè¡Œãƒ—ãƒ©ãƒ³ã¯æ—¢ã«æœ€é©åŒ–æ¸ˆã¿ã§ã™ãŒã€æ›´ãªã‚‹æ”¹å–„ã®ä½™åœ°ãŒã‚ã‚Šã¾ã™")
            else:
                broadcast_summary.append("âœ… æœ€é©åŒ–å®Œäº†: å®Ÿè¡Œãƒ—ãƒ©ãƒ³ã¯é©åˆ‡ã«BROADCAST JOINãŒé©ç”¨ã•ã‚Œã¦ã„ã¾ã™")
        else:
            if broadcast_analysis["feasibility"] in ["recommended", "all_small"]:
                broadcast_summary.append("ğŸš€ æœ€é©åŒ–æ¨å¥¨: BROADCASTãƒ’ãƒ³ãƒˆã®é©ç”¨ã«ã‚ˆã‚Šå¤§å¹…ãªæ€§èƒ½æ”¹å–„ãŒæœŸå¾…ã§ãã¾ã™")
            elif broadcast_analysis["feasibility"] == "not_recommended":
                broadcast_summary.append("âš ï¸ æœ€é©åŒ–å›°é›£: ãƒ†ãƒ¼ãƒ–ãƒ«ã‚µã‚¤ã‚ºãŒå¤§ããã€BROADCASTé©ç”¨ã¯æ¨å¥¨ã•ã‚Œã¾ã›ã‚“")
        
        # é‡è¦ãªæ³¨æ„äº‹é …
        if broadcast_analysis["reasoning"]:
            broadcast_summary.append("âš ï¸ é‡è¦ãªæ³¨æ„äº‹é …:")
            for reason in broadcast_analysis["reasoning"][:3]:  # æœ€å¤§3å€‹ã«æ‹¡å¼µ
                broadcast_summary.append(f"  â€¢ {reason}")
    else:
        broadcast_summary.append("âŒ JOINã‚¯ã‚¨ãƒªã§ã¯ãªã„ãŸã‚ã€BROADCASTãƒ’ãƒ³ãƒˆé©ç”¨å¯¾è±¡å¤–")
    
    optimization_prompt = f"""
ã‚ãªãŸã¯Databricksã®SQLãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æœ€é©åŒ–ã®å°‚é–€å®¶ã§ã™ã€‚ä»¥ä¸‹ã®**è©³ç´°ãªãƒœãƒˆãƒ«ãƒãƒƒã‚¯åˆ†æçµæœ**ã‚’åŸºã«ã€**å‡¦ç†é€Ÿåº¦é‡è¦–**ã§SQLã‚¯ã‚¨ãƒªã‚’æœ€é©åŒ–ã—ã¦ãã ã•ã„ã€‚

ã€é‡è¦ãªå‡¦ç†æ–¹é‡ã€‘
- ä¸€å›ã®å‡ºåŠ›ã§å®Œå…¨ãªSQLã‚¯ã‚¨ãƒªã‚’ç”Ÿæˆã—ã¦ãã ã•ã„
- æ®µéšçš„ãªå‡ºåŠ›ã‚„è¤‡æ•°å›ã«åˆ†ã‘ã¦ã®å‡ºåŠ›ã¯ç¦æ­¢ã§ã™
- thinkingæ©Ÿèƒ½ã§æ§‹é€ ç†è§£â†’ä¸€å›ã§å®Œå…¨ãªSQLå‡ºåŠ›

ã€å…ƒã®SQLã‚¯ã‚¨ãƒªã€‘
```sql
{original_query}
```

ã€ğŸ“Š ã‚»ãƒ«33è©³ç´°ãƒœãƒˆãƒ«ãƒãƒƒã‚¯åˆ†æçµæœã€‘
{chr(10).join(performance_critical_issues) if performance_critical_issues else "ç‰¹åˆ¥ãªé‡è¦èª²é¡Œã¯è¨­å®šãªã—"}

ã€ğŸ”„ REPARTITIONãƒ’ãƒ³ãƒˆï¼ˆã‚¹ãƒ”ãƒ«æ¤œå‡ºæ™‚ã®ã¿ï¼‰ã€‘
{chr(10).join(repartition_hints) if repartition_hints else "ã‚¹ãƒ”ãƒ«ãŒæ¤œå‡ºã•ã‚Œã¦ã„ãªã„ãŸã‚ã€REPARTITIONãƒ’ãƒ³ãƒˆã¯é©ç”¨å¯¾è±¡å¤–ã§ã™"}

ã€ğŸš€ å‡¦ç†é€Ÿåº¦é‡è¦–ã®æœ€é©åŒ–æ¨å¥¨äº‹é …ã€‘
{chr(10).join(speed_optimization_recommendations) if speed_optimization_recommendations else "ç‰¹åˆ¥ãªæ¨å¥¨äº‹é …ã¯ã‚ã‚Šã¾ã›ã‚“"}

ã€åŸºæœ¬çš„ãªãƒœãƒˆãƒ«ãƒãƒƒã‚¯æƒ…å ±ã€‘
{chr(10).join(optimization_context) if optimization_context else "ä¸»è¦ãªãƒœãƒˆãƒ«ãƒãƒƒã‚¯ã¯è¨­å®šãªã—"}

ã€BROADCASTåˆ†æçµæœã€‘
{chr(10).join(broadcast_summary)}

ã€Liquid Clusteringæ¨å¥¨ã€‘
{chr(10).join(clustering_recommendations) if clustering_recommendations else "ç‰¹åˆ¥ãªæ¨å¥¨äº‹é …ã¯ã‚ã‚Šã¾ã›ã‚“"}

ã€ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹åˆ†æçµæœï¼ˆã‚µãƒãƒªãƒ¼ï¼‰ã€‘
{analysis_summary}

ã€ğŸ” EXPLAINçµæœåˆ†æï¼ˆEXPLAIN_ENABLED=Yã®å ´åˆã®ã¿ï¼‰ã€‘
{f'''
**Physical Planåˆ†æ:**
```
{physical_plan}
```

**Photon Explanationåˆ†æ:**
```
{photon_explanation}
```

**Physical Planæœ€é©åŒ–ã®é‡è¦ãƒã‚¤ãƒ³ãƒˆ:**
- ãƒ•ã‚¡ã‚¤ãƒ«ã‚¹ã‚­ãƒ£ãƒ³ã®åŠ¹ç‡æ€§
- ã‚¸ãƒ§ã‚¤ãƒ³æˆ¦ç•¥ã®å¦¥å½“æ€§
- ã‚·ãƒ£ãƒƒãƒ•ãƒ«æ“ä½œã®æœ€å°åŒ–
- ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ã‚·ãƒ§ãƒ³ï¼ˆåˆ—é¸æŠï¼‰ã®æœ€é©åŒ–
- ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼ãƒ—ãƒƒã‚·ãƒ¥ãƒ€ã‚¦ãƒ³ã®æ´»ç”¨

**Photonæœ€é©åŒ–ã®é‡è¦ãƒã‚¤ãƒ³ãƒˆ:**
- Photonæœªå¯¾å¿œé–¢æ•°ã®æ¤œå‡ºã¨ä»£æ›¿é–¢æ•°ã¸ã®å¤‰æ›´
- ãƒ™ã‚¯ãƒˆãƒ«åŒ–å‡¦ç†ã«é©ã—ãŸé–¢æ•°ã®é¸æŠ
- Photonåˆ©ç”¨ç‡å‘ä¸Šã®ãŸã‚ã®æ›¸å¼å¤‰æ›´
- ã‚³ãƒ³ãƒ‘ã‚¤ãƒ«æ™‚æœ€é©åŒ–ã®æ´»ç”¨
''' if explain_enabled.upper() == 'Y' and (physical_plan or photon_explanation) else '(EXPLAINå®Ÿè¡ŒãŒç„¡åŠ¹ã€ã¾ãŸã¯EXPLAINçµæœãŒåˆ©ç”¨ã§ãã¾ã›ã‚“)'}

ã€ğŸ¯ å‡¦ç†é€Ÿåº¦é‡è¦–ã®æœ€é©åŒ–è¦æ±‚ã€‘
**æœ€é‡è¦**: ä»¥ä¸‹ã®é †åºã§å‡¦ç†é€Ÿåº¦ã®æ”¹å–„ã‚’å„ªå…ˆã—ã¦ãã ã•ã„

1. **ğŸš¨ CRITICALå„ªå…ˆåº¦**: ã‚¹ãƒ”ãƒ«å¯¾ç­–ï¼ˆãƒ¡ãƒ¢ãƒªåŠ¹ç‡æ”¹å–„ï¼‰
   - å¤§é‡ã‚¹ãƒ”ãƒ«ï¼ˆ5GBä»¥ä¸Šï¼‰ãŒæ¤œå‡ºã•ã‚ŒãŸå ´åˆã¯æœ€å„ªå…ˆã§å¯¾å‡¦
   - ãƒ¡ãƒ¢ãƒªåŠ¹ç‡çš„ãªJOINé †åºã®æ¤œè¨
   - ä¸­é–“çµæœã®ã‚µã‚¤ã‚ºå‰Šæ¸›

2. **ğŸ”„ REPARTITIONãƒ’ãƒ³ãƒˆé©ç”¨**ï¼ˆã‚¹ãƒ”ãƒ«æ¤œå‡ºæ™‚ã®ã¿ï¼‰
   - **ã‚¹ãƒ”ãƒ«ãŒæ¤œå‡ºã•ã‚ŒãŸå ´åˆã®ã¿**REPARTITIONãƒ’ãƒ³ãƒˆã‚’é©ç”¨
   - æ¤œå‡ºã•ã‚ŒãŸShuffle attributesã‚’åŸºã«å…·ä½“çš„ãªREPARTITIONãƒ’ãƒ³ãƒˆã‚’é©ç”¨
   - GROUP BYå‰ã¾ãŸã¯JOINå‰ã®é©åˆ‡ãªä½ç½®ã«REPARTITIONã‚’é…ç½®
   - æ¨å¥¨ãƒ‘ãƒ¼ãƒ†ã‚£ã‚·ãƒ§ãƒ³æ•°ã‚’ä½¿ç”¨

3. **âš–ï¸ ãƒ‡ãƒ¼ã‚¿ã‚¹ã‚­ãƒ¥ãƒ¼å¯¾ç­–**
   - ã‚¹ã‚­ãƒ¥ãƒ¼ãƒ‘ãƒ¼ãƒ†ã‚£ã‚·ãƒ§ãƒ³ï¼ˆ10å€‹ä»¥ä¸Šï¼‰æ¤œå‡ºæ™‚ã¯åˆ†æ•£æ”¹å–„ã‚’å„ªå…ˆ
   - é©åˆ‡ãªãƒ‘ãƒ¼ãƒ†ã‚£ã‚·ãƒ§ãƒ³ã‚­ãƒ¼ã®é¸æŠ
   - ãƒ‡ãƒ¼ã‚¿åˆ†æ•£ã®å‡ç­‰åŒ–

4. **ğŸ“ˆ ã‚·ãƒ£ãƒƒãƒ•ãƒ«æœ€é©åŒ–**
   - ã‚·ãƒ£ãƒƒãƒ•ãƒ«é‡ã®æœ€å°åŒ–
   - é©åˆ‡ãªJOINæˆ¦ç•¥ã®é¸æŠ
   - ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯è»¢é€é‡ã®å‰Šæ¸›

5. **ğŸ¯ BROADCASTæœ€é©åŒ–**ï¼ˆ30MBé–¾å€¤å³å®ˆï¼‰
   - 30MBä»¥ä¸‹ã®å°ãƒ†ãƒ¼ãƒ–ãƒ«ã®ã¿BROADCASTé©ç”¨
   - BROADCASTãƒ’ãƒ³ãƒˆå¥ã¯å¿…ãšSELECTæ–‡ã®ç›´å¾Œã«é…ç½®
   - BROADCASTãƒ’ãƒ³ãƒˆã«ã¯å¿…ãšãƒ†ãƒ¼ãƒ–ãƒ«å/ã‚¨ã‚¤ãƒªã‚¢ã‚¹åã‚’æŒ‡å®šï¼ˆ`/*+ BROADCAST(table_name) */`ï¼‰

6. **ğŸ’¾ ãƒ¡ãƒ¢ãƒªåŠ¹ç‡åŒ–**
   - ä¸è¦ãªã‚«ãƒ©ãƒ ã®é™¤å»
   - é©åˆ‡ãªãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°é †åº
   - ä¸­é–“çµæœã®ã‚­ãƒ£ãƒƒã‚·ãƒ¥æ´»ç”¨

7. **ğŸ”§ å®Ÿè¡Œãƒ—ãƒ©ãƒ³æœ€é©åŒ–**
   - PHOTONã‚¨ãƒ³ã‚¸ãƒ³æœ€é©åŒ–ï¼ˆç›®æ¨™ã¯Photonåˆ©ç”¨ç‡90%ä»¥ä¸Š)
   - Liquid Clusteringæ´»ç”¨ (Whereæ¡ä»¶ã®æ›¸ãæ›ãˆå«ã‚€æ¤œè¨ã‚’å®Ÿæ–½ï¼‰
   - CTEæ´»ç”¨ã«ã‚ˆã‚‹å…±é€šåŒ–

8. **ğŸ“Š EXPLAINçµæœã«åŸºã¥ãæœ€é©åŒ–**ï¼ˆEXPLAIN_ENABLED=Yã®å ´åˆï¼‰
   - **Physical Planåˆ†æã«åŸºã¥ãæœ€é©åŒ–**: 
     - éåŠ¹ç‡ãªã‚¹ã‚­ãƒ£ãƒ³æ“ä½œã®æ”¹å–„
     - ã‚¸ãƒ§ã‚¤ãƒ³é †åºã®æœ€é©åŒ–
     - ä¸è¦ãªã‚·ãƒ£ãƒƒãƒ•ãƒ«æ“ä½œã®å‰Šé™¤
     - ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ã‚·ãƒ§ãƒ³ãƒ—ãƒƒã‚·ãƒ¥ãƒ€ã‚¦ãƒ³ã®é©ç”¨
   - **Photonæœªå¯¾å¿œé–¢æ•°ã®æœ€é©åŒ–**:
     - Photon Explanationã§æ¤œå‡ºã•ã‚ŒãŸæœªå¯¾å¿œé–¢æ•°ã®ä»£æ›¿é–¢æ•°ã¸ã®å¤‰æ›´
     - ãƒ™ã‚¯ãƒˆãƒ«åŒ–å‡¦ç†ã«é©ã—ãŸé–¢æ•°ã¸ã®æ›¸ãæ›ãˆ
     - Photonåˆ©ç”¨ç‡å‘ä¸Šã®ãŸã‚ã®é–¢æ•°é¸æŠ
     - ã‚³ãƒ³ãƒ‘ã‚¤ãƒ«æ™‚æœ€é©åŒ–ã®æ´»ç”¨

9. **ğŸ¯ çµåˆã¨ãƒ‘ãƒ¼ãƒ†ã‚£ã‚·ãƒ§ãƒ‹ãƒ³ã‚°ã®æœ€é©åŒ–é †åº**ï¼ˆé‡è¦ãªæ§‹é€ çš„æœ€é©åŒ–ï¼‰
   - **BROADCASTçµåˆã‚’æœ€å„ªå…ˆ**: å°ã•ã„ãƒ†ãƒ¼ãƒ–ãƒ«ã¨ã®çµåˆã§ã¯å¿…ãšBROADCASTçµåˆã‚’ä½¿ç”¨
   - **BROADCASTåŠ¹æœã‚’å¦¨ã’ãªã„**: REPARTITIONã¯çµåˆå‰ã«å…¥ã‚Œãšã€BROADCASTçµåˆã®åŠ¹æœã‚’æœ€å¤§åŒ–
   - **çµåˆå¾Œã®REPARTITION**: çµåˆå¾Œã«GROUP BYã®åŠ¹ç‡åŒ–ã®ãŸã‚REPARTITIONãƒ’ãƒ³ãƒˆã‚’é©ç”¨
   - **CTEæ§‹é€ ã®æ´»ç”¨**: å¿…è¦ã«å¿œã˜ã¦CTEã‚’ä½¿ã£ã¦BROADCASTçµåˆå¾Œã«REPARTITIONã™ã‚‹æ§‹é€ ã§å‡ºåŠ›
   - **ã‚¹ãƒ”ãƒ«å›é¿ã¨ä¸¦åˆ—åº¦**: ã‚¹ãƒ”ãƒ«ã‚’å›é¿ã—ã¤ã¤ã€ä¸¦åˆ—åº¦ã®é«˜ã„å‡¦ç†ãŒã§ãã‚‹ã‚ˆã†æœ€é©åŒ–
   
   **ğŸ”„ æ¨å¥¨ã™ã‚‹å‡¦ç†ãƒ•ãƒ­ãƒ¼:**
   ```sql
   -- âœ… æ¨å¥¨ãƒ‘ã‚¿ãƒ¼ãƒ³: BROADCASTçµåˆ â†’ CTE â†’ REPARTITION â†’ GROUP BY
   WITH broadcast_joined AS (
     SELECT /*+ BROADCAST(small_table) */
       large_table.columns...,
       small_table.columns...
     FROM large_table
       JOIN small_table ON large_table.key = small_table.key
   ),
   repartitioned_for_groupby AS (
     SELECT /*+ REPARTITION(200, group_key) */
       columns...
     FROM broadcast_joined
   )
   SELECT 
     group_key,
     COUNT(*),
     SUM(amount)
   FROM repartitioned_for_groupby
   GROUP BY group_key
   ```
   
   **âŒ é¿ã‘ã‚‹ã¹ããƒ‘ã‚¿ãƒ¼ãƒ³:**
   ```sql
   -- âŒ æ‚ªã„ä¾‹: REPARTITION â†’ BROADCAST (BROADCASTåŠ¹æœã‚’é˜»å®³)
   SELECT /*+ REPARTITION(200, key), BROADCAST(small_table) */
     columns...
   FROM (SELECT /*+ REPARTITION(200, key) */ * FROM large_table) large_table
     JOIN small_table ON large_table.key = small_table.key
   ```

ã€ğŸ”„ REPARTITIONãƒ’ãƒ³ãƒˆé©ç”¨ãƒ«ãƒ¼ãƒ« - æ§‹æ–‡ã‚¨ãƒ©ãƒ¼é˜²æ­¢ã€‘
REPARTITIONãƒ’ãƒ³ãƒˆã‚’ä»˜ä¸ã™ã‚‹å ´åˆã¯ä»¥ä¸‹ã®æœ€é©åŒ–ãƒ«ãƒ¼ãƒ«ã‚’å®ˆã£ã¦ãã ã•ã„ï¼š

- **ã‚·ãƒ£ãƒƒãƒ•ãƒ«ã®åŠ¹ç‡åŒ–ãƒ»ã‚¹ã‚­ãƒ¥ãƒ¼é˜²æ­¢ãªã©ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æœ€é©åŒ–ãŒç›®çš„ã§ã‚ã‚‹ãŸã‚ã€ã‚¹ãƒ”ãƒ«ã‚¢ã‚¦ãƒˆã—ã¦ã„ãªã„å ´åˆã«ã¯ REPARTITION ãƒ’ãƒ³ãƒˆã¯ä¸è¦**
- **REPARTITIONãƒ’ãƒ³ãƒˆã¯ SELECT /*+ REPARTITION(ãƒ‘ãƒ¼ãƒ†ã‚£ã‚·ãƒ§ãƒ³æ•°, ã‚«ãƒ©ãƒ å) ã®å½¢å¼ã§æŒ‡å®š**
- **REPARTITIONãƒ’ãƒ³ãƒˆã®é©ç”¨ä½ç½®ã¯ã€å¯¾è±¡ã¨ãªã‚‹JOINã‚„GROUP BYã‚’å«ã‚€SELECTã®ç›´å‰ã§ã‚ã‚‹ãŸã‚ã€å‡ºåŠ›ã•ã‚ŒãŸoutput_explain_plan_*.txtã®Physical Planã‹ã‚‰å®Ÿè¡Œè¨ˆç”»ã‚’ç†è§£ã—ã€é©åˆ‡ãªä½ç½®ã«REPARTITION ãƒ’ãƒ³ãƒˆã‚’ä»˜ä¸ã™ã‚‹ã“ã¨**

**ğŸš¨ REPARTITIONãƒ’ãƒ³ãƒˆé…ç½®ã®é‡è¦ãªæ§‹æ–‡ãƒ«ãƒ¼ãƒ«:**
1. **JOINã‚„GROUP BYã®å‡¦ç†æ®µéšã§åŠ¹æœã‚’ç™ºæ®ã™ã‚‹ãŸã‚ã€å¿…ãšã‚µãƒ–ã‚¯ã‚¨ãƒªå†…éƒ¨ã«é…ç½®ã™ã‚‹**
2. **ãƒˆãƒƒãƒ—ãƒ¬ãƒ™ãƒ«ã®SELECTæ–‡ã«é…ç½®ã™ã‚‹ã¨æœ€çµ‚å‡ºåŠ›æ®µéšã®ã¿ã«å½±éŸ¿ã—ã€JOIN/GROUP BYå‡¦ç†æ®µéšã«ã¯å½±éŸ¿ã—ãªã„**
3. **è¤‡æ•°ã®REPARTITIONãƒ’ãƒ³ãƒˆã¯å„ã‚µãƒ–ã‚¯ã‚¨ãƒªå†…éƒ¨ã«å€‹åˆ¥ã«é…ç½®ã™ã‚‹**
4. **ãƒ‘ãƒ¼ãƒ†ã‚£ã‚·ãƒ§ãƒ³æ•°ã¨ã‚«ãƒ©ãƒ åã¯å¿…é ˆãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã¨ã—ã¦æŒ‡å®šã™ã‚‹**

å¾“æ¥ã®ãƒ«ãƒ¼ãƒ«ï¼š
- **ã‚¹ãƒ”ãƒ«ãŒæ¤œå‡ºã•ã‚ŒãŸå ´åˆã®ã¿é©ç”¨**
- GROUP BYã‚¯ã‚¨ãƒªã®å ´åˆ: GROUP BYå‰ã«REPARTITION(æ¨å¥¨æ•°, group_by_column)
- JOINã‚¯ã‚¨ãƒªã®å ´åˆ: JOINå‰ã«REPARTITION(æ¨å¥¨æ•°, join_key)
- è¤‡æ•°ãƒ†ãƒ¼ãƒ–ãƒ«ã®å ´åˆ: æœ€ã‚‚å¤§ããªãƒ†ãƒ¼ãƒ–ãƒ«ã‚’ãƒªãƒ‘ãƒ¼ãƒ†ã‚£ã‚·ãƒ§ãƒ³
- æ¨å¥¨ãƒ‘ãƒ¼ãƒ†ã‚£ã‚·ãƒ§ãƒ³æ•°: æ¤œå‡ºã•ã‚ŒãŸã‚¿ã‚¹ã‚¯æ•°ã®2å€ä»¥ä¸Šã€æœ€ä½200
- ã‚¹ãƒ”ãƒ«ãŒæ¤œå‡ºã•ã‚Œã¦ã„ãªã„å ´åˆ: REPARTITIONãƒ’ãƒ³ãƒˆã¯é©ç”¨ã—ãªã„

**ğŸš¨ CREATE TABLE AS SELECT (CTAS) ã§ã®REPARTITIONé…ç½®ã®é‡è¦ãªæ³¨æ„äº‹é …:**
- CREATE TABLE AS SELECTæ–‡ã§ã¯ã€ãƒˆãƒƒãƒ—ãƒ¬ãƒ™ãƒ«ã®SELECTå¥ã«REPARTITIONãƒ’ãƒ³ãƒˆã‚’é…ç½®ã™ã‚‹ã¨ã€**æœ€çµ‚çš„ãªå‡ºåŠ›æ›¸ãè¾¼ã¿æ®µéšã®ã¿ã«å½±éŸ¿**ã—ã€JOIN ã‚„é›†è¨ˆãªã©ã®ä¸­é–“å‡¦ç†æ®µéšã«ã¯å½±éŸ¿ã—ãªã„
- JOINã®å‰ã«ãƒ‘ãƒ¼ãƒ†ã‚£ã‚·ãƒ§ãƒ‹ãƒ³ã‚°ã‚’åˆ¶å¾¡ã™ã‚‹ã«ã¯ã€**REPARTITIONãƒ’ãƒ³ãƒˆã‚’ã‚µãƒ–ã‚¯ã‚¨ãƒªå†…éƒ¨ã«é…ç½®ã™ã‚‹å¿…è¦ãŒã‚ã‚‹**
- ã“ã‚Œã«ã‚ˆã‚Šã€SparkãŒãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ­ãƒ¼ã®é©åˆ‡ãªæ™‚ç‚¹ã§ãƒªãƒ‘ãƒ¼ãƒ†ã‚£ã‚·ãƒ§ãƒ³ã‚’é©ç”¨ã—ã€æ›¸ãè¾¼ã¿æ®µéšã§ã¯ãªãå®Ÿè¡Œæ®µéšã§æœ€é©åŒ–ã•ã‚Œã‚‹

**æ­£ã—ã„CTAS REPARTITIONãƒ’ãƒ³ãƒˆé…ç½®ä¾‹:**
```sql
-- âŒ é–“é•ã„: ãƒˆãƒƒãƒ—ãƒ¬ãƒ™ãƒ«ã®SELECTå¥ï¼ˆæ›¸ãè¾¼ã¿æ®µéšã®ã¿ã«å½±éŸ¿ï¼‰
CREATE TABLE optimized_table AS
SELECT /*+ REPARTITION(200, join_key) */
  t1.column1, t2.column2
FROM table1 t1
  JOIN table2 t2 ON t1.join_key = t2.join_key

-- âœ… æ­£ã—ã„: ã‚µãƒ–ã‚¯ã‚¨ãƒªå†…éƒ¨ã«é…ç½®ï¼ˆJOINå‡¦ç†æ®µéšã§æœ€é©åŒ–ï¼‰
CREATE TABLE optimized_table AS
SELECT 
  t1.column1, t2.column2
FROM (
  SELECT /*+ REPARTITION(200, join_key) */
    column1, join_key
  FROM table1
) t1
  JOIN table2 t2 ON t1.join_key = t2.join_key
```

**ğŸš¨ å…¨èˆ¬çš„ãªREPARTITIONãƒ’ãƒ³ãƒˆé…ç½®ã®é‡è¦ãªæ³¨æ„äº‹é …:**
- **CTASä»¥å¤–ã®ã‚¯ã‚¨ãƒªã§ã‚‚åŒæ§˜**ï¼šãƒˆãƒƒãƒ—ãƒ¬ãƒ™ãƒ«ã®ã‚¯ã‚¨ãƒªã«REPARTITIONãƒ’ãƒ³ãƒˆã‚’é…ç½®ã™ã‚‹ã¨ã€**æœ€çµ‚çš„ãªå‡ºåŠ›æ®µéšã®ã¿ã«å½±éŸ¿**ã—ã€JOIN ã‚„é›†è¨ˆãªã©ã®ä¸­é–“å¤‰æ›æ®µéšã«ã¯å½±éŸ¿ã—ãªã„
- ã“ã®å‹•ä½œã¯ã€çµæœã‚’ãƒ†ãƒ¼ãƒ–ãƒ«ã«æ›¸ãè¾¼ã‚€ã‹ã©ã†ã‹ã«é–¢ä¿‚ãªã**ã™ã¹ã¦ã®Spark SQLã‚¯ã‚¨ãƒªã§ä¸€è²«**ã—ã¦ã„ã‚‹
- JOINã®å…¥åŠ›æ®µéšã§ãƒªãƒ‘ãƒ¼ãƒ†ã‚£ã‚·ãƒ§ãƒ³ã‚’ç¢ºå®Ÿã«å®Ÿè¡Œã™ã‚‹ã«ã¯ã€**REPARTITIONãƒ’ãƒ³ãƒˆã‚’ã‚µãƒ–ã‚¯ã‚¨ãƒªå†…éƒ¨ã«é…ç½®ã™ã‚‹å¿…è¦ãŒã‚ã‚‹**
- ã“ã‚Œã«ã‚ˆã‚Šã€SparkãŒé©åˆ‡ãªãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ­ãƒ¼ã®æ™‚ç‚¹ã§ãƒªãƒ‘ãƒ¼ãƒ†ã‚£ã‚·ãƒ§ãƒ³ã‚’é©ç”¨ã—ã€æœ€çµ‚å‡ºåŠ›æ®µéšã§ã¯ãªãå®Ÿè¡Œæ®µéšã§æœ€é©åŒ–ã•ã‚Œã‚‹

**ä¸€èˆ¬çš„ãªã‚¯ã‚¨ãƒªã§ã®æ­£ã—ã„REPARTITIONãƒ’ãƒ³ãƒˆé…ç½®ä¾‹:**
```sql
-- âŒ é–“é•ã„: ãƒˆãƒƒãƒ—ãƒ¬ãƒ™ãƒ«ã®SELECTå¥ï¼ˆæœ€çµ‚å‡ºåŠ›æ®µéšã®ã¿ã«å½±éŸ¿ï¼‰
SELECT /*+ REPARTITION(200, join_key) */
  t1.column1, t2.column2
FROM table1 t1
  JOIN table2 t2 ON t1.join_key = t2.join_key

-- âœ… æ­£ã—ã„: ã‚µãƒ–ã‚¯ã‚¨ãƒªå†…éƒ¨ã«é…ç½®ï¼ˆJOINå‡¦ç†æ®µéšã§æœ€é©åŒ–ï¼‰
SELECT 
  t1.column1, t2.column2
FROM (
  SELECT /*+ REPARTITION(200, join_key) */
    column1, join_key
  FROM table1
) t1
  JOIN table2 t2 ON t1.join_key = t2.join_key

-- âœ… æ­£ã—ã„: ã‚ˆã‚Šè¤‡é›‘ãªã‚±ãƒ¼ã‚¹ï¼ˆè¤‡æ•°ã®ã‚µãƒ–ã‚¯ã‚¨ãƒªã§ã®ãƒªãƒ‘ãƒ¼ãƒ†ã‚£ã‚·ãƒ§ãƒ³ï¼‰
SELECT 
  t1.column1, t2.column2, t3.column3
FROM (
  SELECT /*+ REPARTITION(200, join_key) */
    column1, join_key
  FROM table1
) t1
  JOIN (
    SELECT /*+ REPARTITION(200, join_key) */
      column2, join_key
    FROM table2
  ) t2 ON t1.join_key = t2.join_key
  JOIN table3 t3 ON t2.join_key = t3.join_key
```

**ğŸš¨ å…¨èˆ¬çš„ãªREPARTITIONãƒ’ãƒ³ãƒˆé…ç½®ã®é‡è¦ãªæ³¨æ„äº‹é …:**
- **CTASä»¥å¤–ã®ã‚¯ã‚¨ãƒªã§ã‚‚åŒæ§˜**ï¼šãƒˆãƒƒãƒ—ãƒ¬ãƒ™ãƒ«ã®ã‚¯ã‚¨ãƒªã«REPARTITIONãƒ’ãƒ³ãƒˆã‚’é…ç½®ã™ã‚‹ã¨ã€**æœ€çµ‚çš„ãªå‡ºåŠ›æ®µéšã®ã¿ã«å½±éŸ¿**ã—ã€JOIN ã‚„é›†è¨ˆãªã©ã®ä¸­é–“å¤‰æ›æ®µéšã«ã¯å½±éŸ¿ã—ãªã„
- ã“ã®å‹•ä½œã¯ã€çµæœã‚’ãƒ†ãƒ¼ãƒ–ãƒ«ã«æ›¸ãè¾¼ã‚€ã‹ã©ã†ã‹ã«é–¢ä¿‚ãªã**ã™ã¹ã¦ã®Spark SQLã‚¯ã‚¨ãƒªã§ä¸€è²«**ã—ã¦ã„ã‚‹
- JOINã®å…¥åŠ›æ®µéšã§ãƒªãƒ‘ãƒ¼ãƒ†ã‚£ã‚·ãƒ§ãƒ³ã‚’ç¢ºå®Ÿã«å®Ÿè¡Œã™ã‚‹ã«ã¯ã€**REPARTITIONãƒ’ãƒ³ãƒˆã‚’ã‚µãƒ–ã‚¯ã‚¨ãƒªå†…éƒ¨ã«é…ç½®ã™ã‚‹å¿…è¦ãŒã‚ã‚‹**
- ã“ã‚Œã«ã‚ˆã‚Šã€SparkãŒé©åˆ‡ãªãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ­ãƒ¼ã®æ™‚ç‚¹ã§ãƒªãƒ‘ãƒ¼ãƒ†ã‚£ã‚·ãƒ§ãƒ³ã‚’é©ç”¨ã—ã€æœ€çµ‚å‡ºåŠ›æ®µéšã§ã¯ãªãå®Ÿè¡Œæ®µéšã§æœ€é©åŒ–ã•ã‚Œã‚‹

**ä¸€èˆ¬çš„ãªã‚¯ã‚¨ãƒªã§ã®æ­£ã—ã„REPARTITIONãƒ’ãƒ³ãƒˆé…ç½®ä¾‹:**
```sql
-- âŒ é–“é•ã„: ãƒˆãƒƒãƒ—ãƒ¬ãƒ™ãƒ«ã®SELECTå¥ï¼ˆæœ€çµ‚å‡ºåŠ›æ®µéšã®ã¿ã«å½±éŸ¿ï¼‰
SELECT /*+ REPARTITION(200, join_key) */
  t1.column1, t2.column2
FROM table1 t1
  JOIN table2 t2 ON t1.join_key = t2.join_key

-- âœ… æ­£ã—ã„: ã‚µãƒ–ã‚¯ã‚¨ãƒªå†…éƒ¨ã«é…ç½®ï¼ˆJOINå‡¦ç†æ®µéšã§æœ€é©åŒ–ï¼‰
SELECT 
  t1.column1, t2.column2
FROM (
  SELECT /*+ REPARTITION(200, join_key) */
    column1, join_key
  FROM table1
) t1
  JOIN table2 t2 ON t1.join_key = t2.join_key
```



ã€é‡è¦ãªåˆ¶ç´„ã€‘
- çµ¶å¯¾ã«ä¸å®Œå…¨ãªã‚¯ã‚¨ãƒªã‚’ç”Ÿæˆã—ãªã„ã§ãã ã•ã„
- ã™ã¹ã¦ã®ã‚«ãƒ©ãƒ åã€ãƒ†ãƒ¼ãƒ–ãƒ«åã€CTEåã‚’å®Œå…¨ã«è¨˜è¿°ã—ã¦ãã ã•ã„
- ãƒ—ãƒ¬ãƒ¼ã‚¹ãƒ›ãƒ«ãƒ€ãƒ¼ï¼ˆ...ã€[çœç•¥]ã€ç©ºç™½ãªã©ï¼‰ã¯ä¸€åˆ‡ä½¿ç”¨ã—ãªã„ã§ãã ã•ã„
- ã‚ªãƒªã‚¸ãƒŠãƒ«ã‚¯ã‚¨ãƒªã®ã™ã¹ã¦ã®SELECTé …ç›®ã‚’ä¿æŒã—ã¦ãã ã•ã„
- **ğŸš¨ DISTINCTå¥ã®çµ¶å¯¾ä¿æŒ**: å…ƒã®ã‚¯ã‚¨ãƒªã«DISTINCTå¥ãŒã‚ã‚‹å ´åˆã¯ã€**å¿…ãšDISTINCTå¥ã‚’ä¿æŒ**ã—ã¦ãã ã•ã„
- **ãƒ’ãƒ³ãƒˆå¥è¿½åŠ æ™‚ã®DISTINCTä¿æŒ**: BROADCASTãƒ’ãƒ³ãƒˆã‚„REPARTITIONãƒ’ãƒ³ãƒˆã‚’è¿½åŠ ã™ã‚‹éš›ã‚‚ã€DISTINCTå¥ã¯çµ¶å¯¾ã«å‰Šé™¤ã—ãªã„ã§ãã ã•ã„
- å…ƒã®ã‚¯ã‚¨ãƒªãŒé•·ã„å ´åˆã§ã‚‚ã€ã™ã¹ã¦ã®ã‚«ãƒ©ãƒ ã‚’çœç•¥ã›ãšã«è¨˜è¿°ã—ã¦ãã ã•ã„
- å®Ÿéš›ã«å®Ÿè¡Œã§ãã‚‹å®Œå…¨ãªSQLã‚¯ã‚¨ãƒªã®ã¿ã‚’å‡ºåŠ›ã—ã¦ãã ã•ã„
- å…ƒã®ã‚¯ã‚¨ãƒªã¨åŒã˜ã‚¢ã‚¦ãƒˆãƒ—ãƒƒãƒˆã«ãªã‚‹ã“ã¨ã‚’å³å®ˆã—ã¦ãã ã•ã„

ã€ğŸš¨ BROADCASTãƒ’ãƒ³ãƒˆé…ç½®ã®å³æ ¼ãªãƒ«ãƒ¼ãƒ« - æ§‹æ–‡ã‚¨ãƒ©ãƒ¼é˜²æ­¢ã€‘
**çµ¶å¯¾ã«å®ˆã‚‹ã¹ãæ–‡æ³•ãƒ«ãƒ¼ãƒ«ï¼ˆæ§‹æ–‡ã‚¨ãƒ©ãƒ¼é˜²æ­¢ã®ãŸã‚å¿…é ˆï¼‰:**

âœ… **æ­£ã—ã„é…ç½®ï¼ˆå¿…é ˆï¼‰:**
```sql
SELECT /*+ BROADCAST(table_name) */
  column1, column2, ...
FROM table1 t1
  JOIN table2 t2 ON t1.id = t2.id
```

âœ… **DISTINCTå¥ã¨ã®æ­£ã—ã„çµ„ã¿åˆã‚ã›ï¼ˆçµ¶å¯¾å¿…é ˆï¼‰:**
```sql
-- ğŸš¨ é‡è¦: DISTINCTå¥ã¯å¿…ãšãƒ’ãƒ³ãƒˆå¥ã®å¾Œã«é…ç½®
SELECT /*+ BROADCAST(table_name) */ DISTINCT
  cs.ID, cs.column1, cs.column2, ...
FROM table1 cs
  JOIN table2 t2 ON cs.id = t2.id

-- è¤‡æ•°ãƒ’ãƒ³ãƒˆã¨DISTINCTã®çµ„ã¿åˆã‚ã›
SELECT /*+ REPARTITION(200), BROADCAST(small_table) */ DISTINCT
  t1.column1, t2.column2
FROM table1 t1
  JOIN table2 t2 ON t1.id = t2.id
```

âŒ **çµ¶å¯¾ã«ç¦æ­¢ã•ã‚Œã‚‹èª¤ã£ãŸé…ç½®ï¼ˆæ§‹æ–‡ã‚¨ãƒ©ãƒ¼ã®åŸå› ï¼‰:**
```sql
-- ã“ã‚Œã‚‰ã¯å…¨ã¦æ–‡æ³•ã‚¨ãƒ©ãƒ¼ã«ãªã‚Šã¾ã™
FROM table1 /*+ BROADCAST(table1) */
JOIN /*+ BROADCAST(table2) */ table2 ON ...
WHERE /*+ BROADCAST(table1) */ ...
-- ğŸš¨ ç‰¹ã«é‡è¦: ã‚µãƒ–ã‚¯ã‚¨ãƒªå†…éƒ¨ã¸ã®BROADCASTãƒ’ãƒ³ãƒˆé…ç½®ã¯ç¦æ­¢
LEFT JOIN (
  SELECT /*+ BROADCAST(COUPON) */ distinct  -- âŒ é–“é•ã„: ã‚µãƒ–ã‚¯ã‚¨ãƒªå†…éƒ¨é…ç½®
    qtz_receipt_id
  FROM prd_delta.qtz_s3_etl.fm_coupon_pos COUPON
) COUPON ON ...
```

**ğŸš¨ æ§‹æ–‡ã‚¨ãƒ©ãƒ¼é˜²æ­¢ã®ãŸã‚ã®å¿…é ˆç¢ºèªäº‹é …:**
1. **BROADCASTãƒ’ãƒ³ãƒˆã¯å¿…ãšãƒ¡ã‚¤ãƒ³ã‚¯ã‚¨ãƒªã®æœ€åˆã®SELECTæ–‡ã®ç›´å¾Œã®ã¿**
2. **ã‚µãƒ–ã‚¯ã‚¨ãƒªã€CTEã€JOINã‚µãƒ–ã‚¯ã‚¨ãƒªå†…éƒ¨ã«ã¯çµ¶å¯¾ã«é…ç½®ã—ãªã„**
3. **FROMå¥ã€JOINå¥ã€WHEREå¥å†…ã«ã¯çµ¶å¯¾ã«é…ç½®ã—ãªã„**
4. **è¤‡æ•°ã®BROADCASTãƒ’ãƒ³ãƒˆã¯1ã¤ã®ãƒ¡ã‚¤ãƒ³ã‚¯ã‚¨ãƒªSELECTç›´å¾Œã«çµ±åˆã™ã‚‹**
5. **BROADCASTãƒ’ãƒ³ãƒˆã«ã¯å¿…ãšãƒ†ãƒ¼ãƒ–ãƒ«åã¾ãŸã¯ã‚¨ã‚¤ãƒªã‚¢ã‚¹åã‚’æŒ‡å®šã™ã‚‹**

**é‡è¦ãªé…ç½®ãƒ«ãƒ¼ãƒ«:**
1. **ãƒ’ãƒ³ãƒˆã¯å¿…ãšãƒ¡ã‚¤ãƒ³ã‚¯ã‚¨ãƒªã®SELECTæ–‡ã®ç›´å¾Œ**ã«é…ç½®
2. **FROMå¥ã€JOINå¥ã€WHEREå¥å†…ã«ã¯çµ¶å¯¾ã«é…ç½®ã—ãªã„**
3. **ğŸš¨ ã‚µãƒ–ã‚¯ã‚¨ãƒªå†…éƒ¨ã®SELECTæ–‡ã«ã¯çµ¶å¯¾ã«é…ç½®ã—ãªã„**
4. **è¤‡æ•°ãƒ†ãƒ¼ãƒ–ãƒ«ã®BROADCASTæ™‚ã¯å…¨ã¦ãƒ¡ã‚¤ãƒ³ã‚¯ã‚¨ãƒªã®SELECTç›´å¾Œã«çµ±åˆ**
5. **CTEã‚’ä½¿ç”¨ã™ã‚‹å ´åˆã¯æœ€çµ‚çš„ãªãƒ¡ã‚¤ãƒ³ã‚¯ã‚¨ãƒªã®SELECTç›´å¾Œã«é…ç½®**

**ğŸš¨ ã‚µãƒ–ã‚¯ã‚¨ãƒªä½¿ç”¨æ™‚ã®æ­£ã—ã„é…ç½®ï¼ˆçµ¶å¯¾éµå®ˆï¼‰:**
```sql
-- âœ… æ­£ã—ã„: ãƒ¡ã‚¤ãƒ³ã‚¯ã‚¨ãƒªã®SELECTç›´å¾Œã«å…¨ã¦ã®BROADCASTãƒ’ãƒ³ãƒˆã‚’çµ±åˆ
SELECT /*+ REPARTITION(2316), BROADCAST(COUPON) */
  retail_item_name, qtz_item_id, ...
FROM prd_public.public.dmp_id_pos POS
  LEFT JOIN (
    SELECT distinct  -- âœ… æ­£ã—ã„: ã‚µãƒ–ã‚¯ã‚¨ãƒªå†…éƒ¨ã«ã¯ãƒ’ãƒ³ãƒˆãªã—
      qtz_receipt_id
    FROM prd_delta.qtz_s3_etl.fm_coupon_pos COUPON
    WHERE dt between '20240418' and '20250417'
  ) COUPON ON POS.qtz_receipt_id = COUPON.qtz_receipt_id
WHERE dt between '20240418' and '20250417'
```

**âŒ çµ¶å¯¾ã«ç¦æ­¢ã•ã‚Œã‚‹é–“é•ã£ãŸé…ç½®:**
```sql
-- âŒ é–“é•ã„: ã‚µãƒ–ã‚¯ã‚¨ãƒªå†…éƒ¨ã«BROADCASTãƒ’ãƒ³ãƒˆé…ç½®
SELECT /*+ REPARTITION(2316) */
  retail_item_name, qtz_item_id, ...
FROM prd_public.public.dmp_id_pos POS
  LEFT JOIN (
    SELECT /*+ BROADCAST(COUPON) */ distinct  -- âŒ ç¦æ­¢: ã‚µãƒ–ã‚¯ã‚¨ãƒªå†…éƒ¨é…ç½®
      qtz_receipt_id
    FROM prd_delta.qtz_s3_etl.fm_coupon_pos COUPON
  ) COUPON ON ...
```

**è¤‡æ•°ãƒ†ãƒ¼ãƒ–ãƒ«BROADCASTä¾‹:**
```sql
SELECT /*+ BROADCAST(table1, table2) */
  t1.column1, t2.column2
FROM table1 t1
  JOIN table2 t2 ON t1.id = t2.id
```

**CTEã§ã®BROADCASTä¾‹:**
```sql
WITH cte1 AS (
  SELECT  -- âœ… æ­£ã—ã„: CTEå†…éƒ¨ã«ã¯ãƒ’ãƒ³ãƒˆãªã—
    column1, column2
  FROM table1
)
SELECT /*+ BROADCAST(table2) */  -- âœ… æ­£ã—ã„: ãƒ¡ã‚¤ãƒ³ã‚¯ã‚¨ãƒªã®SELECTç›´å¾Œã«é…ç½®
  c.column1, t.column2
FROM cte1 c
  JOIN table2 t ON c.id = t.id
```

**ğŸš¨ Spark/Databricks SQLãƒ’ãƒ³ãƒˆè§£æã®é‡è¦ãªä»•æ§˜:**
- ãƒ’ãƒ³ãƒˆã¯SELECTæ–‡ã®ç›´å¾Œã§ã®ã¿è§£æã•ã‚Œã‚‹
- ã‚µãƒ–ã‚¯ã‚¨ãƒªå†…éƒ¨ã®ãƒ’ãƒ³ãƒˆã¯ã€å¤–å´ã®ã‚¯ã‚¨ãƒªã‹ã‚‰å‚ç…§ã•ã‚Œã‚‹ãƒ†ãƒ¼ãƒ–ãƒ«ã‚¨ã‚¤ãƒªã‚¢ã‚¹ãŒè§£ææ™‚ã«æœªå®šç¾©ã®ãŸã‚ç„¡è¦–ã•ã‚Œã‚‹
- è¤‡æ•°ã®ãƒ†ãƒ¼ãƒ–ãƒ«ã«å¯¾ã™ã‚‹BROADCASTãƒ’ãƒ³ãƒˆã¯ã€ãƒ¡ã‚¤ãƒ³ã‚¯ã‚¨ãƒªã®SELECTç›´å¾Œã«çµ±åˆã™ã‚‹ã“ã¨ã§å…¨ã¦æœ‰åŠ¹ã«ãªã‚‹

**ğŸš¨ ãƒ’ãƒ³ãƒˆæ§‹æ–‡ã®é‡è¦ãªæ³¨æ„ç‚¹:**
- è¤‡æ•°ã®ãƒ’ãƒ³ãƒˆç¨®é¡ã¯ã‚«ãƒ³ãƒåŒºåˆ‡ã‚Šã§æŒ‡å®š: `/*+ REPARTITION(100), BROADCAST(table1) */` âœ…
- BROADCASTãƒ’ãƒ³ãƒˆã«ã¯å¿…ãšãƒ†ãƒ¼ãƒ–ãƒ«å/ã‚¨ã‚¤ãƒªã‚¢ã‚¹åã‚’æŒ‡å®š: `/*+ BROADCAST(table_name) */` âœ…
- ãƒ†ãƒ¼ãƒ–ãƒ«åãªã—ã®BROADCASTãƒ’ãƒ³ãƒˆã¯ç„¡åŠ¹: `/*+ BROADCAST */` âŒ
- åŒä¸€ãƒ’ãƒ³ãƒˆå†…ã®è¤‡æ•°ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã¯ã‚«ãƒ³ãƒåŒºåˆ‡ã‚Š: `/*+ BROADCAST(table1, table2) */` âœ…
- ç•°ãªã‚‹ãƒ’ãƒ³ãƒˆç¨®é¡ã®çµ„ã¿åˆã‚ã›ã¯ã‚«ãƒ³ãƒåŒºåˆ‡ã‚Š: `/*+ REPARTITION(200), BROADCAST(small_table), COALESCE(1) */` âœ…

ã€å‡ºåŠ›å½¢å¼ã€‘
## ğŸš€ å‡¦ç†é€Ÿåº¦é‡è¦–ã®æœ€é©åŒ–ã•ã‚ŒãŸSQL

**é©ç”¨ã—ãŸæœ€é©åŒ–æ‰‹æ³•**:
- [å…·ä½“çš„ãªæœ€é©åŒ–æ‰‹æ³•ã®ãƒªã‚¹ãƒˆ]
- [REPARTITIONãƒ’ãƒ³ãƒˆã®é©ç”¨è©³ç´°ï¼ˆã‚¹ãƒ”ãƒ«æ¤œå‡ºæ™‚ã®ã¿ï¼‰]
- [BROADCASTãƒ’ãƒ³ãƒˆã®é©ç”¨è©³ç´° - SELECTç›´å¾Œé…ç½®]
- [BROADCASTçµåˆå„ªå…ˆã¨REPARTITIONé †åºæœ€é©åŒ–ã®è©³ç´°]
- [CTEæ§‹é€ ã«ã‚ˆã‚‹æ®µéšçš„æœ€é©åŒ–ã®é©ç”¨è©³ç´°]
- [æ¨å®šã•ã‚Œã‚‹æ€§èƒ½æ”¹å–„åŠ¹æœ]

**ğŸš¨ æ§‹æ–‡ã‚¨ãƒ©ãƒ¼é˜²æ­¢ã®æœ€çµ‚ç¢ºèª**:
- âœ… å…¨ã¦ã®BROADCASTãƒ’ãƒ³ãƒˆãŒãƒ¡ã‚¤ãƒ³ã‚¯ã‚¨ãƒªã®æœ€åˆã®SELECTæ–‡ã®ç›´å¾Œã«é…ç½®ã•ã‚Œã¦ã„ã‚‹
- âœ… ã‚µãƒ–ã‚¯ã‚¨ãƒªã€CTEã€JOINã‚µãƒ–ã‚¯ã‚¨ãƒªå†…éƒ¨ã«BROADCASTãƒ’ãƒ³ãƒˆãŒé…ç½®ã•ã‚Œã¦ã„ãªã„
- âœ… FROMå¥ã€JOINå¥ã€WHEREå¥å†…ã«ãƒ’ãƒ³ãƒˆãŒé…ç½®ã•ã‚Œã¦ã„ãªã„
- âœ… BROADCASTãƒ’ãƒ³ãƒˆã«å¿…ãšãƒ†ãƒ¼ãƒ–ãƒ«å/ã‚¨ã‚¤ãƒªã‚¢ã‚¹åãŒæŒ‡å®šã•ã‚Œã¦ã„ã‚‹
- âœ… REPARTITIONãƒ’ãƒ³ãƒˆã¯é©åˆ‡ãªã‚µãƒ–ã‚¯ã‚¨ãƒªå†…éƒ¨ã«é…ç½®ã•ã‚Œã¦ã„ã‚‹
- âœ… è¤‡æ•°ãƒ’ãƒ³ãƒˆã¯ã‚«ãƒ³ãƒåŒºåˆ‡ã‚Šã§æŒ‡å®šã•ã‚Œã¦ã„ã‚‹
- âœ… **DISTINCTå¥ãŒå…ƒã®ã‚¯ã‚¨ãƒªã«ã‚ã‚‹å ´åˆã¯å¿…ãšä¿æŒã•ã‚Œã¦ã„ã‚‹**
- âœ… **ãƒ’ãƒ³ãƒˆå¥è¿½åŠ æ™‚ã«DISTINCTå¥ãŒå‰Šé™¤ã•ã‚Œã¦ã„ãªã„**
- âœ… **DISTINCTå¥ãŒãƒ’ãƒ³ãƒˆå¥ã®ç›´å¾Œã«æ­£ã—ãé…ç½®ã•ã‚Œã¦ã„ã‚‹ï¼ˆä¾‹: SELECT /*+ BROADCAST(table) */ DISTINCTï¼‰**
- âœ… ãƒ—ãƒ¬ãƒ¼ã‚¹ãƒ›ãƒ«ãƒ€ãƒ¼ï¼ˆ...ã€[çœç•¥]ç­‰ï¼‰ãŒä¸€åˆ‡ä½¿ç”¨ã•ã‚Œã¦ã„ãªã„
- âœ… å®Œå…¨ãªSQLæ§‹æ–‡ã«ãªã£ã¦ã„ã‚‹ï¼ˆä¸å®Œå…¨ãªã‚¯ã‚¨ãƒªã§ã¯ãªã„ï¼‰
- âœ… NULLãƒªãƒ†ãƒ©ãƒ«ãŒé©åˆ‡ãªå‹ã§ã‚­ãƒ£ã‚¹ãƒˆã•ã‚Œã¦ã„ã‚‹
- âœ… BROADCASTçµåˆã‚’å„ªå…ˆã—ã€REPARTITIONã§åŠ¹æœã‚’å¦¨ã’ã¦ã„ãªã„
- âœ… å¿…è¦ã«å¿œã˜ã¦CTEæ§‹é€ ã§BROADCASTçµåˆå¾Œã«REPARTITIONã‚’é…ç½®ã—ã¦ã„ã‚‹
- âœ… ã‚¹ãƒ”ãƒ«å›é¿ã¨ä¸¦åˆ—åº¦å‘ä¸Šã®ä¸¡æ–¹ã‚’è€ƒæ…®ã—ãŸæ§‹é€ ã«ãªã£ã¦ã„ã‚‹

```sql
-- ğŸš¨ é‡è¦: BROADCASTãƒ’ãƒ³ãƒˆã¯å¿…ãšãƒ¡ã‚¤ãƒ³ã‚¯ã‚¨ãƒªã®æœ€åˆã®SELECTæ–‡ã®ç›´å¾Œã«é…ç½®
-- ä¾‹: SELECT /*+ BROADCAST(table_name) */ column1, column2, ...
-- è¤‡æ•°ãƒ’ãƒ³ãƒˆä¾‹ï¼ˆã‚¹ãƒ”ãƒ«æ¤œå‡ºæ™‚ã®ã¿ï¼‰: SELECT /*+ REPARTITION(100), BROADCAST(small_table) */ column1, column2, ...
-- ğŸš¨ DISTINCTå¥ä¿æŒä¾‹: SELECT /*+ BROADCAST(table_name) */ DISTINCT cs.ID, cs.column1, ...
-- ğŸš¨ è¤‡æ•°ãƒ’ãƒ³ãƒˆ+DISTINCTä¾‹: SELECT /*+ REPARTITION(200), BROADCAST(small_table) */ DISTINCT t1.column1, t2.column2, ...
-- ç„¡åŠ¹ãªä¾‹: SELECT /*+ BROADCAST */ column1, column2, ... (ãƒ†ãƒ¼ãƒ–ãƒ«åãªã— - ç„¡åŠ¹)
-- ğŸš¨ REPARTITIONãƒ’ãƒ³ãƒˆã¯ã‚µãƒ–ã‚¯ã‚¨ãƒªå†…éƒ¨ã«é…ç½®: SELECT ... FROM (SELECT /*+ REPARTITION(200, join_key) */ ... FROM table) ...
[å®Œå…¨ãªSQL - ã™ã¹ã¦ã®ã‚«ãƒ©ãƒ ãƒ»CTEãƒ»ãƒ†ãƒ¼ãƒ–ãƒ«åã‚’çœç•¥ãªã—ã§è¨˜è¿°]
```

## æ”¹å–„ãƒã‚¤ãƒ³ãƒˆ
[3ã¤ã®ä¸»è¦æ”¹å–„ç‚¹]

## BROADCASTé©ç”¨æ ¹æ‹ ï¼ˆ30MBé–¾å€¤åŸºæº–ï¼‰
[BROADCASTãƒ’ãƒ³ãƒˆé©ç”¨ã®è©³ç´°æ ¹æ‹ ]
- ğŸ“ Sparké–¾å€¤: 30MBï¼ˆéåœ§ç¸®ã€spark.databricks.optimizer.autoBroadcastJoinThresholdï¼‰
- ğŸ¯ é©ç”¨ãƒ†ãƒ¼ãƒ–ãƒ«: [ãƒ†ãƒ¼ãƒ–ãƒ«å]
  - éåœ§ç¸®æ¨å®šã‚µã‚¤ã‚º: [XX]MB
  - åœ§ç¸®æ¨å®šã‚µã‚¤ã‚º: [YY]MB
  - æ¨å®šåœ§ç¸®ç‡: [ZZ]x
  - ãƒ•ã‚¡ã‚¤ãƒ«å½¢å¼: [parquet/delta/ç­‰]
  - æ¨å®šæ ¹æ‹ : [è¡Œæ•°ãƒ»ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿é‡ãƒ™ãƒ¼ã‚¹]
- âš–ï¸ åˆ¤å®šçµæœ: [strongly_recommended/conditionally_recommended/not_recommended]
- ğŸ” é–¾å€¤é©åˆæ€§: [30MBä»¥ä¸‹ã§é©åˆ/30MBè¶…éã§éé©åˆ]
- ğŸ’¾ ãƒ¡ãƒ¢ãƒªå½±éŸ¿: [æ¨å®šãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡]MB ãŒãƒ¯ãƒ¼ã‚«ãƒ¼ãƒãƒ¼ãƒ‰ã«ãƒ–ãƒ­ãƒ¼ãƒ‰ã‚­ãƒ£ã‚¹ãƒˆ
- ğŸš€ æœŸå¾…åŠ¹æœ: [ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯è»¢é€é‡å‰Šæ¸›ãƒ»JOINå‡¦ç†é«˜é€ŸåŒ–ãƒ»ã‚·ãƒ£ãƒƒãƒ•ãƒ«å‰Šæ¸›ãªã©]
- âœ… **ãƒ’ãƒ³ãƒˆå¥é…ç½®**: SELECTæ–‡ã®ç›´å¾Œã« `/*+ BROADCAST(table_name) */` ã‚’æ­£ã—ãé…ç½®ï¼ˆãƒ†ãƒ¼ãƒ–ãƒ«åå¿…é ˆï¼‰

## æœŸå¾…åŠ¹æœ  
[å®Ÿè¡Œæ™‚é–“ãƒ»ãƒ¡ãƒ¢ãƒªãƒ»ã‚¹ãƒ”ãƒ«æ”¹å–„ã®è¦‹è¾¼ã¿ï¼ˆBROADCASTåŠ¹æœã‚’å«ã‚€ï¼‰]
"""

    # è¨­å®šã•ã‚ŒãŸLLMãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ã‚’ä½¿ç”¨
    provider = LLM_CONFIG["provider"]
    
    try:
        if provider == "databricks":
            optimized_result = _call_databricks_llm(optimization_prompt)
        elif provider == "openai":
            optimized_result = _call_openai_llm(optimization_prompt)
        elif provider == "azure_openai":
            optimized_result = _call_azure_openai_llm(optimization_prompt)
        elif provider == "anthropic":
            optimized_result = _call_anthropic_llm(optimization_prompt)
        else:
            return "âš ï¸ è¨­å®šã•ã‚ŒãŸLLMãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ãŒèªè­˜ã§ãã¾ã›ã‚“"
        
        # thinking_enabled: Trueã®å ´åˆã«optimized_resultãŒãƒªã‚¹ãƒˆã«ãªã‚‹ã“ã¨ãŒã‚ã‚‹ãŸã‚å¯¾å¿œ
        # ã“ã“ã§ã¯å…ƒã®ãƒ¬ã‚¹ãƒãƒ³ã‚¹å½¢å¼ã‚’ä¿æŒã—ã¦è¿”ã™ï¼ˆå¾Œã§ç”¨é€”ã«å¿œã˜ã¦å¤‰æ›ï¼‰
        return optimized_result
        
    except Exception as e:
        return f"âš ï¸ SQLæœ€é©åŒ–ã®ç”Ÿæˆä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(e)}"



def generate_top10_time_consuming_processes_report(extracted_metrics: Dict[str, Any], limit_nodes: int = 10) -> str:
    """
    æœ€ã‚‚æ™‚é–“ãŒã‹ã‹ã£ã¦ã„ã‚‹å‡¦ç†ã®ãƒ¬ãƒãƒ¼ãƒˆã‚’æ–‡å­—åˆ—ã¨ã—ã¦ç”Ÿæˆ
    
    ğŸš¨ é‡è¦: ãƒ‘ãƒ¼ã‚»ãƒ³ãƒ†ãƒ¼ã‚¸è¨ˆç®—ãƒ‡ã‚°ãƒ¬é˜²æ­¢
    - ä¸¦åˆ—å®Ÿè¡Œãƒãƒ¼ãƒ‰ã®æ™‚é–“åˆè¨ˆã‚’å…¨ä½“æ™‚é–“ã¨ã—ã¦ä½¿ç”¨ã™ã‚‹ã“ã¨ã¯çµ¶å¯¾ã«ç¦æ­¢
    - overall_metrics.total_time_msï¼ˆwall-clock timeï¼‰ã‚’å„ªå…ˆä½¿ç”¨
    - ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯æ™‚ã¯æœ€å¤§ãƒãƒ¼ãƒ‰æ™‚é–“ã‚’ä½¿ç”¨ï¼ˆåˆè¨ˆã§ã¯ãªã„ï¼‰
    
    Args:
        extracted_metrics: æŠ½å‡ºã•ã‚ŒãŸãƒ¡ãƒˆãƒªã‚¯ã‚¹
        limit_nodes: è¡¨ç¤ºã™ã‚‹ãƒãƒ¼ãƒ‰æ•°ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ10ã€ãƒ•ã‚¡ã‚¤ãƒ«å‡ºåŠ›æ™‚ã¯5ï¼‰
    
    Returns:
        str: å‡¦ç†ãƒ¬ãƒãƒ¼ãƒˆ
    """
    report_lines = []
    
    # ã‚¿ã‚¤ãƒˆãƒ«ã‚’ãƒãƒ¼ãƒ‰æ•°ã«å¿œã˜ã¦èª¿æ•´
    title = f"æœ€ã‚‚æ™‚é–“ãŒã‹ã‹ã£ã¦ã„ã‚‹å‡¦ç†TOP{limit_nodes}" if limit_nodes <= 10 else "æœ€ã‚‚æ™‚é–“ãŒã‹ã‹ã£ã¦ã„ã‚‹å‡¦ç†TOP10"
    report_lines.append(f"## ğŸŒ {title}")
    report_lines.append("=" * 80)
    report_lines.append("ğŸ“Š ã‚¢ã‚¤ã‚³ãƒ³èª¬æ˜: â±ï¸æ™‚é–“ ğŸ’¾ãƒ¡ãƒ¢ãƒª ğŸ”¥ğŸŒä¸¦åˆ—åº¦ ğŸ’¿ã‚¹ãƒ”ãƒ« âš–ï¸ã‚¹ã‚­ãƒ¥ãƒ¼")
    report_lines.append('ğŸ’¿ ã‚¹ãƒ”ãƒ«åˆ¤å®š: "Num bytes spilled to disk due to memory pressure" ã¾ãŸã¯ "Sink - Num bytes spilled to disk due to memory pressure" > 0')
    report_lines.append("ğŸ¯ ã‚¹ã‚­ãƒ¥ãƒ¼åˆ¤å®š: 'AQEShuffleRead - Number of skewed partitions' > 0")
    report_lines.append("")

    # ãƒãƒ¼ãƒ‰ã‚’å®Ÿè¡Œæ™‚é–“ã§ã‚½ãƒ¼ãƒˆ
    sorted_nodes = sorted(extracted_metrics['node_metrics'], 
                         key=lambda x: x['key_metrics'].get('durationMs', 0), 
                         reverse=True)
    
    # æŒ‡å®šã•ã‚ŒãŸãƒãƒ¼ãƒ‰æ•°ã¾ã§å‡¦ç†
    final_sorted_nodes = sorted_nodes[:limit_nodes]

    if final_sorted_nodes:
        # ğŸš¨ é‡è¦: æ­£ã—ã„å…¨ä½“æ™‚é–“ã®è¨ˆç®—ï¼ˆãƒ‡ã‚°ãƒ¬é˜²æ­¢ï¼‰
        # 1. overall_metricsã‹ã‚‰å…¨ä½“å®Ÿè¡Œæ™‚é–“ã‚’å–å¾—ï¼ˆwall-clock timeï¼‰
        overall_metrics = extracted_metrics.get('overall_metrics', {})
        total_duration = overall_metrics.get('total_time_ms', 0)
        
        # ğŸš¨ ä¸¦åˆ—å®Ÿè¡Œå•é¡Œã®ä¿®æ­£: task_total_time_msã‚’å„ªå…ˆä½¿ç”¨
        task_total_time_ms = overall_metrics.get('task_total_time_ms', 0)
        
        if task_total_time_ms > 0:
            total_duration = task_total_time_ms
            print(f"âœ… generate_top10ãƒ¬ãƒãƒ¼ãƒˆ: ä¸¦åˆ—å®Ÿè¡Œå¯¾å¿œ - task_total_time_msä½¿ç”¨: {total_duration:,} ms ({total_duration/3600000:.1f}æ™‚é–“)")
        elif total_duration <= 0:
            # execution_time_msã‚’æ¬¡ã®å„ªå…ˆåº¦ã§ä½¿ç”¨
            execution_time_ms = overall_metrics.get('execution_time_ms', 0)
            if execution_time_ms > 0:
                total_duration = execution_time_ms
                print(f"âš ï¸ generate_top10ãƒ¬ãƒãƒ¼ãƒˆ: task_total_time_msåˆ©ç”¨ä¸å¯ã€execution_time_msä½¿ç”¨: {total_duration} ms")
            else:
                # æœ€çµ‚ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
                max_node_time = max([node['key_metrics'].get('durationMs', 0) for node in sorted_nodes], default=1)
                total_duration = int(max_node_time * 1.2)
                print(f"âš ï¸ generate_top10ãƒ¬ãƒãƒ¼ãƒˆ: æœ€çµ‚ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ - æ¨å®šæ™‚é–“ä½¿ç”¨: {total_duration} ms")
        
        report_lines.append(f"ğŸ“Š ç´¯ç©ã‚¿ã‚¹ã‚¯å®Ÿè¡Œæ™‚é–“ï¼ˆä¸¦åˆ—ï¼‰: {total_duration:,} ms ({total_duration/3600000:.1f} æ™‚é–“)")
        report_lines.append(f"ğŸ“ˆ TOP{limit_nodes}åˆè¨ˆæ™‚é–“ï¼ˆä¸¦åˆ—å®Ÿè¡Œï¼‰: {sum(node['key_metrics'].get('durationMs', 0) for node in final_sorted_nodes):,} ms")

        report_lines.append("")
        
        for i, node in enumerate(final_sorted_nodes):
            # ãƒã‚°ä¿®æ­£ï¼šå¤‰æ•°ã‚’æ­£ã—ãå®šç¾©
            duration_ms = node['key_metrics'].get('durationMs', 0)
            rows_num = node['key_metrics'].get('numOutputRows', 0)
            memory_mb = node['key_metrics'].get('peakMemoryBytes', 0) / 1024 / 1024
            
            # ğŸš¨ é‡è¦: æ­£ã—ã„ãƒ‘ãƒ¼ã‚»ãƒ³ãƒ†ãƒ¼ã‚¸è¨ˆç®—ï¼ˆãƒ‡ã‚°ãƒ¬é˜²æ­¢ï¼‰
            # wall-clock timeã«å¯¾ã™ã‚‹å„ãƒãƒ¼ãƒ‰ã®å®Ÿè¡Œæ™‚é–“ã®å‰²åˆ
            time_percentage = min((duration_ms / max(total_duration, 1)) * 100, 100.0)
            
            # æ™‚é–“ã®é‡è¦åº¦ã«åŸºã¥ã„ã¦ã‚¢ã‚¤ã‚³ãƒ³ã‚’é¸æŠ
            if duration_ms >= 10000:  # 10ç§’ä»¥ä¸Š
                time_icon = "ğŸ”´"
                severity = "CRITICAL"
            elif duration_ms >= 5000:  # 5ç§’ä»¥ä¸Š
                time_icon = "ğŸŸ "
                severity = "HIGH"
            elif duration_ms >= 1000:  # 1ç§’ä»¥ä¸Š
                time_icon = "ğŸŸ¡"
                severity = "MEDIUM"
            else:
                time_icon = "ğŸŸ¢"
                severity = "LOW"
            
            # ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ã®ã‚¢ã‚¤ã‚³ãƒ³
            memory_icon = "ğŸ’š" if memory_mb < 100 else "âš ï¸" if memory_mb < 1000 else "ğŸš¨"
            
            # ã‚ˆã‚Šæ„å‘³ã®ã‚ã‚‹ãƒãƒ¼ãƒ‰åã‚’å–å¾—
            raw_node_name = node['name']
            node_name = get_meaningful_node_name(node, extracted_metrics)
            short_name = node_name[:100] + "..." if len(node_name) > 100 else node_name
            
            # ä¸¦åˆ—åº¦æƒ…å ±ã®å–å¾—ï¼ˆä¿®æ­£ç‰ˆ: è¤‡æ•°ã®Tasks totalãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚’å–å¾—ï¼‰
            parallelism_data = extract_parallelism_metrics(node)
            
            # å¾“æ¥ã®å˜ä¸€å€¤ï¼ˆäº’æ›æ€§ã®ãŸã‚ï¼‰
            num_tasks = parallelism_data.get('tasks_total', 0)
            
            # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: Sink - Tasks totalã¾ãŸã¯Source - Tasks totalãŒã‚ã‚‹å ´åˆ
            if num_tasks == 0:
                if parallelism_data.get('sink_tasks_total', 0) > 0:
                    num_tasks = parallelism_data.get('sink_tasks_total', 0)
                elif parallelism_data.get('source_tasks_total', 0) > 0:
                    num_tasks = parallelism_data.get('source_tasks_total', 0)
            
            # ã‚¹ãƒ”ãƒ«æ¤œå‡ºï¼ˆã‚»ãƒ«33ã¨åŒã˜ãƒ­ã‚¸ãƒƒã‚¯ - æ­£ç¢ºãªãƒ¡ãƒˆãƒªã‚¯ã‚¹åã®ã¿ï¼‰
            spill_detected = False
            spill_bytes = 0
            exact_spill_metrics = [
                "Num bytes spilled to disk due to memory pressure",
                "Sink - Num bytes spilled to disk due to memory pressure",
                "Sink/Num bytes spilled to disk due to memory pressure"
            ]
            
            # detailed_metricsã‹ã‚‰æ¤œç´¢
            detailed_metrics = node.get('detailed_metrics', {})
            for metric_key, metric_info in detailed_metrics.items():
                metric_value = metric_info.get('value', 0)
                metric_label = metric_info.get('label', '')
                
                if (metric_key in exact_spill_metrics or metric_label in exact_spill_metrics) and metric_value > 0:
                    spill_detected = True
                    spill_bytes = max(spill_bytes, metric_value)
                    break
            
            # raw_metricsã‹ã‚‰æ¤œç´¢ï¼ˆãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼‰
            if not spill_detected:
                raw_metrics = node.get('metrics', [])
                for metric in raw_metrics:
                    metric_key = metric.get('key', '')
                    metric_label = metric.get('label', '')
                    metric_value = metric.get('value', 0)
                    
                    if (metric_key in exact_spill_metrics or metric_label in exact_spill_metrics) and metric_value > 0:
                        spill_detected = True
                        spill_bytes = max(spill_bytes, metric_value)
                        break
            
            # ã‚¹ã‚­ãƒ¥ãƒ¼æ¤œå‡º: AQEShuffleRead - Number of skewed partitions ãƒ¡ãƒˆãƒªã‚¯ã‚¹ä½¿ç”¨ï¼ˆæ­£ç¢ºãªãƒ¡ãƒˆãƒªã‚¯ã‚¹åã®ã¿ï¼‰
            skew_detected = False
            skewed_partitions = 0
            target_skew_metric = "AQEShuffleRead - Number of skewed partitions"
            
            # detailed_metricsã‹ã‚‰æ­£ç¢ºãªãƒ¡ãƒˆãƒªã‚¯ã‚¹åã§æ¤œç´¢
            detailed_metrics = node.get('detailed_metrics', {})
            for metric_key, metric_info in detailed_metrics.items():
                if metric_key == target_skew_metric:
                    try:
                        skewed_partitions = int(metric_info.get('value', 0))
                        if skewed_partitions > 0:
                            skew_detected = True
                        break
                    except (ValueError, TypeError):
                        continue
            
            # key_metricsã‹ã‚‰æ­£ç¢ºãªãƒ¡ãƒˆãƒªã‚¯ã‚¹åã§æ¤œç´¢ï¼ˆãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼‰
            if not skew_detected:
                key_metrics = node.get('key_metrics', {})
                if target_skew_metric in key_metrics:
                    try:
                        skewed_partitions = int(key_metrics[target_skew_metric])
                        if skewed_partitions > 0:
                            skew_detected = True
                    except (ValueError, TypeError):
                        pass
            
            # ä¸¦åˆ—åº¦ã‚¢ã‚¤ã‚³ãƒ³
            parallelism_icon = "ğŸ”¥" if num_tasks >= 10 else "âš ï¸" if num_tasks >= 5 else "ğŸŒ"
            # ã‚¹ãƒ”ãƒ«ã‚¢ã‚¤ã‚³ãƒ³
            spill_icon = "ğŸ’¿" if spill_detected else "âœ…"
            # ã‚¹ã‚­ãƒ¥ãƒ¼ã‚¢ã‚¤ã‚³ãƒ³
            skew_icon = "âš–ï¸" if skew_detected else "âœ…"
            
            report_lines.append(f"{i+1:2d}. {time_icon}{memory_icon}{parallelism_icon}{spill_icon}{skew_icon} [{severity:8}] {short_name}")
            report_lines.append(f"    â±ï¸  å®Ÿè¡Œæ™‚é–“: {duration_ms:>8,} ms ({duration_ms/1000:>6.1f} sec) - ç´¯ç©æ™‚é–“ã® {time_percentage:>5.1f}%")
            report_lines.append(f"    ğŸ“Š å‡¦ç†è¡Œæ•°: {rows_num:>8,} è¡Œ")
            report_lines.append(f"    ğŸ’¾ ãƒ”ãƒ¼ã‚¯ãƒ¡ãƒ¢ãƒª: {memory_mb:>6.1f} MB")
            # è¤‡æ•°ã®Tasks totalãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚’è¡¨ç¤º
            parallelism_display = []
            for task_metric in parallelism_data.get('all_tasks_metrics', []):
                parallelism_display.append(f"{task_metric['name']}: {task_metric['value']}")
            
            if parallelism_display:
                report_lines.append(f"    ğŸ”§ ä¸¦åˆ—åº¦: {' | '.join(parallelism_display)}")
            else:
                report_lines.append(f"    ğŸ”§ ä¸¦åˆ—åº¦: {num_tasks:>3d} ã‚¿ã‚¹ã‚¯")
            
            # ã‚¹ã‚­ãƒ¥ãƒ¼åˆ¤å®šï¼ˆAQEã‚¹ã‚­ãƒ¥ãƒ¼æ¤œå‡ºã¨AQEShuffleReadå¹³å‡ãƒ‘ãƒ¼ãƒ†ã‚£ã‚·ãƒ§ãƒ³ã‚µã‚¤ã‚ºã®ä¸¡æ–¹ã‚’è€ƒæ…®ï¼‰
            aqe_shuffle_skew_warning = parallelism_data.get('aqe_shuffle_skew_warning', False)
            
            if skew_detected:
                skew_status = "AQEã§æ¤œå‡ºãƒ»å¯¾å¿œæ¸ˆ"
            elif aqe_shuffle_skew_warning:
                skew_status = "æ½œåœ¨çš„ãªã‚¹ã‚­ãƒ¥ãƒ¼ã®å¯èƒ½æ€§ã‚ã‚Š"
            else:
                skew_status = "ãªã—"
            
            report_lines.append(f"    ğŸ’¿ ã‚¹ãƒ”ãƒ«: {'ã‚ã‚Š' if spill_detected else 'ãªã—'} | âš–ï¸ ã‚¹ã‚­ãƒ¥ãƒ¼: {skew_status}")
            
            # AQEShuffleReadãƒ¡ãƒˆãƒªã‚¯ã‚¹ã®è¡¨ç¤º
            aqe_shuffle_metrics = parallelism_data.get('aqe_shuffle_metrics', [])
            if aqe_shuffle_metrics:
                aqe_display = []
                for aqe_metric in aqe_shuffle_metrics:
                    if aqe_metric['name'] == "AQEShuffleRead - Number of partitions":
                        aqe_display.append(f"ãƒ‘ãƒ¼ãƒ†ã‚£ã‚·ãƒ§ãƒ³æ•°: {aqe_metric['value']}")
                    elif aqe_metric['name'] == "AQEShuffleRead - Partition data size":
                        aqe_display.append(f"ãƒ‡ãƒ¼ã‚¿ã‚µã‚¤ã‚º: {aqe_metric['value']:,} bytes")
                
                if aqe_display:
                    report_lines.append(f"    ğŸ”„ AQEShuffleRead: {' | '.join(aqe_display)}")
                    
                    # å¹³å‡ãƒ‘ãƒ¼ãƒ†ã‚£ã‚·ãƒ§ãƒ³ã‚µã‚¤ã‚ºã¨è­¦å‘Šè¡¨ç¤º
                    avg_partition_size = parallelism_data.get('aqe_shuffle_avg_partition_size', 0)
                    if avg_partition_size > 0:
                        avg_size_mb = avg_partition_size / (1024 * 1024)
                        report_lines.append(f"    ğŸ“Š å¹³å‡ãƒ‘ãƒ¼ãƒ†ã‚£ã‚·ãƒ§ãƒ³ã‚µã‚¤ã‚º: {avg_size_mb:.2f} MB")
                        
                        # 512MBä»¥ä¸Šã®å ´åˆã«è­¦å‘Š
                        if parallelism_data.get('aqe_shuffle_skew_warning', False):
                            report_lines.append(f"    âš ï¸  ã€è­¦å‘Šã€‘ å¹³å‡ãƒ‘ãƒ¼ãƒ†ã‚£ã‚·ãƒ§ãƒ³ã‚µã‚¤ã‚ºãŒ512MBä»¥ä¸Š - æ½œåœ¨çš„ãªã‚¹ã‚­ãƒ¥ãƒ¼ã®å¯èƒ½æ€§ã‚ã‚Š")
            
            # åŠ¹ç‡æ€§æŒ‡æ¨™ï¼ˆè¡Œ/ç§’ï¼‰ã‚’è¨ˆç®—
            if duration_ms > 0:
                rows_per_sec = (rows_num * 1000) / duration_ms
                report_lines.append(f"    ğŸš€ å‡¦ç†åŠ¹ç‡: {rows_per_sec:>8,.0f} è¡Œ/ç§’")
            
            # ãƒ•ã‚£ãƒ«ã‚¿ç‡è¡¨ç¤ºï¼ˆãƒ‡ãƒãƒƒã‚°æ©Ÿèƒ½ä»˜ãï¼‰
            filter_result = calculate_filter_rate(node)
            filter_display = format_filter_rate_display(filter_result)
            if filter_display:
                report_lines.append(f"    {filter_display}")
            else:
                # ãƒ‡ãƒãƒƒã‚°æƒ…å ±ï¼šãªãœãƒ•ã‚£ãƒ«ã‚¿ç‡ãŒè¡¨ç¤ºã•ã‚Œãªã„ã‹ã‚’ç¢ºèª
                if filter_result["has_filter_metrics"]:
                    report_lines.append(f"    ğŸ“‚ ãƒ•ã‚£ãƒ«ã‚¿ç‡: {filter_result['filter_rate']:.1%} (èª­ã¿è¾¼ã¿: {filter_result['files_read_bytes']/(1024*1024*1024):.2f}GB, ãƒ—ãƒ«ãƒ¼ãƒ³: {filter_result['files_pruned_bytes']/(1024*1024*1024):.2f}GB)")
                else:
                    # ãƒ¡ãƒˆãƒªã‚¯ã‚¹æ¤œç´¢ã®ãƒ‡ãƒãƒƒã‚°
                    debug_info = []
                    detailed_metrics = node.get('detailed_metrics', {})
                    for metric_key, metric_info in detailed_metrics.items():
                        metric_label = metric_info.get('label', '')
                        if 'file' in metric_label.lower() and ('read' in metric_label.lower() or 'prun' in metric_label.lower()):
                            debug_info.append(f"{metric_label}: {metric_info.get('value', 0)}")
                    
                    if debug_info:
                        report_lines.append(f"    ğŸ“‚ ãƒ•ã‚£ãƒ«ã‚¿é–¢é€£ãƒ¡ãƒˆãƒªã‚¯ã‚¹æ¤œå‡º: {', '.join(debug_info[:2])}")
            
            # ã‚¹ãƒ”ãƒ«è©³ç´°æƒ…å ±ï¼ˆã‚·ãƒ³ãƒ—ãƒ«è¡¨ç¤ºï¼‰
            spill_display = ""
            if spill_detected and spill_bytes > 0:
                spill_mb = spill_bytes / 1024 / 1024
                if spill_mb >= 1024:  # GBå˜ä½
                    spill_display = f"{spill_mb/1024:.2f} GB"
                else:  # MBå˜ä½
                    spill_display = f"{spill_mb:.1f} MB"
                report_lines.append(f"    ğŸ’¿ ã‚¹ãƒ”ãƒ«: {spill_display}")
            
            # Shuffleãƒãƒ¼ãƒ‰ã®å ´åˆã¯å¸¸ã«Shuffle attributesã‚’è¡¨ç¤º
            if "shuffle" in raw_node_name.lower():
                shuffle_attributes = extract_shuffle_attributes(node)
                if shuffle_attributes:
                    report_lines.append(f"    ğŸ”„ Shuffleå±æ€§: {', '.join(shuffle_attributes)}")
                    
                    # REPARTITIONãƒ’ãƒ³ãƒˆã®ææ¡ˆï¼ˆã‚¹ãƒ”ãƒ«ãŒæ¤œå‡ºã•ã‚ŒãŸå ´åˆã®ã¿ï¼‰
                    if spill_detected and spill_bytes > 0 and spill_display:
                        suggested_partitions = max(num_tasks * 2, 200)  # æœ€å°200ãƒ‘ãƒ¼ãƒ†ã‚£ã‚·ãƒ§ãƒ³
                        
                        # Shuffleå±æ€§ã§æ¤œå‡ºã•ã‚ŒãŸã‚«ãƒ©ãƒ ã‚’å…¨ã¦ä½¿ç”¨ï¼ˆå®Œå…¨ä¸€è‡´ï¼‰
                        repartition_columns = ", ".join(shuffle_attributes)
                        
                        report_lines.append(f"    ğŸ’¡ æœ€é©åŒ–ææ¡ˆ: REPARTITION({suggested_partitions}, {repartition_columns})")
                        report_lines.append(f"       ç†ç”±: ã‚¹ãƒ”ãƒ«({spill_display})ã‚’æ”¹å–„ã™ã‚‹ãŸã‚")
                        report_lines.append(f"       å¯¾è±¡: Shuffleå±æ€§å…¨{len(shuffle_attributes)}ã‚«ãƒ©ãƒ ã‚’å®Œå…¨ä½¿ç”¨")
                else:
                    report_lines.append(f"    ğŸ”„ Shuffleå±æ€§: è¨­å®šãªã—")
            
            # ã‚¹ã‚­ãƒ£ãƒ³ãƒãƒ¼ãƒ‰ã®å ´åˆã¯ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°ã‚­ãƒ¼ã‚’è¡¨ç¤º
            if "scan" in raw_node_name.lower():
                cluster_attributes = extract_cluster_attributes(node)
                if cluster_attributes:
                    report_lines.append(f"    ğŸ“Š ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°ã‚­ãƒ¼: {', '.join(cluster_attributes)}")
                else:
                    report_lines.append(f"    ğŸ“Š ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°ã‚­ãƒ¼: è¨­å®šãªã—")
            
            # ã‚¹ã‚­ãƒ¥ãƒ¼è©³ç´°æƒ…å ±
            if skew_detected and skewed_partitions > 0:
                report_lines.append(f"    âš–ï¸ ã‚¹ã‚­ãƒ¥ãƒ¼è©³ç´°: {skewed_partitions} å€‹ã®ã‚¹ã‚­ãƒ¥ãƒ¼ãƒ‘ãƒ¼ãƒ†ã‚£ã‚·ãƒ§ãƒ³ï¼ˆAQEShuffleReadæ¤œå‡ºï¼‰")
            
            # ãƒãƒ¼ãƒ‰IDã‚‚è¡¨ç¤º
            report_lines.append(f"    ğŸ†” ãƒãƒ¼ãƒ‰ID: {node.get('node_id', node.get('id', 'N/A'))}")
            report_lines.append("")
            
    else:
        report_lines.append("âš ï¸ ãƒãƒ¼ãƒ‰ãƒ¡ãƒˆãƒªã‚¯ã‚¹ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ")
    
    return "\n".join(report_lines)

def save_execution_plan_analysis(plan_info: Dict[str, Any], output_dir: str = "/tmp") -> Dict[str, str]:
    """
    å®Ÿè¡Œãƒ—ãƒ©ãƒ³åˆ†æçµæœã‚’ãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜
    
    Args:
        plan_info: extract_execution_plan_info()ã®çµæœ
        output_dir: å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª
        
    Returns:
        Dict: ä¿å­˜ã•ã‚ŒãŸãƒ•ã‚¡ã‚¤ãƒ«åã®è¾æ›¸
    """
    from datetime import datetime
    import json
    
    # ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—ç”Ÿæˆ
    timestamp = datetime.now().strftime("%Y%m%d-%H%M%S")
    
    # ãƒ•ã‚¡ã‚¤ãƒ«åå®šç¾©
    plan_json_filename = f"output_execution_plan_analysis_{timestamp}.json"
    plan_report_filename = f"output_execution_plan_report_{timestamp}.md"
    
    # JSONå½¢å¼ã§ãƒ—ãƒ©ãƒ³æƒ…å ±ã‚’ä¿å­˜
    with open(plan_json_filename, 'w', encoding='utf-8') as f:
        json.dump(plan_info, f, ensure_ascii=False, indent=2)
    
    # Markdownå½¢å¼ã§ãƒ—ãƒ©ãƒ³åˆ†æãƒ¬ãƒãƒ¼ãƒˆã‚’ä¿å­˜
    with open(plan_report_filename, 'w', encoding='utf-8') as f:
        report_content = generate_execution_plan_markdown_report(plan_info)
        f.write(report_content)
    
    return {
        'plan_json_file': plan_json_filename,
        'plan_report_file': plan_report_filename
    }

def generate_execution_plan_markdown_report(plan_info: Dict[str, Any]) -> str:
    """
    å®Ÿè¡Œãƒ—ãƒ©ãƒ³åˆ†æçµæœã®Markdownãƒ¬ãƒãƒ¼ãƒˆã‚’ç”Ÿæˆ
    
    Args:
        plan_info: extract_execution_plan_info()ã®çµæœ
        
    Returns:
        str: Markdownãƒ¬ãƒãƒ¼ãƒˆ
    """
    if OUTPUT_LANGUAGE == 'ja':
        return generate_execution_plan_markdown_report_ja(plan_info)
    else:
        return generate_execution_plan_markdown_report_en(plan_info)

def generate_execution_plan_markdown_report_ja(plan_info: Dict[str, Any]) -> str:
    """
    å®Ÿè¡Œãƒ—ãƒ©ãƒ³åˆ†æçµæœã®Markdownãƒ¬ãƒãƒ¼ãƒˆï¼ˆæ—¥æœ¬èªç‰ˆï¼‰
    """
    from datetime import datetime
    
    lines = []
    lines.append("# Databricks SQLå®Ÿè¡Œãƒ—ãƒ©ãƒ³åˆ†æãƒ¬ãƒãƒ¼ãƒˆ")
    lines.append("")
    lines.append(f"**ç”Ÿæˆæ—¥æ™‚**: {datetime.now().strftime('%Yå¹´%mæœˆ%dæ—¥ %H:%M:%S')}")
    lines.append("")
    
    # ãƒ—ãƒ©ãƒ³ã‚µãƒãƒªãƒ¼
    plan_summary = plan_info.get("plan_summary", {})
    lines.append("## ğŸ“Š å®Ÿè¡Œãƒ—ãƒ©ãƒ³ã‚µãƒãƒªãƒ¼")
    lines.append("")
    lines.append(f"- **ç·ãƒãƒ¼ãƒ‰æ•°**: {plan_summary.get('total_nodes', 0)}")
    lines.append(f"- **BROADCASTãƒãƒ¼ãƒ‰æ•°**: {plan_summary.get('broadcast_nodes_count', 0)}")
    lines.append(f"- **JOINãƒãƒ¼ãƒ‰æ•°**: {plan_summary.get('join_nodes_count', 0)}")
    lines.append(f"- **ã‚¹ã‚­ãƒ£ãƒ³ãƒãƒ¼ãƒ‰æ•°**: {plan_summary.get('scan_nodes_count', 0)}")
    lines.append(f"- **ã‚·ãƒ£ãƒƒãƒ•ãƒ«ãƒãƒ¼ãƒ‰æ•°**: {plan_summary.get('shuffle_nodes_count', 0)}")
    lines.append(f"- **é›†ç´„ãƒãƒ¼ãƒ‰æ•°**: {plan_summary.get('aggregate_nodes_count', 0)}")
    lines.append(f"- **BROADCASTãŒä½¿ç”¨ä¸­**: {'ã¯ã„' if plan_summary.get('has_broadcast_joins', False) else 'ã„ã„ãˆ'}")
    lines.append(f"- **ã‚¹ã‚­ãƒ£ãƒ³ã•ã‚Œã‚‹ãƒ†ãƒ¼ãƒ–ãƒ«æ•°**: {plan_summary.get('tables_scanned', 0)}")
    lines.append("")
    
    # JOINæˆ¦ç•¥åˆ†æ
    unique_join_strategies = plan_summary.get('unique_join_strategies', [])
    if unique_join_strategies:
        lines.append("## ğŸ”— JOINæˆ¦ç•¥åˆ†æ")
        lines.append("")
        for strategy in unique_join_strategies:
            strategy_jp = {
                'broadcast_hash_join': 'ãƒ–ãƒ­ãƒ¼ãƒ‰ã‚­ãƒ£ã‚¹ãƒˆãƒãƒƒã‚·ãƒ¥JOIN',
                'sort_merge_join': 'ã‚½ãƒ¼ãƒˆãƒãƒ¼ã‚¸JOIN',
                'shuffle_hash_join': 'ã‚·ãƒ£ãƒƒãƒ•ãƒ«ãƒãƒƒã‚·ãƒ¥JOIN',
                'broadcast_nested_loop_join': 'ãƒ–ãƒ­ãƒ¼ãƒ‰ã‚­ãƒ£ã‚¹ãƒˆãƒã‚¹ãƒˆãƒ«ãƒ¼ãƒ—JOIN'
            }.get(strategy, strategy)
            lines.append(f"- **{strategy_jp}** (`{strategy}`)")
        lines.append("")
    
    # BROADCASTãƒãƒ¼ãƒ‰è©³ç´°
    broadcast_nodes = plan_info.get("broadcast_nodes", [])
    if broadcast_nodes:
        lines.append("## ğŸ“¡ BROADCASTãƒãƒ¼ãƒ‰è©³ç´°")
        lines.append("")
        for i, node in enumerate(broadcast_nodes, 1):
            lines.append(f"### {i}. {node['node_name']}")
            lines.append("")
            lines.append(f"- **ãƒãƒ¼ãƒ‰ID**: {node['node_id']}")
            lines.append(f"- **ãƒãƒ¼ãƒ‰ã‚¿ã‚°**: {node['node_tag']}")
            
            metadata = node.get('metadata', [])
            if metadata:
                lines.append("- **é–¢é€£ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿**:")
                for meta in metadata[:5]:  # æœ€å¤§5å€‹ã¾ã§è¡¨ç¤º
                    key = meta.get('key', '')
                    value = meta.get('value', '')
                    values = meta.get('values', [])
                    if values:
                        lines.append(f"  - **{key}**: {', '.join(map(str, values[:3]))}")
                    elif value:
                        lines.append(f"  - **{key}**: {value}")
            lines.append("")
    
    # JOINãƒãƒ¼ãƒ‰è©³ç´°
    join_nodes = plan_info.get("join_nodes", [])
    if join_nodes:
        lines.append("## ğŸ”— JOINãƒãƒ¼ãƒ‰è©³ç´°")
        lines.append("")
        for i, node in enumerate(join_nodes, 1):
            lines.append(f"### {i}. {node['node_name']}")
            lines.append("")
            lines.append(f"- **ãƒãƒ¼ãƒ‰ID**: {node['node_id']}")
            lines.append(f"- **JOINæˆ¦ç•¥**: {node['join_strategy']}")
            lines.append(f"- **JOINã‚¿ã‚¤ãƒ—**: {node['join_type']}")
            
            join_keys = node.get('join_keys', [])
            if join_keys:
                lines.append(f"- **JOINã‚­ãƒ¼**: {', '.join(join_keys[:5])}")
            lines.append("")
    
    # ãƒ†ãƒ¼ãƒ–ãƒ«ã‚¹ã‚­ãƒ£ãƒ³è©³ç´°ï¼ˆã‚µã‚¤ã‚ºæ¨å®šæƒ…å ±ã‚’å«ã‚€ï¼‰
    table_scan_details = plan_info.get("table_scan_details", {})
    table_size_estimates = plan_info.get("table_size_estimates", {})
    if table_scan_details:
        lines.append("## ğŸ“‹ ãƒ†ãƒ¼ãƒ–ãƒ«ã‚¹ã‚­ãƒ£ãƒ³è©³ç´°")
        lines.append("")
        for table_name, scan_detail in table_scan_details.items():
            lines.append(f"### {table_name}")
            lines.append("")
            lines.append(f"- **ãƒ•ã‚¡ã‚¤ãƒ«å½¢å¼**: {scan_detail.get('file_format', 'unknown')}")
            lines.append(f"- **ãƒ—ãƒƒã‚·ãƒ¥ãƒ€ã‚¦ãƒ³ãƒ•ã‚£ãƒ«ã‚¿æ•°**: {len(scan_detail.get('pushed_filters', []))}")
            lines.append(f"- **å‡ºåŠ›ã‚«ãƒ©ãƒ æ•°**: {len(scan_detail.get('output_columns', []))}")
            
            # å®Ÿè¡Œãƒ—ãƒ©ãƒ³ã‹ã‚‰ã®ã‚µã‚¤ã‚ºæ¨å®šæƒ…å ±ï¼ˆestimatedSizeInBytesåˆ©ç”¨ä¸å¯ã®ãŸã‚ç„¡åŠ¹åŒ–ï¼‰
            # size_info = table_size_estimates.get(table_name)
            # if size_info:
            #     lines.append(f"- **æ¨å®šã‚µã‚¤ã‚ºï¼ˆå®Ÿè¡Œãƒ—ãƒ©ãƒ³ï¼‰**: {size_info['estimated_size_mb']:.1f}MB")
            #     lines.append(f"- **ã‚µã‚¤ã‚ºæ¨å®šä¿¡é ¼åº¦**: {size_info.get('confidence', 'medium')}")
            #     if 'num_files' in size_info:
            #         lines.append(f"- **ãƒ•ã‚¡ã‚¤ãƒ«æ•°**: {size_info['num_files']}")
            #     if 'num_partitions' in size_info:
            #         lines.append(f"- **ãƒ‘ãƒ¼ãƒ†ã‚£ã‚·ãƒ§ãƒ³æ•°**: {size_info['num_partitions']}")
            
            pushed_filters = scan_detail.get('pushed_filters', [])
            if pushed_filters:
                lines.append("- **ãƒ—ãƒƒã‚·ãƒ¥ãƒ€ã‚¦ãƒ³ãƒ•ã‚£ãƒ«ã‚¿**:")
                for filter_expr in pushed_filters[:3]:  # æœ€å¤§3å€‹ã¾ã§è¡¨ç¤º
                    lines.append(f"  - `{filter_expr}`")
            lines.append("")
    
    # ã‚·ãƒ£ãƒƒãƒ•ãƒ«ãƒãƒ¼ãƒ‰è©³ç´°
    shuffle_nodes = plan_info.get("shuffle_nodes", [])
    if shuffle_nodes:
        lines.append("## ğŸ”„ ã‚·ãƒ£ãƒƒãƒ•ãƒ«ãƒãƒ¼ãƒ‰è©³ç´°")
        lines.append("")
        for i, node in enumerate(shuffle_nodes, 1):
            lines.append(f"### {i}. {node['node_name']}")
            lines.append("")
            lines.append(f"- **ãƒãƒ¼ãƒ‰ID**: {node['node_id']}")
            
            partition_keys = node.get('partition_keys', [])
            if partition_keys:
                lines.append(f"- **ãƒ‘ãƒ¼ãƒ†ã‚£ã‚·ãƒ§ãƒ³ã‚­ãƒ¼**: {', '.join(partition_keys)}")
            lines.append("")
    
    # é›†ç´„ãƒãƒ¼ãƒ‰è©³ç´°
    aggregate_nodes = plan_info.get("aggregate_nodes", [])
    if aggregate_nodes:
        lines.append("## ğŸ“Š é›†ç´„ãƒãƒ¼ãƒ‰è©³ç´°")
        lines.append("")
        for i, node in enumerate(aggregate_nodes, 1):
            lines.append(f"### {i}. {node['node_name']}")
            lines.append("")
            lines.append(f"- **ãƒãƒ¼ãƒ‰ID**: {node['node_id']}")
            
            group_keys = node.get('group_keys', [])
            if group_keys:
                lines.append(f"- **ã‚°ãƒ«ãƒ¼ãƒ—åŒ–ã‚­ãƒ¼**: {', '.join(group_keys[:5])}")
            
            agg_expressions = node.get('aggregate_expressions', [])
            if agg_expressions:
                lines.append(f"- **é›†ç´„é–¢æ•°**: {', '.join(agg_expressions[:5])}")
            lines.append("")
    
    # ãƒ†ãƒ¼ãƒ–ãƒ«ã‚µã‚¤ã‚ºæ¨å®šæƒ…å ±ã‚µãƒãƒªãƒ¼ï¼ˆestimatedSizeInBytesåˆ©ç”¨ä¸å¯ã®ãŸã‚ç„¡åŠ¹åŒ–ï¼‰
    # table_size_estimates = plan_info.get("table_size_estimates", {})
    # if table_size_estimates:
    #     lines.append("## ğŸ“ ãƒ†ãƒ¼ãƒ–ãƒ«ã‚µã‚¤ã‚ºæ¨å®šæƒ…å ±ï¼ˆå®Ÿè¡Œãƒ—ãƒ©ãƒ³ãƒ™ãƒ¼ã‚¹ï¼‰")
    #     lines.append("")
    #     total_estimated_size = sum(size_info['estimated_size_mb'] for size_info in table_size_estimates.values())
    #     lines.append(f"- **æ¨å®šå¯¾è±¡ãƒ†ãƒ¼ãƒ–ãƒ«æ•°**: {len(table_size_estimates)}")
    #     lines.append(f"- **ç·æ¨å®šã‚µã‚¤ã‚º**: {total_estimated_size:.1f}MB")
    #     lines.append("")
    #     
    #     for table_name, size_info in list(table_size_estimates.items())[:5]:  # æœ€å¤§5ãƒ†ãƒ¼ãƒ–ãƒ«è¡¨ç¤º
    #         lines.append(f"### {table_name}")
    #         lines.append(f"- **æ¨å®šã‚µã‚¤ã‚º**: {size_info['estimated_size_mb']:.1f}MB")
    #         lines.append(f"- **ä¿¡é ¼åº¦**: {size_info.get('confidence', 'medium')}")
    #         lines.append(f"- **ãƒãƒ¼ãƒ‰**: {size_info.get('node_name', 'unknown')}")
    #         if 'num_files' in size_info:
    #             lines.append(f"- **ãƒ•ã‚¡ã‚¤ãƒ«æ•°**: {size_info['num_files']}")
    #         lines.append("")
    #     
    #     if len(table_size_estimates) > 5:
    #         lines.append(f"...ä»– {len(table_size_estimates) - 5} ãƒ†ãƒ¼ãƒ–ãƒ«ï¼ˆè©³ç´°ã¯ä¸Šè¨˜ã‚»ã‚¯ã‚·ãƒ§ãƒ³å‚ç…§ï¼‰")
    #         lines.append("")
    
    # æœ€é©åŒ–æ¨å¥¨äº‹é …
    lines.append("## ğŸ’¡ ãƒ—ãƒ©ãƒ³ãƒ™ãƒ¼ã‚¹æœ€é©åŒ–æ¨å¥¨äº‹é …")
    lines.append("")
    
    if plan_summary.get('has_broadcast_joins', False):
        lines.append("âœ… **æ—¢ã«BROADCAST JOINãŒé©ç”¨ã•ã‚Œã¦ã„ã¾ã™**")
        lines.append("- ç¾åœ¨ã®å®Ÿè¡Œãƒ—ãƒ©ãƒ³ã§BROADCASTæœ€é©åŒ–ãŒæœ‰åŠ¹")
        
        # BROADCASTã•ã‚Œã¦ã„ã‚‹ãƒ†ãƒ¼ãƒ–ãƒ«ä¸€è¦§ã‚’è¡¨ç¤º
        broadcast_tables = plan_summary.get('broadcast_tables', [])
        if broadcast_tables:
            lines.append(f"- **BROADCASTã•ã‚Œã¦ã„ã‚‹ãƒ†ãƒ¼ãƒ–ãƒ«**: {', '.join(broadcast_tables)}")
        
        lines.append("- è¿½åŠ ã®BROADCASTé©ç”¨æ©Ÿä¼šã‚’ç¢ºèªã—ã¦ãã ã•ã„")
    else:
        lines.append("âš ï¸ **BROADCAST JOINãŒæœªé©ç”¨ã§ã™**")
        lines.append("- å°ãƒ†ãƒ¼ãƒ–ãƒ«ã«BROADCASTãƒ’ãƒ³ãƒˆã®é©ç”¨ã‚’æ¤œè¨")
        lines.append("- 30MBé–¾å€¤ä»¥ä¸‹ã®ãƒ†ãƒ¼ãƒ–ãƒ«ã‚’ç‰¹å®šã—ã¦ãã ã•ã„")
    lines.append("")
    
    if plan_summary.get('shuffle_nodes_count', 0) > 3:
        lines.append("âš ï¸ **å¤šæ•°ã®ã‚·ãƒ£ãƒƒãƒ•ãƒ«æ“ä½œãŒæ¤œå‡ºã•ã‚Œã¾ã—ãŸ**")
        lines.append("- ãƒ‡ãƒ¼ã‚¿ã®åˆ†æ•£ã¨ãƒ‘ãƒ¼ãƒ†ã‚£ã‚·ãƒ§ãƒ‹ãƒ³ã‚°æˆ¦ç•¥ã‚’è¦‹ç›´ã—")
        lines.append("- Liquid Clusteringã®é©ç”¨ã‚’æ¤œè¨")
    lines.append("")
    
    # ã‚µã‚¤ã‚ºæ¨å®šãƒ™ãƒ¼ã‚¹ã®æœ€é©åŒ–ææ¡ˆï¼ˆestimatedSizeInBytesåˆ©ç”¨ä¸å¯ã®ãŸã‚ç„¡åŠ¹åŒ–ï¼‰
    # table_size_estimates = plan_info.get("table_size_estimates", {})
    # if table_size_estimates:
    #     small_tables = [name for name, info in table_size_estimates.items() if info['estimated_size_mb'] <= 30]
    #     if small_tables:
    #         lines.append("ğŸ’¡ **å®Ÿè¡Œãƒ—ãƒ©ãƒ³ãƒ™ãƒ¼ã‚¹BROADCASTæ¨å¥¨**")
    #         lines.append(f"- 30MBä»¥ä¸‹ã®å°ãƒ†ãƒ¼ãƒ–ãƒ«: {len(small_tables)}å€‹æ¤œå‡º")
    #         for table in small_tables[:3]:  # æœ€å¤§3å€‹è¡¨ç¤º
    #             size_mb = table_size_estimates[table]['estimated_size_mb']
    #             lines.append(f"  â€¢ {table}: {size_mb:.1f}MBï¼ˆBROADCASTå€™è£œï¼‰")
    #         if len(small_tables) > 3:
    #             lines.append(f"  â€¢ ...ä»– {len(small_tables) - 3} ãƒ†ãƒ¼ãƒ–ãƒ«")
    #         lines.append("")
    
    lines.append("---")
    lines.append("")
    lines.append("ã“ã®ãƒ¬ãƒãƒ¼ãƒˆã¯ã€Databricks SQLå®Ÿè¡Œãƒ—ãƒ©ãƒ³åˆ†æãƒ„ãƒ¼ãƒ«ã«ã‚ˆã£ã¦è‡ªå‹•ç”Ÿæˆã•ã‚Œã¾ã—ãŸã€‚")
    
    return '\n'.join(lines)

def generate_execution_plan_markdown_report_en(plan_info: Dict[str, Any]) -> str:
    """
    å®Ÿè¡Œãƒ—ãƒ©ãƒ³åˆ†æçµæœã®Markdownãƒ¬ãƒãƒ¼ãƒˆï¼ˆè‹±èªç‰ˆï¼‰
    """
    from datetime import datetime
    
    lines = []
    lines.append("# Databricks SQL Execution Plan Analysis Report")
    lines.append("")
    lines.append(f"**Generated**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    lines.append("")
    
    # Plan Summary
    plan_summary = plan_info.get("plan_summary", {})
    lines.append("## ğŸ“Š Execution Plan Summary")
    lines.append("")
    lines.append(f"- **Total Nodes**: {plan_summary.get('total_nodes', 0)}")
    lines.append(f"- **BROADCAST Nodes**: {plan_summary.get('broadcast_nodes_count', 0)}")
    lines.append(f"- **JOIN Nodes**: {plan_summary.get('join_nodes_count', 0)}")
    lines.append(f"- **Scan Nodes**: {plan_summary.get('scan_nodes_count', 0)}")
    lines.append(f"- **Shuffle Nodes**: {plan_summary.get('shuffle_nodes_count', 0)}")
    lines.append(f"- **Aggregate Nodes**: {plan_summary.get('aggregate_nodes_count', 0)}")
    lines.append(f"- **BROADCAST in Use**: {'Yes' if plan_summary.get('has_broadcast_joins', False) else 'No'}")
    lines.append(f"- **Tables Scanned**: {plan_summary.get('tables_scanned', 0)}")
    lines.append("")
    
    # JOIN Strategy Analysis
    unique_join_strategies = plan_summary.get('unique_join_strategies', [])
    if unique_join_strategies:
        lines.append("## ğŸ”— JOIN Strategy Analysis")
        lines.append("")
        for strategy in unique_join_strategies:
            lines.append(f"- **{strategy.replace('_', ' ').title()}** (`{strategy}`)")
        lines.append("")
    
    # BROADCAST Node Details
    broadcast_nodes = plan_info.get("broadcast_nodes", [])
    if broadcast_nodes:
        lines.append("## ğŸ“¡ BROADCAST Node Details")
        lines.append("")
        for i, node in enumerate(broadcast_nodes, 1):
            lines.append(f"### {i}. {node['node_name']}")
            lines.append("")
            lines.append(f"- **Node ID**: {node['node_id']}")
            lines.append(f"- **Node Tag**: {node['node_tag']}")
            
            metadata = node.get('metadata', [])
            if metadata:
                lines.append("- **Related Metadata**:")
                for meta in metadata[:5]:  # Show up to 5
                    key = meta.get('key', '')
                    value = meta.get('value', '')
                    values = meta.get('values', [])
                    if values:
                        lines.append(f"  - **{key}**: {', '.join(map(str, values[:3]))}")
                    elif value:
                        lines.append(f"  - **{key}**: {value}")
            lines.append("")
    
    # JOIN Node Details
    join_nodes = plan_info.get("join_nodes", [])
    if join_nodes:
        lines.append("## ğŸ”— JOIN Node Details")
        lines.append("")
        for i, node in enumerate(join_nodes, 1):
            lines.append(f"### {i}. {node['node_name']}")
            lines.append("")
            lines.append(f"- **Node ID**: {node['node_id']}")
            lines.append(f"- **JOIN Strategy**: {node['join_strategy']}")
            lines.append(f"- **JOIN Type**: {node['join_type']}")
            
            join_keys = node.get('join_keys', [])
            if join_keys:
                lines.append(f"- **JOIN Keys**: {', '.join(join_keys[:5])}")
            lines.append("")
    
    # Table Scan Details (with size estimation info)
    table_scan_details = plan_info.get("table_scan_details", {})
    table_size_estimates = plan_info.get("table_size_estimates", {})
    if table_scan_details:
        lines.append("## ğŸ“‹ Table Scan Details")
        lines.append("")
        for table_name, scan_detail in table_scan_details.items():
            lines.append(f"### {table_name}")
            lines.append("")
            lines.append(f"- **File Format**: {scan_detail.get('file_format', 'unknown')}")
            lines.append(f"- **Pushed Filters**: {len(scan_detail.get('pushed_filters', []))}")
            lines.append(f"- **Output Columns**: {len(scan_detail.get('output_columns', []))}")
            
            # Add execution plan size estimation info (disabled - estimatedSizeInBytes not available)
            # size_info = table_size_estimates.get(table_name)
            # if size_info:
            #     lines.append(f"- **Estimated Size (Execution Plan)**: {size_info['estimated_size_mb']:.1f}MB")
            #     lines.append(f"- **Size Estimation Confidence**: {size_info.get('confidence', 'medium')}")
            #     if 'num_files' in size_info:
            #         lines.append(f"- **Number of Files**: {size_info['num_files']}")
            #     if 'num_partitions' in size_info:
            #         lines.append(f"- **Number of Partitions**: {size_info['num_partitions']}")
            
            pushed_filters = scan_detail.get('pushed_filters', [])
            if pushed_filters:
                lines.append("- **Pushed Down Filters**:")
                for filter_expr in pushed_filters[:3]:  # Show up to 3
                    lines.append(f"  - `{filter_expr}`")
            lines.append("")
    
    # Shuffle Node Details
    shuffle_nodes = plan_info.get("shuffle_nodes", [])
    if shuffle_nodes:
        lines.append("## ğŸ”„ Shuffle Node Details")
        lines.append("")
        for i, node in enumerate(shuffle_nodes, 1):
            lines.append(f"### {i}. {node['node_name']}")
            lines.append("")
            lines.append(f"- **Node ID**: {node['node_id']}")
            
            partition_keys = node.get('partition_keys', [])
            if partition_keys:
                lines.append(f"- **Partition Keys**: {', '.join(partition_keys)}")
            lines.append("")
    
    # Aggregate Node Details
    aggregate_nodes = plan_info.get("aggregate_nodes", [])
    if aggregate_nodes:
        lines.append("## ğŸ“Š Aggregate Node Details")
        lines.append("")
        for i, node in enumerate(aggregate_nodes, 1):
            lines.append(f"### {i}. {node['node_name']}")
            lines.append("")
            lines.append(f"- **Node ID**: {node['node_id']}")
            
            group_keys = node.get('group_keys', [])
            if group_keys:
                lines.append(f"- **Group Keys**: {', '.join(group_keys[:5])}")
            
            agg_expressions = node.get('aggregate_expressions', [])
            if agg_expressions:
                lines.append(f"- **Aggregate Functions**: {', '.join(agg_expressions[:5])}")
            lines.append("")
    
    # Table Size Estimation Summary (disabled - estimatedSizeInBytes not available)
    # table_size_estimates = plan_info.get("table_size_estimates", {})
    # if table_size_estimates:
    #     lines.append("## ğŸ“ Table Size Estimation (Execution Plan Based)")
    #     lines.append("")
    #     total_estimated_size = sum(size_info['estimated_size_mb'] for size_info in table_size_estimates.values())
    #     lines.append(f"- **Estimated Tables Count**: {len(table_size_estimates)}")
    #     lines.append(f"- **Total Estimated Size**: {total_estimated_size:.1f}MB")
    #     lines.append("")
    #     
    #     for table_name, size_info in list(table_size_estimates.items())[:5]:  # Show up to 5 tables
    #         lines.append(f"### {table_name}")
    #         lines.append(f"- **Estimated Size**: {size_info['estimated_size_mb']:.1f}MB")
    #         lines.append(f"- **Confidence**: {size_info.get('confidence', 'medium')}")
    #         lines.append(f"- **Node**: {size_info.get('node_name', 'unknown')}")
    #         if 'num_files' in size_info:
    #             lines.append(f"- **Number of Files**: {size_info['num_files']}")
    #         lines.append("")
    #     
    #     if len(table_size_estimates) > 5:
    #         lines.append(f"...and {len(table_size_estimates) - 5} more tables (see details in sections above)")
    #         lines.append("")
    
    # Plan-based Optimization Recommendations
    lines.append("## ğŸ’¡ Plan-based Optimization Recommendations")
    lines.append("")
    
    if plan_summary.get('has_broadcast_joins', False):
        lines.append("âœ… **BROADCAST JOIN is already applied**")
        lines.append("- Current execution plan has BROADCAST optimization enabled")
        
        # Show list of broadcast tables
        broadcast_tables = plan_summary.get('broadcast_tables', [])
        if broadcast_tables:
            lines.append(f"- **Tables Being Broadcast**: {', '.join(broadcast_tables)}")
        
        lines.append("- Check for additional BROADCAST application opportunities")
    else:
        lines.append("âš ï¸ **BROADCAST JOIN is not applied**")
        lines.append("- Consider applying BROADCAST hints to small tables")
        lines.append("- Identify tables under 30MB threshold")
    lines.append("")
    
    if plan_summary.get('shuffle_nodes_count', 0) > 3:
        lines.append("âš ï¸ **Multiple shuffle operations detected**")
        lines.append("- Review data distribution and partitioning strategy")
        lines.append("- Consider applying Liquid Clustering")
    lines.append("")
    
    # Size estimation based optimization suggestions (disabled - estimatedSizeInBytes not available)
    # table_size_estimates = plan_info.get("table_size_estimates", {})
    # if table_size_estimates:
    #     small_tables = [name for name, info in table_size_estimates.items() if info['estimated_size_mb'] <= 30]
    #     if small_tables:
    #         lines.append("ğŸ’¡ **Execution Plan Based BROADCAST Recommendations**")
    #         lines.append(f"- Small tables â‰¤30MB detected: {len(small_tables)}")
    #         for table in small_tables[:3]:  # Show up to 3 tables
    #             size_mb = table_size_estimates[table]['estimated_size_mb']
    #             lines.append(f"  â€¢ {table}: {size_mb:.1f}MB (BROADCAST candidate)")
    #         if len(small_tables) > 3:
    #             lines.append(f"  â€¢ ...and {len(small_tables) - 3} more tables")
    #         lines.append("")
    
    lines.append("---")
    lines.append("")
    lines.append("This report was automatically generated by the Databricks SQL Execution Plan Analysis Tool.")
    
    return '\n'.join(lines)

def generate_comprehensive_optimization_report(query_id: str, optimized_result: str, metrics: Dict[str, Any], analysis_result: str = "") -> str:
    """
    åŒ…æ‹¬çš„ãªæœ€é©åŒ–ãƒ¬ãƒãƒ¼ãƒˆã‚’ç”Ÿæˆ
    EXPLAINå®Ÿè¡Œãƒ•ãƒ©ã‚°ãŒYã®å ´åˆã¯ã€EXPLAINçµæœã‚‚å«ã‚ã‚‹
    
    Args:
        query_id: ã‚¯ã‚¨ãƒªID
        optimized_result: æœ€é©åŒ–çµæœ
        metrics: ãƒ¡ãƒˆãƒªã‚¯ã‚¹æƒ…å ±
        analysis_result: ãƒœãƒˆãƒ«ãƒãƒƒã‚¯åˆ†æçµæœ
    
    Returns:
        str: èª­ã¿ã‚„ã™ãæ§‹æˆã•ã‚ŒãŸãƒ¬ãƒãƒ¼ãƒˆ
    """
    from datetime import datetime
    
    # EXPLAINçµæœãƒ•ã‚¡ã‚¤ãƒ«ã®èª­ã¿è¾¼ã¿ï¼ˆEXPLAIN_ENABLEDãŒYã®å ´åˆï¼‰
    explain_section = ""
    explain_enabled = globals().get('EXPLAIN_ENABLED', 'N')
    
    if explain_enabled.upper() == 'Y':
        # æœ€æ–°ã®EXPLAINçµæœãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ¤œç´¢
        import glob
        import os
        
        explain_files = glob.glob("output_explain_plan_*.txt")
        if explain_files:
            # æœ€æ–°ã®ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å–å¾—
            latest_explain_file = max(explain_files, key=os.path.getctime)
            try:
                with open(latest_explain_file, 'r', encoding='utf-8') as f:
                    explain_content = f.read()
                
                if OUTPUT_LANGUAGE == 'ja':
                    explain_section = f"""

## ğŸ” 6. EXPLAINå®Ÿè¡Œçµæœ

### ğŸ“„ å®Ÿè¡Œãƒ—ãƒ©ãƒ³è©³ç´°

**ãƒ•ã‚¡ã‚¤ãƒ«**: {latest_explain_file}

```
{explain_content}
```

### ğŸ“Š Physical Planåˆ†æãƒã‚¤ãƒ³ãƒˆ

ä»¥ä¸‹ã®è¦³ç‚¹ã‹ã‚‰å®Ÿè¡Œãƒ—ãƒ©ãƒ³ã‚’åˆ†æã—ã¾ã—ãŸï¼š

1. **ãƒ•ã‚¡ã‚¤ãƒ«ã‚¹ã‚­ãƒ£ãƒ³åŠ¹ç‡**: ãƒ†ãƒ¼ãƒ–ãƒ«ã‚¹ã‚­ãƒ£ãƒ³ã®ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼ãƒ—ãƒƒã‚·ãƒ¥ãƒ€ã‚¦ãƒ³é©ç”¨çŠ¶æ³
2. **ã‚¸ãƒ§ã‚¤ãƒ³æˆ¦ç•¥**: BROADCASTã€SortMergeã€HashJoinã®é©åˆ‡ãªé¸æŠ
3. **ã‚·ãƒ£ãƒƒãƒ•ãƒ«æœ€é©åŒ–**: ãƒ‡ãƒ¼ã‚¿ç§»å‹•ã®æœ€å°åŒ–ã¨ãƒ‘ãƒ¼ãƒ†ã‚£ã‚·ãƒ§ãƒ³æˆ¦ç•¥
4. **ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ã‚·ãƒ§ãƒ³**: ä¸è¦ãªã‚«ãƒ©ãƒ èª­ã¿è¾¼ã¿ã®å‰Šé™¤
5. **Photonåˆ©ç”¨çŠ¶æ³**: ãƒ™ã‚¯ãƒˆãƒ«åŒ–å‡¦ç†ã®é©ç”¨ç¯„å›²

### ğŸš€ Photon Explanationåˆ†æ

Photonæœªå¯¾å¿œæ“ä½œã‚„æœ€é©åŒ–æ©Ÿä¼šã«ã¤ã„ã¦è©³ç´°ãªåˆ†æã‚’å®Ÿæ–½ã—ã¾ã—ãŸã€‚

"""
                else:
                    explain_section = f"""

## ğŸ” 6. EXPLAIN Execution Results

### ğŸ“„ Execution Plan Details

**File**: {latest_explain_file}

```
{explain_content}
```

### ğŸ“Š Physical Plan Analysis Points

The execution plan was analyzed from the following perspectives:

1. **File Scan Efficiency**: Filter pushdown application status for table scans
2. **Join Strategy**: Appropriate selection of BROADCAST, SortMerge, HashJoin
3. **Shuffle Optimization**: Data movement minimization and partitioning strategy
4. **Projection**: Removal of unnecessary column reads
5. **Photon Utilization**: Vectorized processing application scope

### ğŸš€ Photon Explanation Analysis

Detailed analysis of Photon-incompatible operations and optimization opportunities was performed.

"""
                    
            except Exception as e:
                if OUTPUT_LANGUAGE == 'ja':
                    explain_section = f"\n\n## ğŸ” 6. EXPLAINå®Ÿè¡Œçµæœ\n\nâš ï¸ EXPLAINçµæœãƒ•ã‚¡ã‚¤ãƒ«ã®èª­ã¿è¾¼ã¿ã«å¤±æ•—: {str(e)}\n"
                else:
                    explain_section = f"\n\n## ğŸ” 6. EXPLAIN Execution Results\n\nâš ï¸ Failed to load EXPLAIN result file: {str(e)}\n"
        else:
            if OUTPUT_LANGUAGE == 'ja':
                explain_section = f"\n\n## ğŸ” 6. EXPLAINå®Ÿè¡Œçµæœ\n\nâš ï¸ EXPLAINçµæœãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚\n"
            else:
                explain_section = f"\n\n## ğŸ” 6. EXPLAIN Execution Results\n\nâš ï¸ EXPLAIN result file not found.\n"
    
    # åŸºæœ¬æƒ…å ±ã®å–å¾—
    overall_metrics = metrics.get('overall_metrics', {})
    bottleneck_indicators = metrics.get('bottleneck_indicators', {})
    liquid_analysis = metrics.get('liquid_clustering_analysis', {})
    
    # thinking_enabledå¯¾å¿œ: analysis_resultãŒãƒªã‚¹ãƒˆã®å ´åˆã®å‡¦ç†
    if isinstance(analysis_result, list):
        analysis_result_str = format_thinking_response(analysis_result)
    else:
        analysis_result_str = str(analysis_result)
    
    # signatureæƒ…å ±ã®é™¤å»
    import re
    signature_pattern = r"'signature':\s*'[A-Za-z0-9+/=]{100,}'"
    analysis_result_str = re.sub(signature_pattern, "'signature': '[REMOVED]'", analysis_result_str)
    
    # ãƒ¬ãƒãƒ¼ãƒˆã®æ§‹æˆ
    if OUTPUT_LANGUAGE == 'ja':
        report = f"""# ğŸ“Š SQLæœ€é©åŒ–ãƒ¬ãƒãƒ¼ãƒˆ

**ã‚¯ã‚¨ãƒªID**: {query_id}  
**ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆæ—¥æ™‚**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}

---

## ğŸ¯ 1. ãƒœãƒˆãƒ«ãƒãƒƒã‚¯åˆ†æçµæœ

### ğŸ¤– AIã«ã‚ˆã‚‹è©³ç´°åˆ†æ

{analysis_result_str}

### ğŸ“Š ä¸»è¦ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æŒ‡æ¨™

| æŒ‡æ¨™ | å€¤ | è©•ä¾¡ |
|------|-----|------|
| å®Ÿè¡Œæ™‚é–“ | {overall_metrics.get('total_time_ms', 0):,} ms | {'âœ… è‰¯å¥½' if overall_metrics.get('total_time_ms', 0) < 60000 else 'âš ï¸ æ”¹å–„å¿…è¦'} |
| Photonæœ‰åŠ¹ | {'ã¯ã„' if overall_metrics.get('photon_enabled', False) else 'ã„ã„ãˆ'} | {'âœ… è‰¯å¥½' if overall_metrics.get('photon_enabled', False) else 'âŒ æœªæœ‰åŠ¹'} |
| ã‚­ãƒ£ãƒƒã‚·ãƒ¥åŠ¹ç‡ | {bottleneck_indicators.get('cache_hit_ratio', 0) * 100:.1f}% | {'âœ… è‰¯å¥½' if bottleneck_indicators.get('cache_hit_ratio', 0) > 0.8 else 'âš ï¸ æ”¹å–„å¿…è¦'} |
| ãƒ•ã‚£ãƒ«ã‚¿ç‡ | {bottleneck_indicators.get('data_selectivity', 0) * 100:.2f}% | {'âœ… è‰¯å¥½' if bottleneck_indicators.get('data_selectivity', 0) > 0.5 else 'âš ï¸ ãƒ•ã‚£ãƒ«ã‚¿æ¡ä»¶ã‚’ç¢ºèª'} |
| ã‚·ãƒ£ãƒƒãƒ•ãƒ«æ“ä½œ | {bottleneck_indicators.get('shuffle_operations_count', 0)}å› | {'âœ… è‰¯å¥½' if bottleneck_indicators.get('shuffle_operations_count', 0) < 5 else 'âš ï¸ å¤šæ•°'} |
| ã‚¹ãƒ”ãƒ«ç™ºç”Ÿ | {'ã¯ã„' if bottleneck_indicators.get('has_spill', False) else 'ã„ã„ãˆ'} | {'âŒ å•é¡Œã‚ã‚Š' if bottleneck_indicators.get('has_spill', False) else 'âœ… è‰¯å¥½'} |
| ã‚¹ã‚­ãƒ¥ãƒ¼æ¤œå‡º | {'AQEã§æ¤œå‡ºãƒ»å¯¾å¿œæ¸ˆ' if bottleneck_indicators.get('has_skew', False) else 'æ½œåœ¨çš„ãªã‚¹ã‚­ãƒ¥ãƒ¼ã®å¯èƒ½æ€§ã‚ã‚Š' if bottleneck_indicators.get('has_aqe_shuffle_skew_warning', False) else 'æœªæ¤œå‡º'} | {'ğŸ”§ AQEå¯¾å¿œæ¸ˆ' if bottleneck_indicators.get('has_skew', False) else 'âš ï¸ æ”¹å–„å¿…è¦' if bottleneck_indicators.get('has_aqe_shuffle_skew_warning', False) else 'âœ… è‰¯å¥½'} |

### ğŸš¨ ä¸»è¦ãƒœãƒˆãƒ«ãƒãƒƒã‚¯

"""
        
        # ä¸»è¦ãƒœãƒˆãƒ«ãƒãƒƒã‚¯ã®è©³ç´°
        bottlenecks = []
        
        if bottleneck_indicators.get('has_spill', False):
            spill_gb = bottleneck_indicators.get('spill_bytes', 0) / 1024 / 1024 / 1024
            bottlenecks.append(f"**ãƒ¡ãƒ¢ãƒªã‚¹ãƒ”ãƒ«**: {spill_gb:.2f}GB - ãƒ¡ãƒ¢ãƒªä¸è¶³ã«ã‚ˆã‚‹æ€§èƒ½ä½ä¸‹")
        
        if bottleneck_indicators.get('has_shuffle_bottleneck', False):
            bottlenecks.append("**ã‚·ãƒ£ãƒƒãƒ•ãƒ«ãƒœãƒˆãƒ«ãƒãƒƒã‚¯**: JOIN/GROUP BYå‡¦ç†ã§ã®å¤§é‡ãƒ‡ãƒ¼ã‚¿è»¢é€")
        
        if bottleneck_indicators.get('has_skew', False):
            bottlenecks.append("**ãƒ‡ãƒ¼ã‚¿ã‚¹ã‚­ãƒ¥ãƒ¼**: AQEã§æ¤œå‡ºãƒ»å¯¾å¿œæ¸ˆ - SparkãŒè‡ªå‹•çš„ã«æœ€é©åŒ–å®Ÿè¡Œ")
        elif bottleneck_indicators.get('has_aqe_shuffle_skew_warning', False):
            bottlenecks.append("**ãƒ‡ãƒ¼ã‚¿ã‚¹ã‚­ãƒ¥ãƒ¼**: æ½œåœ¨çš„ãªã‚¹ã‚­ãƒ¥ãƒ¼ã®å¯èƒ½æ€§ã‚ã‚Š - ãƒ‘ãƒ¼ãƒ†ã‚£ã‚·ãƒ§ãƒ³ã‚µã‚¤ã‚ºãŒ512MBä»¥ä¸Š")
        
        if bottleneck_indicators.get('cache_hit_ratio', 0) < 0.5:
            bottlenecks.append("**ã‚­ãƒ£ãƒƒã‚·ãƒ¥åŠ¹ç‡ä½ä¸‹**: ãƒ‡ãƒ¼ã‚¿å†åˆ©ç”¨åŠ¹ç‡ãŒä½ã„")
        
        if not overall_metrics.get('photon_enabled', False):
            bottlenecks.append("**Photonæœªæœ‰åŠ¹**: é«˜é€Ÿå‡¦ç†ã‚¨ãƒ³ã‚¸ãƒ³ãŒåˆ©ç”¨ã•ã‚Œã¦ã„ãªã„")
        
        if bottleneck_indicators.get('data_selectivity', 0) < 0.2:
            bottlenecks.append("**ãƒ•ã‚£ãƒ«ã‚¿åŠ¹ç‡ä½ä¸‹**: å¿…è¦ä»¥ä¸Šã®ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã‚“ã§ã„ã‚‹")
        
        if bottlenecks:
            for i, bottleneck in enumerate(bottlenecks, 1):
                report += f"{i}. {bottleneck}\n"
        else:
            report += "ä¸»è¦ãªãƒœãƒˆãƒ«ãƒãƒƒã‚¯ã¯è¨­å®šãªã—ã€‚\n"
        
        report += "\n"
        
        # Liquid Clusteringåˆ†æçµæœã®è¿½åŠ 
        if liquid_analysis:
            performance_context = liquid_analysis.get('performance_context', {})
            llm_analysis = liquid_analysis.get('llm_analysis', '')
            
            report += f"""

## ğŸ—‚ï¸ 3. Liquid Clusteringåˆ†æçµæœ

### ğŸ“Š ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æ¦‚è¦

| é …ç›® | å€¤ |
|------|-----|
| å®Ÿè¡Œæ™‚é–“ | {performance_context.get('total_time_sec', 0):.1f}ç§’ |
| ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ | {performance_context.get('read_gb', 0):.2f}GB |
| å‡ºåŠ›è¡Œæ•° | {performance_context.get('rows_produced', 0):,}è¡Œ |
| èª­ã¿è¾¼ã¿è¡Œæ•° | {performance_context.get('rows_read', 0):,}è¡Œ |
| ãƒ•ã‚£ãƒ«ã‚¿ç‡ | {performance_context.get('data_selectivity', 0):.4f} |

### ğŸ¤– AIåˆ†æçµæœ

{llm_analysis}

"""
        
        # æœ€ã‚‚æ™‚é–“ãŒã‹ã‹ã£ã¦ã„ã‚‹å‡¦ç†TOP10ã‚’çµ±åˆ
        report += f"""
## ğŸŒ 2. æœ€ã‚‚æ™‚é–“ãŒã‹ã‹ã£ã¦ã„ã‚‹å‡¦ç†TOP10

### ğŸ“Š è©³ç´°ãªãƒœãƒˆãƒ«ãƒãƒƒã‚¯åˆ†æ

ä»¥ä¸‹ã®ãƒˆãƒ”ãƒƒã‚¯ã«åŸºã¥ã„ã¦å‡¦ç†ã‚’åˆ†æã—ã¾ã™ï¼š

#### ğŸ” åˆ†æå¯¾è±¡ãƒˆãƒ”ãƒƒã‚¯
- **â±ï¸ å®Ÿè¡Œæ™‚é–“**: å…¨ä½“ã«å ã‚ã‚‹å‡¦ç†æ™‚é–“ã®å‰²åˆ
- **ğŸ’¾ ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡**: ãƒ”ãƒ¼ã‚¯ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ã¨ãƒ¡ãƒ¢ãƒªãƒ—ãƒ¬ãƒƒã‚·ãƒ£ãƒ¼
- **ğŸ”§ ä¸¦åˆ—åº¦**: ã‚¿ã‚¹ã‚¯æ•°ã¨ä¸¦åˆ—å®Ÿè¡ŒåŠ¹ç‡
- **ğŸ’¿ ã‚¹ãƒ”ãƒ«æ¤œå‡º**: ãƒ¡ãƒ¢ãƒªä¸è¶³ã«ã‚ˆã‚‹ãƒ‡ã‚£ã‚¹ã‚¯ã‚¹ãƒ”ãƒ«
- **âš–ï¸ ã‚¹ã‚­ãƒ¥ãƒ¼æ¤œå‡º**: AQEãƒ™ãƒ¼ã‚¹ã®ãƒ‡ãƒ¼ã‚¿åˆ†æ•£ä¸å‡ç­‰æ¤œå‡º
- **ğŸ”„ Shuffleå±æ€§**: ãƒ‘ãƒ¼ãƒ†ã‚£ã‚·ãƒ§ãƒ³å†åˆ†æ•£ã®æœ€é©åŒ–ãƒã‚¤ãƒ³ãƒˆ
- **ğŸš€ å‡¦ç†åŠ¹ç‡**: è¡Œ/ç§’ã§ã®å‡¦ç†åŠ¹ç‡æŒ‡æ¨™

"""
        
        # TOP10ãƒ¬ãƒãƒ¼ãƒˆã®ç”Ÿæˆã¨çµ±åˆ
        try:
            top10_report = generate_top10_time_consuming_processes_report(metrics, 10)
            # ãƒ¬ãƒãƒ¼ãƒˆã‹ã‚‰ãƒ˜ãƒƒãƒ€ãƒ¼ã‚’é™¤å»ã—ã¦çµ±åˆ
            top10_lines = top10_report.split('\n')
            # "## ğŸŒ æœ€ã‚‚æ™‚é–“ãŒã‹ã‹ã£ã¦ã„ã‚‹å‡¦ç†TOP10"ã®è¡Œã‚’ã‚¹ã‚­ãƒƒãƒ—
            filtered_lines = []
            skip_header = True
            for line in top10_lines:
                if skip_header and line.startswith("## ğŸŒ"):
                    skip_header = False
                    continue
                if not skip_header:
                    filtered_lines.append(line)
            
            report += '\n'.join(filtered_lines)
            
        except Exception as e:
            report += f"âš ï¸ TOP10å‡¦ç†æ™‚é–“åˆ†æã®ç”Ÿæˆã§ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(e)}\n"
        
        # SQLæœ€é©åŒ–åˆ†æçµæœã®è¿½åŠ 
        report += f"""

## ğŸš€ 4. SQLæœ€é©åŒ–åˆ†æçµæœ

### ğŸ’¡ æœ€é©åŒ–ææ¡ˆ

{optimized_result}

### ğŸ“ˆ 5. æœŸå¾…ã•ã‚Œã‚‹ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æ”¹å–„åŠ¹æœ

#### ğŸ¯ äºˆæƒ³ã•ã‚Œã‚‹æ”¹å–„ç‚¹

"""
        
        # æœŸå¾…ã•ã‚Œã‚‹æ”¹å–„åŠ¹æœã‚’è¨ˆç®—
        expected_improvements = []
        
        if bottleneck_indicators.get('has_spill', False):
            expected_improvements.append("**ãƒ¡ãƒ¢ãƒªã‚¹ãƒ”ãƒ«è§£æ¶ˆ**: æœ€å¤§50-80%ã®æ€§èƒ½æ”¹å–„ãŒæœŸå¾…ã•ã‚Œã¾ã™")
        
        if bottleneck_indicators.get('has_shuffle_bottleneck', False):
            expected_improvements.append("**ã‚·ãƒ£ãƒƒãƒ•ãƒ«æœ€é©åŒ–**: 20-60%ã®å®Ÿè¡Œæ™‚é–“çŸ­ç¸®ãŒæœŸå¾…ã•ã‚Œã¾ã™")
        
        if bottleneck_indicators.get('cache_hit_ratio', 0) < 0.5:
            expected_improvements.append("**ã‚­ãƒ£ãƒƒã‚·ãƒ¥åŠ¹ç‡å‘ä¸Š**: 30-70%ã®èª­ã¿è¾¼ã¿æ™‚é–“çŸ­ç¸®ãŒæœŸå¾…ã•ã‚Œã¾ã™")
        
        if not overall_metrics.get('photon_enabled', False):
            expected_improvements.append("**Photonæœ‰åŠ¹åŒ–**: 2-10å€ã®å‡¦ç†é€Ÿåº¦å‘ä¸ŠãŒæœŸå¾…ã•ã‚Œã¾ã™")
        
        if bottleneck_indicators.get('data_selectivity', 0) < 0.2:
            expected_improvements.append("**ãƒ•ã‚£ãƒ«ã‚¿åŠ¹ç‡æ”¹å–„**: 40-90%ã®ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿é‡å‰Šæ¸›ãŒæœŸå¾…ã•ã‚Œã¾ã™")
        
        if expected_improvements:
            for i, improvement in enumerate(expected_improvements, 1):
                report += f"{i}. {improvement}\n"
            
            # ç·åˆçš„ãªæ”¹å–„åŠ¹æœ
            total_time_ms = overall_metrics.get('total_time_ms', 0)
            if total_time_ms > 0:
                improvement_ratio = min(0.8, len(expected_improvements) * 0.15)  # æœ€å¤§80%æ”¹å–„
                expected_time = total_time_ms * (1 - improvement_ratio)
                report += f"\n**ç·åˆæ”¹å–„åŠ¹æœ**: å®Ÿè¡Œæ™‚é–“ {total_time_ms:,}ms â†’ {expected_time:,.0f}msï¼ˆç´„{improvement_ratio*100:.0f}%æ”¹å–„ï¼‰\n"
        else:
            report += "ç¾åœ¨ã®ã‚¯ã‚¨ãƒªã¯æ—¢ã«æœ€é©åŒ–ã•ã‚Œã¦ã„ã¾ã™ã€‚å¤§å¹…ãªæ”¹å–„ã¯æœŸå¾…ã•ã‚Œã¾ã›ã‚“ã€‚\n"
        
        report += f"""

#### ğŸ”§ å®Ÿè£…å„ªå…ˆåº¦

1. **é«˜å„ªå…ˆåº¦**: Photonæœ‰åŠ¹åŒ–ã€ãƒ¡ãƒ¢ãƒªã‚¹ãƒ”ãƒ«è§£æ¶ˆ
2. **ä¸­å„ªå…ˆåº¦**: ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹æœ€é©åŒ–ã€ãƒ‘ãƒ¼ãƒ†ã‚£ã‚·ãƒ§ãƒ³æˆ¦ç•¥
3. **ä½å„ªå…ˆåº¦**: çµ±è¨ˆæƒ…å ±æ›´æ–°ã€ã‚­ãƒ£ãƒƒã‚·ãƒ¥æˆ¦ç•¥

{explain_section}

---

*ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆæ™‚åˆ»: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}*
"""
        
    else:
        # è‹±èªç‰ˆï¼ˆåŒæ§˜ã®æ§‹æˆï¼‰
        report = f"""# ğŸ“Š SQL Optimization Report

**Query ID**: {query_id}  
**Report Generation Time**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}

---

## ğŸ¯ 1. Bottleneck Analysis Results

### ğŸ¤– AI-Powered Analysis

{analysis_result_str}

### ğŸ“Š Key Performance Indicators

| Metric | Value | Status |
|--------|-------|--------|
| Execution Time | {overall_metrics.get('total_time_ms', 0):,} ms | {'âœ… Good' if overall_metrics.get('total_time_ms', 0) < 60000 else 'âš ï¸ Needs Improvement'} |
| Photon Enabled | {'Yes' if overall_metrics.get('photon_enabled', False) else 'No'} | {'âœ… Good' if overall_metrics.get('photon_enabled', False) else 'âŒ Not Enabled'} |
| Cache Efficiency | {bottleneck_indicators.get('cache_hit_ratio', 0) * 100:.1f}% | {'âœ… Good' if bottleneck_indicators.get('cache_hit_ratio', 0) > 0.8 else 'âš ï¸ Needs Improvement'} |
| Filter Rate | {bottleneck_indicators.get('data_selectivity', 0) * 100:.2f}% | {'âœ… Good' if bottleneck_indicators.get('data_selectivity', 0) > 0.5 else 'âš ï¸ Check Filter Conditions'} |
| Shuffle Operations | {bottleneck_indicators.get('shuffle_operations_count', 0)} times | {'âœ… Good' if bottleneck_indicators.get('shuffle_operations_count', 0) < 5 else 'âš ï¸ High'} |
| Spill Occurrence | {'Yes' if bottleneck_indicators.get('has_spill', False) else 'No'} | {'âŒ Issues' if bottleneck_indicators.get('has_spill', False) else 'âœ… Good'} |
| Skew Detection | {'AQE Detected & Handled' if bottleneck_indicators.get('has_skew', False) else 'Not Detected'} | {'ğŸ”§ AQE Handled' if bottleneck_indicators.get('has_skew', False) else 'âœ… Good'} |

### ğŸš¨ Key Bottlenecks

"""
        
        # ä¸»è¦ãƒœãƒˆãƒ«ãƒãƒƒã‚¯ã®è©³ç´°ï¼ˆè‹±èªç‰ˆï¼‰
        bottlenecks = []
        
        if bottleneck_indicators.get('has_spill', False):
            spill_gb = bottleneck_indicators.get('spill_bytes', 0) / 1024 / 1024 / 1024
            bottlenecks.append(f"**Memory Spill**: {spill_gb:.2f}GB - Performance degradation due to memory shortage")
        
        if bottleneck_indicators.get('has_shuffle_bottleneck', False):
            bottlenecks.append("**Shuffle Bottleneck**: Large data transfer in JOIN/GROUP BY operations")
        
        if bottleneck_indicators.get('has_skew', False):
            bottlenecks.append("**Data Skew**: AQE Detected & Handled - Spark automatically optimized execution")
        elif bottleneck_indicators.get('has_aqe_shuffle_skew_warning', False):
            bottlenecks.append("**Data Skew**: Potential skew possibility - Partition size â‰¥ 512MB")
        
        if bottleneck_indicators.get('cache_hit_ratio', 0) < 0.5:
            bottlenecks.append("**Cache Inefficiency**: Low data reuse efficiency")
        
        if not overall_metrics.get('photon_enabled', False):
            bottlenecks.append("**Photon Not Enabled**: High-speed processing engine not utilized")
        
        if bottleneck_indicators.get('data_selectivity', 0) < 0.2:
            bottlenecks.append("**Poor Filter Efficiency**: Reading more data than necessary")
        
        if bottlenecks:
            for i, bottleneck in enumerate(bottlenecks, 1):
                report += f"{i}. {bottleneck}\n"
        else:
            report += "No major bottlenecks detected.\n"
        
        report += "\n"
        
        # æœ€ã‚‚æ™‚é–“ãŒã‹ã‹ã£ã¦ã„ã‚‹å‡¦ç†TOP10ã‚’çµ±åˆï¼ˆè‹±èªç‰ˆï¼‰
        report += f"""
## ğŸŒ 2. Top 10 Most Time-Consuming Processes

### ğŸ“Š Detailed Bottleneck Analysis

The following topics are analyzed for process evaluation:

#### ğŸ” Analysis Topics
- **â±ï¸ Execution Time**: Percentage of total processing time
- **ğŸ’¾ Memory Usage**: Peak memory usage and memory pressure
- **ğŸ”§ Parallelism**: Number of tasks and parallel execution efficiency
- **ğŸ’¿ Spill Detection**: Disk spill due to memory shortage
- **âš–ï¸ Skew Detection**: AQE-based data distribution imbalance detection
- **ğŸ”„ Shuffle Attributes**: Optimization points for partition redistribution
- **ğŸš€ Processing Efficiency**: Processing efficiency metrics in rows/second

"""
        
        # TOP10ãƒ¬ãƒãƒ¼ãƒˆã®ç”Ÿæˆã¨çµ±åˆï¼ˆè‹±èªç‰ˆï¼‰
        try:
            top10_report = generate_top10_time_consuming_processes_report(metrics, 10)
            # ãƒ¬ãƒãƒ¼ãƒˆã‹ã‚‰ãƒ˜ãƒƒãƒ€ãƒ¼ã‚’é™¤å»ã—ã¦çµ±åˆ
            top10_lines = top10_report.split('\n')
            # "## ğŸŒ æœ€ã‚‚æ™‚é–“ãŒã‹ã‹ã£ã¦ã„ã‚‹å‡¦ç†TOP10"ã®è¡Œã‚’ã‚¹ã‚­ãƒƒãƒ—
            filtered_lines = []
            skip_header = True
            for line in top10_lines:
                if skip_header and line.startswith("## ğŸŒ"):
                    skip_header = False
                    continue
                if not skip_header:
                    filtered_lines.append(line)
            
            report += '\n'.join(filtered_lines)
            
        except Exception as e:
            report += f"âš ï¸ Error generating TOP10 analysis: {str(e)}\n"
        
        # Liquid Clusteringåˆ†æçµæœã®è¿½åŠ ï¼ˆè‹±èªç‰ˆï¼‰
        if liquid_analysis:
            performance_context = liquid_analysis.get('performance_context', {})
            llm_analysis = liquid_analysis.get('llm_analysis', '')
            
            report += f"""

## ğŸ—‚ï¸ 3. Liquid Clustering Analysis Results

### ğŸ“Š Performance Overview

| Item | Value |
|------|-------|
| Execution Time | {performance_context.get('total_time_sec', 0):.1f}s |
| Data Read | {performance_context.get('read_gb', 0):.2f}GB |
| Output Rows | {performance_context.get('rows_produced', 0):,} |
| Read Rows | {performance_context.get('rows_read', 0):,} |
| Filter Rate | {performance_context.get('data_selectivity', 0):.4f} |

### ğŸ¤– AI Analysis Results

{llm_analysis}

"""
        
        # SQLæœ€é©åŒ–åˆ†æçµæœã®è¿½åŠ ï¼ˆè‹±èªç‰ˆï¼‰
        report += f"""
## ğŸš€ 4. SQL Optimization Analysis Results

### ğŸ’¡ Optimization Recommendations

{optimized_result}

### ğŸ“ˆ 5. Expected Performance Improvement

#### ğŸ¯ Anticipated Improvements

"""
        
        # æœŸå¾…ã•ã‚Œã‚‹æ”¹å–„åŠ¹æœã‚’è¨ˆç®—ï¼ˆè‹±èªç‰ˆï¼‰
        expected_improvements = []
        
        if bottleneck_indicators.get('has_spill', False):
            expected_improvements.append("**Memory Spill Resolution**: Up to 50-80% performance improvement expected")
        
        if bottleneck_indicators.get('has_shuffle_bottleneck', False):
            expected_improvements.append("**Shuffle Optimization**: 20-60% execution time reduction expected")
        
        if bottleneck_indicators.get('cache_hit_ratio', 0) < 0.5:
            expected_improvements.append("**Cache Efficiency**: 30-70% read time reduction expected")
        
        if not overall_metrics.get('photon_enabled', False):
            expected_improvements.append("**Photon Enablement**: 2-10x processing speed improvement expected")
        
        if bottleneck_indicators.get('data_selectivity', 0) < 0.2:
            expected_improvements.append("**Filter Efficiency**: 40-90% data read volume reduction expected")
        
        if expected_improvements:
            for i, improvement in enumerate(expected_improvements, 1):
                report += f"{i}. {improvement}\n"
            
            # ç·åˆçš„ãªæ”¹å–„åŠ¹æœ
            total_time_ms = overall_metrics.get('total_time_ms', 0)
            if total_time_ms > 0:
                improvement_ratio = min(0.8, len(expected_improvements) * 0.15)  # æœ€å¤§80%æ”¹å–„
                expected_time = total_time_ms * (1 - improvement_ratio)
                report += f"\n**Overall Improvement**: Execution time {total_time_ms:,}ms â†’ {expected_time:,.0f}ms (approx. {improvement_ratio*100:.0f}% improvement)\n"
        else:
            report += "Current query is already optimized. No significant improvements expected.\n"
        
        report += f"""

#### ğŸ”§ Implementation Priority

1. **High Priority**: Photon enablement, Memory spill resolution
2. **Medium Priority**: Index optimization, Partitioning strategy
3. **Low Priority**: Statistics update, Cache strategy

{explain_section}

---

*Report generated at: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}*
"""
    
    return report

def refine_report_with_llm(raw_report: str, query_id: str) -> str:
    """
    LLMã‚’ä½¿ã£ã¦ãƒ¬ãƒãƒ¼ãƒˆã‚’æ¨æ•²ã—ã€èª­ã¿ã‚„ã™ã„æœ€çµ‚ãƒ¬ãƒãƒ¼ãƒˆã‚’ç”Ÿæˆ
    
    Args:
        raw_report: åˆæœŸç”Ÿæˆã•ã‚ŒãŸãƒ¬ãƒãƒ¼ãƒˆ
        query_id: ã‚¯ã‚¨ãƒªID
        
    Returns:
        str: LLMã§æ¨æ•²ã•ã‚ŒãŸèª­ã¿ã‚„ã™ã„ãƒ¬ãƒãƒ¼ãƒˆ
    """
    
    print("ğŸ¤– LLMã«ã‚ˆã‚‹ãƒ¬ãƒãƒ¼ãƒˆæ¨æ•²ã‚’å®Ÿè¡Œä¸­...")
    
    # ä¿®æ­£æƒ…å ±ãŒç¢ºå®Ÿã«åæ˜ ã•ã‚Œã‚‹ã‚ˆã†ã€æ¨æ•²å‡¦ç†ã‚’æœ‰åŠ¹åŒ–

    
    refinement_prompt = f"""
æŠ€è¡“æ–‡æ›¸ã®ç·¨é›†è€…ã¨ã—ã¦ã€Databricks SQLãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹åˆ†æãƒ¬ãƒãƒ¼ãƒˆã‚’ä»¥ä¸‹ã®ãƒ«ãƒ¼ãƒ«ã«å¾“ã£ã¦æ¨æ•²ã—ã¦ãã ã•ã„ã€‚

ã€çµ¶å¯¾ã«å®ˆã‚‹ã¹ãè¦‹å‡ºã—æ§‹é€ ã€‘
```
# Databricks SQLãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹åˆ†æãƒ¬ãƒãƒ¼ãƒˆ

## 1. ãƒœãƒˆãƒ«ãƒãƒƒã‚¯åˆ†æçµæœ

### AIã«ã‚ˆã‚‹è©³ç´°åˆ†æ

#### (1) ä¸»è¦ãƒœãƒˆãƒ«ãƒãƒƒã‚¯ã¨åŸå› 
#### (2) ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æŒ‡æ¨™ã®è©•ä¾¡
#### (3) æ¨å¥¨æ”¹å–„ã‚¢ã‚¯ã‚·ãƒ§ãƒ³

## 2. TOP10æ™‚é–“æ¶ˆè²»ãƒ—ãƒ­ã‚»ã‚¹åˆ†æ

### å®Ÿè¡Œæ™‚é–“ãƒ©ãƒ³ã‚­ãƒ³ã‚°

## 3. Liquid Clusteringåˆ†æçµæœ

### æ¨å¥¨ãƒ†ãƒ¼ãƒ–ãƒ«åˆ†æ

## 4. æœ€é©åŒ–ã•ã‚ŒãŸSQLã‚¯ã‚¨ãƒª

### æ”¹å–„ææ¡ˆ
```

ã€å³æ ¼ãªç¦æ­¢äº‹é …ã€‘
- TOP10ã‚’çµ¶å¯¾ã«TOP5ã«å¤‰æ›´ã—ãªã„
- "=========="ç­‰ã®åŒºåˆ‡ã‚Šæ–‡å­—ã‚’å‰Šé™¤ï¼ˆãŸã ã—çµµæ–‡å­—ã«ã‚ˆã‚‹è¦–è¦šçš„è¡¨ç¤ºã¯ä¿æŒï¼‰
- ç•ªå·ä»˜ããƒªã‚¹ãƒˆã§åŒã˜ç•ªå·ã‚’é‡è¤‡ã•ã›ãªã„
- ãƒ¡ãƒˆãƒªã‚¯ã‚¹å€¤ã‚„æŠ€è¡“æƒ…å ±ã‚’å‰Šé™¤ã—ãªã„

ã€ğŸš¨ é‡è¦ãªæƒ…å ±ä¿æŒã®å¿…é ˆè¦ä»¶ã€‘
- **ç¾åœ¨ã®ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°ã‚­ãƒ¼æƒ…å ±**: å„ãƒ†ãƒ¼ãƒ–ãƒ«ã®ã€Œç¾åœ¨ã®ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°ã‚­ãƒ¼: XXã€æƒ…å ±ã¯å¿…ãšä¿æŒ
- **ãƒ•ã‚£ãƒ«ã‚¿ç‡æƒ…å ±**: ã€Œãƒ•ã‚£ãƒ«ã‚¿ç‡: X.X% (èª­ã¿è¾¼ã¿: XX.XXGB, ãƒ—ãƒ«ãƒ¼ãƒ³: XX.XXGB)ã€å½¢å¼ã®æƒ…å ±ã¯å¿…ãšä¿æŒ
- **ãƒ‘ãƒ¼ã‚»ãƒ³ãƒ†ãƒ¼ã‚¸è¨ˆç®—**: ãƒœãƒˆãƒ«ãƒãƒƒã‚¯åˆ†æã®ãƒ‘ãƒ¼ã‚»ãƒ³ãƒ†ãƒ¼ã‚¸ï¼ˆå…¨ä½“ã®â—‹â—‹%ï¼‰ã¯æ­£ç¢ºãªå€¤ã‚’ä¿æŒ
- **æ¨å¥¨vsç¾åœ¨ã®æ¯”è¼ƒ**: æ¨å¥¨ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°ã‚­ãƒ¼ã¨ç¾åœ¨ã®ã‚­ãƒ¼ã®æ¯”è¼ƒæƒ…å ±ã¯å‰Šé™¤ç¦æ­¢
- **æ•°å€¤ãƒ¡ãƒˆãƒªã‚¯ã‚¹**: å®Ÿè¡Œæ™‚é–“ã€ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿é‡ã€ã‚¹ãƒ”ãƒ«é‡ç­‰ã®æ•°å€¤ãƒ‡ãƒ¼ã‚¿ã¯å‰Šé™¤ç¦æ­¢
- **SQLå®Ÿè£…ä¾‹**: ALTER TABLEæ–‡ã‚„CLUSTER BYæ§‹æ–‡ã®å…·ä½“ä¾‹ã¯å‰Šé™¤ç¦æ­¢

ã€å‡¦ç†è¦ä»¶ã€‘
1. ä¸Šè¨˜ã®è¦‹å‡ºã—æ§‹é€ ã‚’å¿…ãšä½¿ç”¨
2. æŠ€è¡“æƒ…å ±ã¨ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚’å®Œå…¨ä¿æŒï¼ˆç‰¹ã«ä¸Šè¨˜ã®é‡è¦æƒ…å ±ï¼‰
3. TOP10è¡¨ç¤ºã‚’ç¶­æŒ
4. çµµæ–‡å­—ã«ã‚ˆã‚‹è¦–è¦šçš„è¡¨ç¤ºã‚’ä¿æŒï¼ˆğŸš¨ CRITICALã€âš ï¸ HIGHã€âœ…è‰¯å¥½ç­‰ï¼‰
5. ä¸è¦ãªåŒºåˆ‡ã‚Šæ–‡å­—ï¼ˆ========ç­‰ï¼‰ã®ã¿å‰Šé™¤
6. ç¾åœ¨ã®ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°ã‚­ãƒ¼æƒ…å ±ã¨ãƒ•ã‚£ãƒ«ã‚¿ç‡æƒ…å ±ã¯çµ¶å¯¾ã«ä¿æŒ

ã€ç¾åœ¨ã®ãƒ¬ãƒãƒ¼ãƒˆã€‘
```
{raw_report}
```

ä¸Šè¨˜ã®è¦‹å‡ºã—æ§‹é€ ã«å¾“ã£ã¦æ¨æ•²ã—ã€æŠ€è¡“æƒ…å ±ã‚’å®Œå…¨ã«ä¿æŒã—ãŸãƒ¬ãƒãƒ¼ãƒˆã‚’å‡ºåŠ›ã—ã¦ãã ã•ã„ã€‚
"""
    
    try:
        provider = LLM_CONFIG.get("provider", "databricks")
        
        if provider == "databricks":
            refined_report = _call_databricks_llm(refinement_prompt)
        elif provider == "openai":
            refined_report = _call_openai_llm(refinement_prompt)
        elif provider == "azure_openai":
            refined_report = _call_azure_openai_llm(refinement_prompt)
        elif provider == "anthropic":
            refined_report = _call_anthropic_llm(refinement_prompt)
        else:
            raise ValueError(f"Unsupported LLM provider: {provider}")
        
        # thinking_enabledå¯¾å¿œ
        if isinstance(refined_report, list):
            refined_report = format_thinking_response(refined_report)
        
        # signatureæƒ…å ±ã®é™¤å»
        import re
        signature_pattern = r"'signature':\s*'[A-Za-z0-9+/=]{100,}'"
        refined_report = re.sub(signature_pattern, "'signature': '[REMOVED]'", refined_report)
        
        return refined_report
        
    except Exception as e:
        print(f"âš ï¸ LLMã«ã‚ˆã‚‹ãƒ¬ãƒãƒ¼ãƒˆæ¨æ•²ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(e)}")
        print("ğŸ“„ å…ƒã®ãƒ¬ãƒãƒ¼ãƒˆã‚’è¿”ã—ã¾ã™")
        return raw_report

def validate_and_fix_sql_syntax(sql_query: str) -> str:
    """
    SQLæ§‹æ–‡ã®åŸºæœ¬ãƒã‚§ãƒƒã‚¯ã¨ä¿®æ­£ã‚’è¡Œã†ï¼ˆæ§‹æ–‡ã‚¨ãƒ©ãƒ¼é˜²æ­¢ï¼‰
    
    ä¸»è¦ãƒã‚§ãƒƒã‚¯é …ç›®ï¼š
    1. BROADCASTãƒ’ãƒ³ãƒˆã®é…ç½®ä½ç½®æ¤œè¨¼
    2. å®Œå…¨æ€§ãƒã‚§ãƒƒã‚¯ï¼ˆSELECTã€FROMã€WHEREç­‰ã®åŸºæœ¬æ§‹æ–‡ï¼‰
    3. åŸºæœ¬çš„ãªæ§‹æ–‡ã‚¨ãƒ©ãƒ¼ã®ä¿®æ­£
    4. ã‚³ãƒ¡ãƒ³ãƒˆã‚„ãƒ—ãƒ¬ãƒ¼ã‚¹ãƒ›ãƒ«ãƒ€ãƒ¼ã®é™¤å»
    
    Args:
        sql_query: ãƒã‚§ãƒƒã‚¯å¯¾è±¡ã®SQLã‚¯ã‚¨ãƒª
        
    Returns:
        str: ä¿®æ­£ã•ã‚ŒãŸSQLã‚¯ã‚¨ãƒª
    """
    import re
    
    if not sql_query or not sql_query.strip():
        return ""
    
    # åŸºæœ¬çš„ãªã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—
    sql_query = sql_query.strip()
    
    # 1. BROADCASTãƒ’ãƒ³ãƒˆã®é…ç½®ä½ç½®ãƒã‚§ãƒƒã‚¯
    sql_query = fix_broadcast_hint_placement(sql_query)
    
    # 2. ä¸å®Œå…¨ãªSQLæ§‹æ–‡ã®æ¤œå‡ºã¨ä¿®æ­£
    sql_query = fix_incomplete_sql_syntax(sql_query)
    
    # 3. ãƒ—ãƒ¬ãƒ¼ã‚¹ãƒ›ãƒ«ãƒ€ãƒ¼ã‚„çœç•¥è¨˜å·ã®é™¤å»
    sql_query = remove_sql_placeholders(sql_query)
    
    # 4. åŸºæœ¬çš„ãªæ§‹æ–‡ã‚¨ãƒ©ãƒ¼ã®ä¿®æ­£
    sql_query = fix_basic_syntax_errors(sql_query)
    
    return sql_query

def fix_broadcast_hint_placement(sql_query: str) -> str:
    """
    BROADCASTãƒ’ãƒ³ãƒˆã®é…ç½®ä½ç½®ã‚’ä¿®æ­£ï¼ˆã‚µãƒ–ã‚¯ã‚¨ãƒªå†…éƒ¨é…ç½®ã‚’ç¦æ­¢ï¼‰
    
    ä¿®æ­£å†…å®¹ï¼š
    - ã‚µãƒ–ã‚¯ã‚¨ãƒªå†…éƒ¨ã®BROADCASTãƒ’ãƒ³ãƒˆã‚’ãƒ¡ã‚¤ãƒ³ã‚¯ã‚¨ãƒªã«ç§»å‹•
    - FROMå¥ã€JOINå¥ã€WHEREå¥å†…ã®ãƒ’ãƒ³ãƒˆã‚’å‰Šé™¤
    - è¤‡æ•°ã®BROADCASTãƒ’ãƒ³ãƒˆã‚’çµ±åˆ
    - DISTINCTå¥ã®ä¿æŒã‚’ç¢ºä¿
    """
    import re
    
    # ã‚µãƒ–ã‚¯ã‚¨ãƒªå†…éƒ¨ã®BROADCASTãƒ’ãƒ³ãƒˆã‚’æ¤œå‡ºã¨å‰Šé™¤
    # ãƒ‘ã‚¿ãƒ¼ãƒ³1: LEFT JOIN (SELECT /*+ BROADCAST(...) */ ... ã®ãƒ‘ã‚¿ãƒ¼ãƒ³
    subquery_broadcast_pattern = r'JOIN\s*\(\s*SELECT\s*/\*\+\s*BROADCAST\([^)]+\)\s*\*/'
    sql_query = re.sub(subquery_broadcast_pattern, 'JOIN (\n  SELECT', sql_query, flags=re.IGNORECASE)
    
    # ãƒ‘ã‚¿ãƒ¼ãƒ³2: WITHå¥ã‚„ã‚µãƒ–ã‚¯ã‚¨ãƒªå†…éƒ¨ã®BROADCASTãƒ’ãƒ³ãƒˆ
    cte_broadcast_pattern = r'(WITH\s+\w+\s+AS\s*\(\s*SELECT\s*)/\*\+\s*BROADCAST\([^)]+\)\s*\*/'
    sql_query = re.sub(cte_broadcast_pattern, r'\1', sql_query, flags=re.IGNORECASE)
    
    # ãƒ‘ã‚¿ãƒ¼ãƒ³3: FROMå¥å†…ã®BROADCASTãƒ’ãƒ³ãƒˆ
    from_broadcast_pattern = r'FROM\s+\w+\s*/\*\+\s*BROADCAST\([^)]+\)\s*\*/'
    sql_query = re.sub(from_broadcast_pattern, 'FROM', sql_query, flags=re.IGNORECASE)
    
    # ãƒ‘ã‚¿ãƒ¼ãƒ³4: WHEREå¥å†…ã®BROADCASTãƒ’ãƒ³ãƒˆ
    where_broadcast_pattern = r'WHERE\s*/\*\+\s*BROADCAST\([^)]+\)\s*\*/'
    sql_query = re.sub(where_broadcast_pattern, 'WHERE', sql_query, flags=re.IGNORECASE)
    
    # DISTINCTå¥ã®å­˜åœ¨ç¢ºèªï¼ˆå¤§æ–‡å­—å°æ–‡å­—ã‚’åŒºåˆ¥ã—ãªã„ï¼‰
    distinct_pattern = r'^\s*SELECT\s*(/\*\+[^*]*\*/)?\s*DISTINCT\b'
    has_distinct = bool(re.search(distinct_pattern, sql_query, re.IGNORECASE))
    
    # BROADCASTãƒ’ãƒ³ãƒˆãŒãƒ¡ã‚¤ãƒ³ã‚¯ã‚¨ãƒªã®SELECTç›´å¾Œã«ã‚ã‚‹ã‹ãƒã‚§ãƒƒã‚¯
    main_select_pattern = r'^\s*SELECT\s*(/\*\+[^*]*\*/)?\s*(DISTINCT\s*)?'
    if not re.search(main_select_pattern, sql_query, re.IGNORECASE):
        # ãƒ¡ã‚¤ãƒ³ã‚¯ã‚¨ãƒªã®SELECTç›´å¾Œã«BROADCASTãƒ’ãƒ³ãƒˆãŒãªã„å ´åˆã®å‡¦ç†
        # å‰Šé™¤ã•ã‚ŒãŸBROADCASTãƒ’ãƒ³ãƒˆã‚’å¾©å…ƒã—ã¦ãƒ¡ã‚¤ãƒ³ã‚¯ã‚¨ãƒªã«é…ç½®
        broadcast_tables = extract_broadcast_tables_from_sql(sql_query)
        if broadcast_tables:
            broadcast_hint = f"/*+ BROADCAST({', '.join(broadcast_tables)}) */"
            if has_distinct:
                # DISTINCTå¥ãŒã‚ã‚‹å ´åˆï¼šSELECT /*+ BROADCAST(...) */ DISTINCT ã®å½¢å¼ã«ã™ã‚‹
                sql_query = re.sub(r'^\s*SELECT\s*', f'SELECT {broadcast_hint} ', sql_query, flags=re.IGNORECASE)
            else:
                # DISTINCTå¥ãŒãªã„å ´åˆï¼šå¾“æ¥ã®å½¢å¼
                sql_query = re.sub(r'^\s*SELECT\s*', f'SELECT {broadcast_hint}\n  ', sql_query, flags=re.IGNORECASE)
    else:
        # æ—¢ã«ãƒ’ãƒ³ãƒˆãŒã‚ã‚‹å ´åˆã€DISTINCTå¥ãŒæ­£ã—ã„ä½ç½®ã«ã‚ã‚‹ã‹ç¢ºèª
        # é–“é•ã£ãŸé †åºï¼ˆSELECT DISTINCT /*+ BROADCAST(...) */ ï¼‰ã‚’ä¿®æ­£
        wrong_order_pattern = r'^\s*SELECT\s*DISTINCT\s*(/\*\+[^*]*\*/)'
        if re.search(wrong_order_pattern, sql_query, re.IGNORECASE):
            # é–“é•ã£ãŸé †åºã‚’ä¿®æ­£ï¼šSELECT DISTINCT /*+ HINT */ â†’ SELECT /*+ HINT */ DISTINCT
            sql_query = re.sub(wrong_order_pattern, lambda m: f'SELECT {m.group(1)} DISTINCT', sql_query, flags=re.IGNORECASE)
    
    return sql_query

def fix_incomplete_sql_syntax(sql_query: str) -> str:
    """
    ä¸å®Œå…¨ãªSQLæ§‹æ–‡ã®æ¤œå‡ºã¨ä¿®æ­£
    """
    import re
    
    # åŸºæœ¬çš„ãªSQLã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã®å­˜åœ¨ãƒã‚§ãƒƒã‚¯
    has_select = bool(re.search(r'\bSELECT\b', sql_query, re.IGNORECASE))
    has_from = bool(re.search(r'\bFROM\b', sql_query, re.IGNORECASE))
    
    # SELECTãŒãªã„å ´åˆã¯åŸºæœ¬çš„ãªSQLã§ã¯ãªã„å¯èƒ½æ€§ãŒé«˜ã„
    if not has_select:
        return sql_query
    
    # FROMãŒãªã„å ´åˆã¯ä¸å®Œå…¨ãªSQLã®å¯èƒ½æ€§
    if not has_from:
        # ä¸å®Œå…¨ãªSQLã®å ´åˆã¯ã‚³ãƒ¡ãƒ³ãƒˆã§è­¦å‘Šã‚’è¿½åŠ 
        sql_query = f"-- âš ï¸ ä¸å®Œå…¨ãªSQLæ§‹æ–‡ãŒæ¤œå‡ºã•ã‚Œã¾ã—ãŸã€‚æ‰‹å‹•ã§ç¢ºèªã—ã¦ãã ã•ã„ã€‚\n{sql_query}"
    
    return sql_query

def remove_sql_placeholders(sql_query: str) -> str:
    """
    ãƒ—ãƒ¬ãƒ¼ã‚¹ãƒ›ãƒ«ãƒ€ãƒ¼ã‚„çœç•¥è¨˜å·ã®é™¤å»ï¼ˆSQLãƒ’ãƒ³ãƒˆã¯ä¿æŒï¼‰
    """
    import re
    
    # ä¸€èˆ¬çš„ãªãƒ—ãƒ¬ãƒ¼ã‚¹ãƒ›ãƒ«ãƒ€ãƒ¼ãƒ‘ã‚¿ãƒ¼ãƒ³ï¼ˆSQLãƒ’ãƒ³ãƒˆã¯é™¤å¤–ï¼‰
    placeholders = [
        r'\.\.\.',  # çœç•¥è¨˜å·
        r'\[çœç•¥\]',  # çœç•¥è¡¨è¨˜
        r'\[ã‚«ãƒ©ãƒ å\]',  # ãƒ—ãƒ¬ãƒ¼ã‚¹ãƒ›ãƒ«ãƒ€ãƒ¼
        r'\[ãƒ†ãƒ¼ãƒ–ãƒ«å\]',  # ãƒ—ãƒ¬ãƒ¼ã‚¹ãƒ›ãƒ«ãƒ€ãƒ¼
        r'column1, column2, \.\.\.',  # ã‚«ãƒ©ãƒ çœç•¥
        r'-- \.\.\.',  # ã‚³ãƒ¡ãƒ³ãƒˆå†…ã®çœç•¥
        r'column1, column2, \.\.\.',  # ã‚«ãƒ©ãƒ çœç•¥ãƒ‘ã‚¿ãƒ¼ãƒ³
        r', \.\.\.',  # æœ«å°¾ã®çœç•¥è¨˜å·
        r'å®Œå…¨ãªSQL - ã™ã¹ã¦ã®ã‚«ãƒ©ãƒ .*?ã‚’çœç•¥ãªã—ã§è¨˜è¿°',  # æŒ‡ç¤ºæ–‡ã®é™¤å»
        r'\[å®Œå…¨ãªSQL.*?\]',  # å®Œå…¨ãªSQLæŒ‡ç¤ºã®é™¤å»
    ]
    
    for pattern in placeholders:
        sql_query = re.sub(pattern, '', sql_query, flags=re.IGNORECASE)
    
    # SQLãƒ’ãƒ³ãƒˆä»¥å¤–ã®è¤‡æ•°è¡Œã‚³ãƒ¡ãƒ³ãƒˆã‚’é™¤å»ï¼ˆãƒ’ãƒ³ãƒˆã¯ä¿æŒï¼‰
    # /*+ ... */ å½¢å¼ã®ãƒ’ãƒ³ãƒˆã¯ä¿æŒã—ã€ãã®ä»–ã® /* ... */ ã‚³ãƒ¡ãƒ³ãƒˆã®ã¿å‰Šé™¤
    sql_query = re.sub(r'/\*(?!\+).*?\*/', '', sql_query, flags=re.DOTALL)
    
    # ä¸å®Œå…¨ãªSQLæŒ‡ç¤ºã‚³ãƒ¡ãƒ³ãƒˆã‚’é™¤å»
    instruction_comments = [
        r'-- ğŸš¨ é‡è¦:.*',
        r'-- ä¾‹:.*',
        r'-- è¤‡æ•°ãƒ’ãƒ³ãƒˆä¾‹.*',
        r'-- ç„¡åŠ¹ãªä¾‹:.*',
        r'-- ğŸš¨ REPARTITIONãƒ’ãƒ³ãƒˆ.*',
    ]
    
    for pattern in instruction_comments:
        sql_query = re.sub(pattern, '', sql_query, flags=re.IGNORECASE)
    
    # ç©ºè¡Œã‚’æ­£è¦åŒ–
    sql_query = re.sub(r'\n\s*\n\s*\n+', '\n\n', sql_query)
    
    return sql_query.strip()

def fix_basic_syntax_errors(sql_query: str) -> str:
    """
    åŸºæœ¬çš„ãªæ§‹æ–‡ã‚¨ãƒ©ãƒ¼ã®ä¿®æ­£
    """
    import re
    
    # 1. NULLãƒªãƒ†ãƒ©ãƒ«ã®å‹ã‚­ãƒ£ã‚¹ãƒˆä¿®æ­£ - ã‚³ãƒ¡ãƒ³ãƒˆã‚¢ã‚¦ãƒˆï¼ˆå†—é•·CASTç”Ÿæˆã®åŸå› ï¼‰
    # SELECT null as col01 â†’ SELECT cast(null as String) as col01
    # null_literal_pattern = r'\bnull\s+as\s+(\w+)'
    # sql_query = re.sub(null_literal_pattern, r'cast(null as String) as \1', sql_query, flags=re.IGNORECASE)
    
    # 2. é€£ç¶šã™ã‚‹ã‚«ãƒ³ãƒã®ä¿®æ­£
    sql_query = re.sub(r',\s*,', ',', sql_query)
    
    # 3. ä¸æ­£ãªç©ºç™½ã®ä¿®æ­£ï¼ˆè¡Œå†…ã®é€£ç¶šã™ã‚‹ç©ºç™½ã‚’1ã¤ã«ï¼‰
    sql_query = re.sub(r'[ \t]+', ' ', sql_query)
    
    # 4. è¡Œæœ«ã®ä¸è¦ãªæ–‡å­—å‰Šé™¤
    sql_query = re.sub(r'[,;]\s*$', '', sql_query.strip())
    
    # 5. ä¸å®Œå…¨ãªSELECTæ–‡ã®ä¿®æ­£
    # SELECTã®å¾Œã«ç›´æ¥FROMãŒæ¥ã‚‹å ´åˆã‚’ä¿®æ­£
    sql_query = re.sub(r'SELECT\s+FROM', 'SELECT *\nFROM', sql_query, flags=re.IGNORECASE)
    
    # 6. ä¸å®Œå…¨ãªJOINå¥ã®ä¿®æ­£
    # JOINã®å¾Œã«ONãŒæ¥ãªã„å ´åˆã®åŸºæœ¬çš„ãªä¿®æ­£
    lines = sql_query.split('\n')
    fixed_lines = []
    
    for line in lines:
        line = line.strip()
        if line:
            # JOINã®å¾Œã«ONãŒãªã„å ´åˆã®è­¦å‘Šã‚³ãƒ¡ãƒ³ãƒˆè¿½åŠ 
            if re.search(r'\bJOIN\s+\w+\s*$', line, re.IGNORECASE):
                fixed_lines.append(line)
                fixed_lines.append('  -- âš ï¸ JOINæ¡ä»¶ï¼ˆONå¥ï¼‰ã‚’ç¢ºèªã—ã¦ãã ã•ã„')
            else:
                fixed_lines.append(line)
    
    sql_query = '\n'.join(fixed_lines)
    
    # 7. åŸºæœ¬çš„ãªæ§‹æ–‡ãƒã‚§ãƒƒã‚¯
    sql_query = add_syntax_warnings(sql_query)
    
    return sql_query

def add_syntax_warnings(sql_query: str) -> str:
    """
    åŸºæœ¬çš„ãªæ§‹æ–‡ãƒã‚§ãƒƒã‚¯ã¨è­¦å‘Šã®è¿½åŠ 
    """
    import re
    
    warnings = []
    
    # åŸºæœ¬çš„ãªSQLã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã®å­˜åœ¨ãƒã‚§ãƒƒã‚¯
    has_select = bool(re.search(r'\bSELECT\b', sql_query, re.IGNORECASE))
    has_from = bool(re.search(r'\bFROM\b', sql_query, re.IGNORECASE))
    
    # JOINãŒã‚ã‚‹ãŒONãŒãªã„å ´åˆ
    joins = re.findall(r'\b(LEFT|RIGHT|INNER|OUTER)?\s*JOIN\s+\w+', sql_query, re.IGNORECASE)
    ons = re.findall(r'\bON\b', sql_query, re.IGNORECASE)
    
    if len(joins) > len(ons):
        warnings.append('-- âš ï¸ JOINå¥ã®æ•°ã«å¯¾ã—ã¦ONå¥ãŒä¸è¶³ã—ã¦ã„ã‚‹å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™')
    
    # WITHå¥ãŒã‚ã‚‹å ´åˆã®åŸºæœ¬ãƒã‚§ãƒƒã‚¯
    if re.search(r'\bWITH\s+\w+\s+AS\s*\(', sql_query, re.IGNORECASE):
        if not re.search(r'\)\s*SELECT\b', sql_query, re.IGNORECASE):
            warnings.append('-- âš ï¸ WITHå¥ã®å¾Œã®ãƒ¡ã‚¤ãƒ³SELECTæ–‡ã‚’ç¢ºèªã—ã¦ãã ã•ã„')
    
    # è­¦å‘ŠãŒã‚ã‚‹å ´åˆã¯å…ˆé ­ã«è¿½åŠ 
    if warnings:
        sql_query = '\n'.join(warnings) + '\n\n' + sql_query
    
    return sql_query

def extract_broadcast_tables_from_sql(sql_query: str) -> list:
    """
    SQLã‚¯ã‚¨ãƒªã‹ã‚‰BROADCASTã•ã‚Œã‚‹ã¹ããƒ†ãƒ¼ãƒ–ãƒ«åã‚’æŠ½å‡º
    """
    import re
    
    # å‰Šé™¤ã•ã‚ŒãŸBROADCASTãƒ’ãƒ³ãƒˆã‹ã‚‰ãƒ†ãƒ¼ãƒ–ãƒ«åã‚’æŠ½å‡º
    broadcast_pattern = r'BROADCAST\(([^)]+)\)'
    matches = re.findall(broadcast_pattern, sql_query, re.IGNORECASE)
    
    tables = []
    for match in matches:
        # ã‚«ãƒ³ãƒã§åŒºåˆ‡ã‚‰ã‚ŒãŸãƒ†ãƒ¼ãƒ–ãƒ«åã‚’åˆ†å‰²
        table_names = [name.strip() for name in match.split(',')]
        tables.extend(table_names)
    
    return list(set(tables))  # é‡è¤‡ã‚’é™¤å»

def validate_final_sql_syntax(sql_query: str) -> bool:
    """
    æœ€çµ‚çš„ãªSQLæ§‹æ–‡ãƒã‚§ãƒƒã‚¯ï¼ˆä¿å­˜å‰ã®ç¢ºèªï¼‰
    
    Returns:
        bool: æ§‹æ–‡ãŒæ­£ã—ã„ã¨åˆ¤å®šã•ã‚ŒãŸå ´åˆTrueã€å•é¡ŒãŒã‚ã‚‹å ´åˆFalse
    """
    import re
    
    if not sql_query or not sql_query.strip():
        return False
    
    # åŸºæœ¬çš„ãªSQLã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã®å­˜åœ¨ãƒã‚§ãƒƒã‚¯
    has_select = bool(re.search(r'\bSELECT\b', sql_query, re.IGNORECASE))
    
    # SELECTãŒãªã„å ´åˆã¯ä¸æ­£
    if not has_select:
        return False
    
    # æ˜ã‚‰ã‹ã«ä¸å®Œå…¨ãªãƒ‘ã‚¿ãƒ¼ãƒ³ã®ãƒã‚§ãƒƒã‚¯
    incomplete_patterns = [
        r'\.\.\.',  # çœç•¥è¨˜å·
        r'\[çœç•¥\]',  # çœç•¥è¡¨è¨˜
        r'\[ã‚«ãƒ©ãƒ å\]',  # ãƒ—ãƒ¬ãƒ¼ã‚¹ãƒ›ãƒ«ãƒ€ãƒ¼
        r'\[ãƒ†ãƒ¼ãƒ–ãƒ«å\]',  # ãƒ—ãƒ¬ãƒ¼ã‚¹ãƒ›ãƒ«ãƒ€ãƒ¼
        r'column1, column2, \.\.\.',  # ã‚«ãƒ©ãƒ çœç•¥
        r'å®Œå…¨ãªSQL.*?ã‚’.*?è¨˜è¿°',  # æŒ‡ç¤ºæ–‡
    ]
    
    for pattern in incomplete_patterns:
        if re.search(pattern, sql_query, re.IGNORECASE):
            return False
    
    # BROADCASTãƒ’ãƒ³ãƒˆé…ç½®ã®åŸºæœ¬ãƒã‚§ãƒƒã‚¯
    broadcast_hints = re.findall(r'/\*\+\s*BROADCAST\([^)]+\)\s*\*/', sql_query, re.IGNORECASE)
    if broadcast_hints:
        # BROADCASTãƒ’ãƒ³ãƒˆãŒã‚µãƒ–ã‚¯ã‚¨ãƒªå†…éƒ¨ã«ã‚ã‚‹ã‹ãƒã‚§ãƒƒã‚¯
        subquery_broadcast = re.search(r'JOIN\s*\(\s*SELECT\s*/\*\+\s*BROADCAST', sql_query, re.IGNORECASE)
        if subquery_broadcast:
            return False
    
    # åŸºæœ¬çš„ãªæ§‹æ–‡ã‚¨ãƒ©ãƒ¼ãƒã‚§ãƒƒã‚¯
    # é€£ç¶šã™ã‚‹ã‚«ãƒ³ãƒ
    if re.search(r',\s*,', sql_query):
        return False
    
    # ä¸æ­£ãªç©ºç™½ãƒ‘ã‚¿ãƒ¼ãƒ³
    if re.search(r'\s{5,}', sql_query):  # 5å€‹ä»¥ä¸Šã®é€£ç¶šã™ã‚‹ç©ºç™½
        return False
    
    return True

def save_optimized_sql_files(original_query: str, optimized_result: str, metrics: Dict[str, Any], analysis_result: str = "") -> Dict[str, str]:
    """
    æœ€é©åŒ–ã•ã‚ŒãŸSQLã‚¯ã‚¨ãƒªã‚’å®Ÿè¡Œå¯èƒ½ãªå½¢ã§ãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜
    
    ç‰¹å¾´:
    - SQLãƒ•ã‚¡ã‚¤ãƒ«ã®æœ«å°¾ã«è‡ªå‹•ã§ã‚»ãƒŸã‚³ãƒ­ãƒ³(;)ã‚’ä»˜ä¸
    - ãã®ã¾ã¾Databricks Notebookã§å®Ÿè¡Œå¯èƒ½
    - %sql ãƒã‚¸ãƒƒã‚¯ã‚³ãƒãƒ³ãƒ‰ã§ã‚‚ç›´æ¥å®Ÿè¡Œå¯èƒ½
    - LLMã«ã‚ˆã‚‹ãƒ¬ãƒãƒ¼ãƒˆæ¨æ•²ã§èª­ã¿ã‚„ã™ã„æœ€çµ‚ãƒ¬ãƒãƒ¼ãƒˆã‚’ç”Ÿæˆ
    """
    
    import re
    from datetime import datetime
    
    # thinking_enabled: Trueã®å ´åˆã«optimized_resultãŒãƒªã‚¹ãƒˆã«ãªã‚‹ã“ã¨ãŒã‚ã‚‹ãŸã‚å¯¾å¿œ
    optimized_result_for_file = optimized_result
    optimized_result_main_content = optimized_result
    
    if isinstance(optimized_result, list):
        # ãƒ•ã‚¡ã‚¤ãƒ«ä¿å­˜ç”¨ã¯äººé–“ã«èª­ã¿ã‚„ã™ã„å½¢å¼ã«å¤‰æ›
        optimized_result_for_file = format_thinking_response(optimized_result)
        # SQLæŠ½å‡ºç”¨ã¯ä¸»è¦ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã®ã¿ã‚’ä½¿ç”¨
        optimized_result_main_content = extract_main_content_from_thinking_response(optimized_result)
    
    timestamp = datetime.now().strftime("%Y%m%d-%H%M%S")
    query_id = metrics.get('query_info', {}).get('query_id', 'unknown')
    
    # ã‚ªãƒªã‚¸ãƒŠãƒ«ã‚¯ã‚¨ãƒªãƒ•ã‚¡ã‚¤ãƒ«ã®ä¿å­˜ã¯é™¤å¤–ï¼ˆä¸è¦ï¼‰
    original_filename = None
    
    # æœ€é©åŒ–ã•ã‚ŒãŸã‚¯ã‚¨ãƒªã®æŠ½å‡ºã¨ä¿å­˜
    optimized_filename = f"output_optimized_query_{timestamp}.sql"
    
    # æœ€é©åŒ–çµæœã‹ã‚‰SQLã‚³ãƒ¼ãƒ‰ã‚’æŠ½å‡ºï¼ˆä¸»è¦ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‹ã‚‰æŠ½å‡ºï¼‰ - æ”¹å–„ç‰ˆ
    sql_pattern = r'```sql\s*(.*?)\s*```'
    sql_matches = re.findall(sql_pattern, optimized_result_main_content, re.DOTALL | re.IGNORECASE)
    
    optimized_sql = ""
    if sql_matches:
        # æœ€ã‚‚é•·ã„SQLãƒ–ãƒ­ãƒƒã‚¯ã‚’ä½¿ç”¨ï¼ˆå®Œå…¨æ€§ã‚’å„ªå…ˆï¼‰
        optimized_sql = max(sql_matches, key=len).strip()
    else:
        # SQLãƒ–ãƒ­ãƒƒã‚¯ãŒè¦‹ã¤ã‹ã‚‰ãªã„å ´åˆã¯ã€SQLé–¢é€£ã®è¡Œã‚’æŠ½å‡ºï¼ˆæ”¹å–„ç‰ˆï¼‰
        lines = optimized_result_main_content.split('\n')
        sql_lines = []
        in_sql_section = False
        
        for line in lines:
            line_stripped = line.strip()
            
            # SQLã®é–‹å§‹ã‚’æ¤œå‡º
            if any(keyword in line.upper() for keyword in ['SELECT', 'FROM', 'WHERE', 'WITH', 'CREATE', 'INSERT', 'UPDATE', 'DELETE']):
                in_sql_section = True
            
            if in_sql_section:
                # SQLã®çµ‚äº†ã‚’æ¤œå‡ºï¼ˆãƒãƒ¼ã‚¯ãƒ€ã‚¦ãƒ³ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã‚„ãƒ¬ãƒãƒ¼ãƒˆã‚»ã‚¯ã‚·ãƒ§ãƒ³ï¼‰
                if (line_stripped.startswith('#') or 
                    line_stripped.startswith('*') or 
                    line_stripped.startswith('##') or
                    line_stripped.startswith('**') or
                    line_stripped.startswith('---') or
                    line_stripped.startswith('===') or
                    'æ”¹å–„ãƒã‚¤ãƒ³ãƒˆ' in line_stripped or
                    'æœŸå¾…åŠ¹æœ' in line_stripped or
                    'BROADCASTé©ç”¨æ ¹æ‹ ' in line_stripped):
                    in_sql_section = False
                else:
                    # ç©ºè¡Œã‚„æœ‰åŠ¹ãªSQLè¡Œã‚’è¿½åŠ 
                    sql_lines.append(line)
        
        optimized_sql = '\n'.join(sql_lines).strip()
    
    # SQLæ§‹æ–‡ã®åŸºæœ¬ãƒã‚§ãƒƒã‚¯ï¼ˆå®Œå…¨æ€§ç¢ºèªï¼‰
    if optimized_sql:
        optimized_sql = validate_and_fix_sql_syntax(optimized_sql)
    
    # æœ€é©åŒ–ã•ã‚ŒãŸã‚¯ã‚¨ãƒªãƒ•ã‚¡ã‚¤ãƒ«ã®ä¿å­˜ï¼ˆã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°å¼·åŒ–ï¼‰
    try:
        with open(optimized_filename, 'w', encoding='utf-8') as f:
            f.write(f"-- æœ€é©åŒ–ã•ã‚ŒãŸSQLã‚¯ã‚¨ãƒª\n")
            f.write(f"-- å…ƒã‚¯ã‚¨ãƒªID: {query_id}\n")
            f.write(f"-- æœ€é©åŒ–æ—¥æ™‚: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
            f.write(f"-- ãƒ•ã‚¡ã‚¤ãƒ«: {optimized_filename}\n\n")
            
            if optimized_sql:
                # SQLã®æœ«å°¾ã«ã‚»ãƒŸã‚³ãƒ­ãƒ³ã‚’ç¢ºå®Ÿã«è¿½åŠ 
                optimized_sql_clean = optimized_sql.strip()
                if optimized_sql_clean and not optimized_sql_clean.endswith(';'):
                    optimized_sql_clean += ';'
                
                # æœ€çµ‚çš„ãªæ§‹æ–‡ãƒã‚§ãƒƒã‚¯
                if validate_final_sql_syntax(optimized_sql_clean):
                    f.write(optimized_sql_clean)
                else:
                    f.write("-- âš ï¸ æ§‹æ–‡ã‚¨ãƒ©ãƒ¼ãŒæ¤œå‡ºã•ã‚Œã¾ã—ãŸã€‚æ‰‹å‹•ã§ç¢ºèªã—ã¦ãã ã•ã„ã€‚\n")
                    f.write(f"-- å…ƒã®SQL:\n{optimized_sql_clean}\n")
                    f.write("-- ä»¥ä¸‹ã¯æœ€é©åŒ–åˆ†æã®å…¨çµæœã§ã™:\n\n")
                    f.write(f"/*\n{optimized_result_main_content}\n*/")
            else:
                f.write("-- âš ï¸ SQLã‚³ãƒ¼ãƒ‰ã®è‡ªå‹•æŠ½å‡ºã«å¤±æ•—ã—ã¾ã—ãŸ\n")
                f.write("-- ä»¥ä¸‹ã¯æœ€é©åŒ–åˆ†æã®å…¨çµæœã§ã™:\n\n")
                f.write(f"/*\n{optimized_result_main_content}\n*/")
    except Exception as e:
        print(f"âš ï¸ SQLãƒ•ã‚¡ã‚¤ãƒ«ä¿å­˜ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(e)}")
        # ã‚¨ãƒ©ãƒ¼æ™‚ã¯åŸºæœ¬çš„ãªãƒ•ã‚¡ã‚¤ãƒ«ã‚’ç”Ÿæˆ
        with open(optimized_filename, 'w', encoding='utf-8') as f:
            f.write(f"-- âš ï¸ SQLãƒ•ã‚¡ã‚¤ãƒ«ä¿å­˜ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(e)}\n")
            f.write(f"-- æœ€é©åŒ–çµæœ:\n{optimized_result_main_content}\n")
    
    # åˆ†æãƒ¬ãƒãƒ¼ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ã®ä¿å­˜ï¼ˆLLMã§æ¨æ•²ã•ã‚ŒãŸèª­ã¿ã‚„ã™ã„ãƒ¬ãƒãƒ¼ãƒˆï¼‰
    report_filename = f"output_optimization_report_{timestamp}.md"
    
    print("ğŸ¤– LLMã«ã‚ˆã‚‹ãƒ¬ãƒãƒ¼ãƒˆæ¨æ•²ã‚’å®Ÿè¡Œä¸­...")
    
    # åˆæœŸãƒ¬ãƒãƒ¼ãƒˆã®ç”Ÿæˆ
    initial_report = generate_comprehensive_optimization_report(
        query_id, optimized_result_for_file, metrics, analysis_result
    )
    
    # LLMã§ãƒ¬ãƒãƒ¼ãƒˆã‚’æ¨æ•²ï¼ˆè©³ç´°ãªæŠ€è¡“æƒ…å ±ã‚’ä¿æŒï¼‰
    refined_report = refine_report_with_llm(initial_report, query_id)
    
    with open(report_filename, 'w', encoding='utf-8') as f:
        f.write(refined_report)
    
    print("âœ… LLMã«ã‚ˆã‚‹ãƒ¬ãƒãƒ¼ãƒˆæ¨æ•²å®Œäº†")
    
    # å‡ºåŠ›ãƒ•ã‚¡ã‚¤ãƒ«ã®çµæœï¼ˆç‹¬ç«‹ã—ãŸTOP10ãƒ•ã‚¡ã‚¤ãƒ«ã¯å‰Šé™¤ã—ã€æœ€é©åŒ–ãƒ¬ãƒãƒ¼ãƒˆã«çµ±åˆï¼‰
    result = {
        'optimized_file': optimized_filename,
        'report_file': report_filename
    }
    
    return result

def demonstrate_execution_plan_size_extraction():
    """
    å®Ÿè¡Œãƒ—ãƒ©ãƒ³ã‹ã‚‰ã®ã‚µã‚¤ã‚ºæ¨å®šæ©Ÿèƒ½ã®ãƒ‡ãƒ¢ãƒ³ã‚¹ãƒˆãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³
    """
    print("ğŸ§ª å®Ÿè¡Œãƒ—ãƒ©ãƒ³ã‹ã‚‰ã®ãƒ†ãƒ¼ãƒ–ãƒ«ã‚µã‚¤ã‚ºæ¨å®šæ©Ÿèƒ½ã®ãƒ‡ãƒ¢")
    print("-" * 50)
    
    # ã‚µãƒ³ãƒ—ãƒ«ã®ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ©ãƒ¼ãƒ‡ãƒ¼ã‚¿æ§‹é€ 
    sample_profiler_data = {
        "executionPlan": {
            "physicalPlan": {
                "nodes": [
                    {
                        "nodeName": "Scan Delta orders",
                        "id": "1",
                        "metrics": {
                            "estimatedSizeInBytes": 10485760,  # 10MB
                            "numFiles": 5,
                            "numPartitions": 2
                        },
                        "output": "[order_id#123, customer_id#124, amount#125] orders",
                        "details": "Table: catalog.database.orders"
                    },
                    {
                        "nodeName": "Scan Delta customers",
                        "id": "2", 
                        "metrics": {
                            "estimatedSizeInBytes": 52428800,  # 50MB
                            "numFiles": 10,
                            "numPartitions": 4
                        },
                        "output": "[customer_id#126, name#127, region#128] customers"
                    }
                ]
            }
        }
    }
    
    print("ğŸ“Š ã‚µãƒ³ãƒ—ãƒ«å®Ÿè¡Œãƒ—ãƒ©ãƒ³:")
    print("  â€¢ orders ãƒ†ãƒ¼ãƒ–ãƒ«: estimatedSizeInBytes = 10,485,760 (10MB)")
    print("  â€¢ customers ãƒ†ãƒ¼ãƒ–ãƒ«: estimatedSizeInBytes = 52,428,800 (50MB)")
    print("")
    
    # ãƒ†ãƒ¼ãƒ–ãƒ«ã‚µã‚¤ã‚ºæ¨å®šã®å®Ÿè¡Œ
    table_size_estimates = extract_table_size_estimates_from_plan(sample_profiler_data)
    
    print("ğŸ” æŠ½å‡ºã•ã‚ŒãŸãƒ†ãƒ¼ãƒ–ãƒ«ã‚µã‚¤ã‚ºæ¨å®š:")
    if table_size_estimates:
        for table_name, size_info in table_size_estimates.items():
            print(f"  ğŸ“‹ {table_name}:")
            print(f"    - ã‚µã‚¤ã‚º: {size_info['estimated_size_mb']:.1f}MB")
            print(f"    - ä¿¡é ¼åº¦: {size_info['confidence']}")
            print(f"    - ã‚½ãƒ¼ã‚¹: {size_info['source']}")
            if 'num_files' in size_info:
                print(f"    - ãƒ•ã‚¡ã‚¤ãƒ«æ•°: {size_info['num_files']}")
            if 'num_partitions' in size_info:
                print(f"    - ãƒ‘ãƒ¼ãƒ†ã‚£ã‚·ãƒ§ãƒ³æ•°: {size_info['num_partitions']}")
            print("")
    else:
        print("  âš ï¸ ãƒ†ãƒ¼ãƒ–ãƒ«ã‚µã‚¤ã‚ºæ¨å®šæƒ…å ±ãŒæŠ½å‡ºã•ã‚Œã¾ã›ã‚“ã§ã—ãŸ")
    
    print("ğŸ’¡ BROADCASTåˆ†æã¸ã®å½±éŸ¿:")
    if table_size_estimates:
        for table_name, size_info in table_size_estimates.items():
            size_mb = size_info['estimated_size_mb']
            if size_mb <= 30:
                print(f"  âœ… {table_name}: {size_mb:.1f}MB â‰¤ 30MB â†’ BROADCASTæ¨å¥¨")
            else:
                print(f"  âŒ {table_name}: {size_mb:.1f}MB > 30MB â†’ BROADCASTéæ¨å¥¨")
    
    print("")
    print("ğŸ¯ å¾“æ¥ã®æ¨å®šæ–¹æ³•ã¨ã®æ¯”è¼ƒ:")
    print("  ğŸ“ˆ å¾“æ¥: ãƒ¡ãƒˆãƒªã‚¯ã‚¹ãƒ™ãƒ¼ã‚¹ã®é–“æ¥æ¨å®šï¼ˆæ¨å®šç²¾åº¦: ä¸­ï¼‰")
    print("  âŒ æ–°æ©Ÿèƒ½: å®Ÿè¡Œãƒ—ãƒ©ãƒ³ã® estimatedSizeInBytes æ´»ç”¨ï¼ˆåˆ©ç”¨ä¸å¯ã®ãŸã‚ç„¡åŠ¹åŒ–ï¼‰")
    print("  â„¹ï¸ ç¾åœ¨: 3.0å€åœ§ç¸®ç‡ã§ã®ä¿å®ˆçš„æ¨å®šã‚’æ¡ç”¨")
    
    return {}

print("âœ… é–¢æ•°å®šç¾©å®Œäº†: SQLæœ€é©åŒ–é–¢é€£é–¢æ•°ï¼ˆå®Ÿè¡Œãƒ—ãƒ©ãƒ³ã‚µã‚¤ã‚ºæ¨å®šå¯¾å¿œï¼‰")

# COMMAND ----------

# MAGIC %md
# MAGIC ## ğŸš€ ã‚¯ã‚¨ãƒªæŠ½å‡º
# MAGIC
# MAGIC ã“ã®ã‚»ãƒ«ã§ã¯ä»¥ä¸‹ã®å‡¦ç†ã‚’å®Ÿè¡Œã—ã¾ã™ï¼š
# MAGIC - ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ©ãƒ¼ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰ã‚ªãƒªã‚¸ãƒŠãƒ«ã‚¯ã‚¨ãƒªã®æŠ½å‡º
# MAGIC - æŠ½å‡ºã•ã‚ŒãŸã‚¯ã‚¨ãƒªã®è©³ç´°è¡¨ç¤ºï¼ˆ64KBã¾ã§ï¼‰
# MAGIC - ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯å‡¦ç†ï¼ˆã‚µãƒ³ãƒ—ãƒ«ã‚¯ã‚¨ãƒªã®è¨­å®šï¼‰

# COMMAND ----------

# ğŸš€ SQLã‚¯ã‚¨ãƒªæœ€é©åŒ–ã®å®Ÿè¡Œ
print("\n" + "ğŸš€" * 20)
print("ğŸ”§ ã€SQLã‚¯ã‚¨ãƒªæœ€é©åŒ–ã®å®Ÿè¡Œã€‘")
print("ğŸš€" * 20)

# 1. ã‚ªãƒªã‚¸ãƒŠãƒ«ã‚¯ã‚¨ãƒªã®æŠ½å‡º
print("\nğŸ“‹ ã‚¹ãƒ†ãƒƒãƒ—1: ã‚ªãƒªã‚¸ãƒŠãƒ«ã‚¯ã‚¨ãƒªã®æŠ½å‡º")
print("-" * 40)

original_query = extract_original_query_from_profiler_data(profiler_data)

if original_query:
    print(f"âœ… ã‚ªãƒªã‚¸ãƒŠãƒ«ã‚¯ã‚¨ãƒªã‚’æŠ½å‡ºã—ã¾ã—ãŸ ({len(original_query)} æ–‡å­—)")
    print(f"ğŸ” ã‚¯ã‚¨ãƒªãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼:")
    # 64KB (65536æ–‡å­—) ã¾ã§è¡¨ç¤º
    max_display_chars = 65536
    if len(original_query) > max_display_chars:
        preview = original_query[:max_display_chars] + f"\n... (æ®‹ã‚Š {len(original_query) - max_display_chars} æ–‡å­—ã¯çœç•¥)"
    else:
        preview = original_query
    print(f"   {preview}")
else:
    print("âš ï¸ ã‚ªãƒªã‚¸ãƒŠãƒ«ã‚¯ã‚¨ãƒªãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ")
    print("   æ‰‹å‹•ã§ã‚¯ã‚¨ãƒªã‚’è¨­å®šã—ã¦ãã ã•ã„")
    
    # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: ã‚µãƒ³ãƒ—ãƒ«ã‚¯ã‚¨ãƒªã‚’è¨­å®š
    original_query = """
    -- ã‚µãƒ³ãƒ—ãƒ«ã‚¯ã‚¨ãƒªï¼ˆå®Ÿéš›ã®ã‚¯ã‚¨ãƒªã«ç½®ãæ›ãˆã¦ãã ã•ã„ï¼‰
    SELECT 
        customer_id,
        SUM(order_amount) as total_amount,
        COUNT(*) as order_count
    FROM orders 
    WHERE order_date >= '2023-01-01'
    GROUP BY customer_id
    ORDER BY total_amount DESC
    LIMIT 100
    """
    print(f"ğŸ“ ã‚µãƒ³ãƒ—ãƒ«ã‚¯ã‚¨ãƒªã‚’è¨­å®šã—ã¾ã—ãŸ")

# COMMAND ----------

# MAGIC %md
# MAGIC ## ğŸ” EXPLAINæ–‡å®Ÿè¡Œã¨ãƒ•ã‚¡ã‚¤ãƒ«å‡ºåŠ›
# MAGIC
# MAGIC ã“ã®ã‚»ãƒ«ã§ã¯ä»¥ä¸‹ã®å‡¦ç†ã‚’å®Ÿè¡Œã—ã¾ã™ï¼š
# MAGIC - ã‚»ãƒ«43ã§æŠ½å‡ºã—ãŸã‚ªãƒªã‚¸ãƒŠãƒ«ã‚¯ã‚¨ãƒªã‚’å–å¾—
# MAGIC - EXPLAINæ–‡ã‚’ç”Ÿæˆã—ã¦Databricksã§å®Ÿè¡Œ
# MAGIC - å®Ÿè¡Œãƒ—ãƒ©ãƒ³ã®è©³ç´°ã‚’ãƒ•ã‚¡ã‚¤ãƒ«ã«å‡ºåŠ›
# MAGIC - ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°ã¨çµæœã®ç¢ºèª

# COMMAND ----------

def extract_select_from_ctas(query: str) -> str:
    """
    CREATE TABLE AS SELECT (CTAS) ã‚¯ã‚¨ãƒªã‹ã‚‰ASä»¥é™ã®éƒ¨åˆ†ã®ã¿ã‚’æŠ½å‡º
    
    å¯¾å¿œãƒ‘ã‚¿ãƒ¼ãƒ³:
    - CREATE TABLE ... AS SELECT ...
    - CREATE OR REPLACE TABLE ... AS SELECT ...
    - CREATE TABLE ... AS WITH ... SELECT ...
    - AS ã®å¾Œã‚ã«æ‹¬å¼§ãŒãªã„å ´åˆ
    - è¤‡æ•°è¡Œã«ã¾ãŸãŒã‚‹å ´åˆ
    - ãƒ†ãƒ¼ãƒ–ãƒ«å®šç¾©ã®è¤‡é›‘ãªãƒ‘ã‚¿ãƒ¼ãƒ³ï¼ˆUSINGã€PARTITIONED BYã€TBLPROPERTIESç­‰ï¼‰
    
    Args:
        query: å…ƒã®ã‚¯ã‚¨ãƒª
    
    Returns:
        str: ASä»¥é™ã®éƒ¨åˆ†ã®ã¿ã®ã‚¯ã‚¨ãƒªã€ã¾ãŸã¯CTASã§ãªã„å ´åˆã¯å…ƒã®ã‚¯ã‚¨ãƒª
    """
    import re
    
    # ã‚¯ã‚¨ãƒªã‚’æ­£è¦åŒ–ï¼ˆæ”¹è¡Œãƒ»ç©ºç™½ã‚’çµ±ä¸€ï¼‰
    normalized_query = re.sub(r'\s+', ' ', query.strip())
    
    # CTAS ãƒ‘ã‚¿ãƒ¼ãƒ³ã®æ¤œå‡ºï¼ˆåŒ…æ‹¬çš„ãªãƒ‘ã‚¿ãƒ¼ãƒ³ï¼‰
    # CREATE [OR REPLACE] TABLE ... AS ... ã®å½¢å¼ã‚’æ¤œå‡º
    # ASã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã®ä½ç½®ã‚’æ­£ç¢ºã«ç‰¹å®šã™ã‚‹
    
    # CREATE [OR REPLACE] TABLEéƒ¨åˆ†ã®æ¤œå‡º
    create_patterns = [
        r'CREATE\s+OR\s+REPLACE\s+TABLE',
        r'CREATE\s+TABLE'
    ]
    
    for create_pattern in create_patterns:
        # CREATE TABLEéƒ¨åˆ†ã‚’æ¤œå‡º
        create_match = re.search(create_pattern, normalized_query, re.IGNORECASE)
        if create_match:
            # CREATE TABLEä»¥é™ã®éƒ¨åˆ†ã‚’å–å¾—
            after_create = normalized_query[create_match.end():].strip()
            
            # AS ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã®ä½ç½®ã‚’æ¤œç´¢ï¼ˆå¤§æ–‡å­—å°æ–‡å­—ã‚’åŒºåˆ¥ã—ãªã„ï¼‰
            # AS ã¯å˜èªå¢ƒç•Œã§åŒºåˆ‡ã‚‰ã‚Œã¦ã„ã‚‹å¿…è¦ãŒã‚ã‚‹
            as_pattern = r'\bAS\b'
            as_match = re.search(as_pattern, after_create, re.IGNORECASE)
            
            if as_match:
                # ASä»¥é™ã®éƒ¨åˆ†ã‚’å–å¾—
                as_part = after_create[as_match.end():].strip()
                
                if as_part:
                    print(f"âœ… CTASæ¤œå‡º: ASä»¥é™ã®éƒ¨åˆ†ã‚’EXPLAINæ–‡ã«ä½¿ç”¨")
                    print(f"ğŸ“Š å…ƒã®ã‚¯ã‚¨ãƒªé•·: {len(query):,} æ–‡å­—")
                    print(f"ğŸ“Š ASä»¥é™éƒ¨åˆ†é•·: {len(as_part):,} æ–‡å­—")
                    
                    # WITHå¥ã§å§‹ã¾ã‚‹å ´åˆã‚„SELECTå¥ã§å§‹ã¾ã‚‹å ´åˆã‚’åˆ¤å®š
                    if as_part.upper().startswith('WITH'):
                        print("ğŸ“‹ WITHå¥ã§å§‹ã¾ã‚‹ã‚¯ã‚¨ãƒªã‚’æ¤œå‡º")
                    elif as_part.upper().startswith('SELECT'):
                        print("ğŸ“‹ SELECTå¥ã§å§‹ã¾ã‚‹ã‚¯ã‚¨ãƒªã‚’æ¤œå‡º")
                    else:
                        print("ğŸ“‹ ãã®ä»–ã®ã‚¯ã‚¨ãƒªå½¢å¼ã‚’æ¤œå‡º")
                    
                    return as_part
    
    print("ğŸ“‹ é€šå¸¸ã®ã‚¯ã‚¨ãƒª: ãã®ã¾ã¾EXPLAINæ–‡ã«ä½¿ç”¨")
    return query

def generate_optimized_query_with_error_feedback(original_query: str, analysis_result: str, metrics: Dict[str, Any], error_info: str = "") -> str:
    """
    ã‚¨ãƒ©ãƒ¼æƒ…å ±ã‚’å«ã‚ã¦LLMã«ã‚ˆã‚‹SQLæœ€é©åŒ–ã‚’å®Ÿè¡Œ
    ã‚¨ãƒ©ãƒ¼ä¿®æ­£ã«ç‰¹åŒ–ã—ãŸãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’ä½¿ç”¨
    """
    
    error_feedback_prompt = f"""
ã‚ãªãŸã¯Databricksã®SQLãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æœ€é©åŒ–ã¨ã‚¨ãƒ©ãƒ¼ä¿®æ­£ã®å°‚é–€å®¶ã§ã™ã€‚

ä»¥ä¸‹ã®æœ€é©åŒ–ã‚¯ã‚¨ãƒªã§EXPLAINå®Ÿè¡Œæ™‚ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸã€‚ã‚¨ãƒ©ãƒ¼æƒ…å ±ã‚’åŸºã«ä¿®æ­£ã—ã¦ãã ã•ã„ã€‚

ã€ğŸš¨ ç™ºç”Ÿã—ãŸã‚¨ãƒ©ãƒ¼æƒ…å ±ã€‘
{error_info}

ã€å…ƒã®åˆ†æå¯¾è±¡ã‚¯ã‚¨ãƒªã€‘
```sql
{original_query}
```

ã€è©³ç´°ãªãƒœãƒˆãƒ«ãƒãƒƒã‚¯åˆ†æçµæœã€‘
{analysis_result}

ã€ğŸ”§ ã‚¨ãƒ©ãƒ¼ä¿®æ­£ã®é‡è¦ãªæŒ‡é‡ã€‘
1. **æ§‹æ–‡ã‚¨ãƒ©ãƒ¼ã®ä¿®æ­£**: SQLæ§‹æ–‡ã®æ–‡æ³•ã‚¨ãƒ©ãƒ¼ã‚’æœ€å„ªå…ˆã§ä¿®æ­£
2. **ãƒ†ãƒ¼ãƒ–ãƒ«ãƒ»ã‚«ãƒ©ãƒ åã®ç¢ºèª**: å­˜åœ¨ã—ãªã„ãƒ†ãƒ¼ãƒ–ãƒ«ã‚„ã‚«ãƒ©ãƒ ã®ä¿®æ­£
3. **å‹å¤‰æ›ã‚¨ãƒ©ãƒ¼ã®ä¿®æ­£**: ä¸é©åˆ‡ãªå‹å¤‰æ›ã‚„ã‚­ãƒ£ã‚¹ãƒˆã®ä¿®æ­£
4. **ãƒ’ãƒ³ãƒˆå¥ã®ä¿®æ­£**: ä¸æ­£ãªãƒ’ãƒ³ãƒˆæ§‹æ–‡ã®ä¿®æ­£
5. **æ¨©é™ã‚¨ãƒ©ãƒ¼ã®å›é¿**: ã‚¢ã‚¯ã‚»ã‚¹æ¨©é™ã®ãªã„ãƒ†ãƒ¼ãƒ–ãƒ«ã®ä»£æ›¿ç­–
6. **æœ€é©åŒ–ãƒ¬ãƒ™ãƒ«ã®èª¿æ•´**: è¤‡é›‘ã™ãã‚‹æœ€é©åŒ–ã®ç°¡ç´ åŒ–

ã€ğŸš¨ BROADCASTãƒ’ãƒ³ãƒˆé…ç½®ã®å³æ ¼ãªãƒ«ãƒ¼ãƒ« - ã‚¨ãƒ©ãƒ¼ä¿®æ­£ç‰ˆã€‘
- **å¿…ãšãƒ¡ã‚¤ãƒ³ã‚¯ã‚¨ãƒªã®æœ€åˆã®SELECTæ–‡ã®ç›´å¾Œã®ã¿**ã«é…ç½®
- **ã‚µãƒ–ã‚¯ã‚¨ãƒªå†…éƒ¨ã«ã¯çµ¶å¯¾ã«é…ç½®ã—ãªã„**
- **FROMå¥ã€JOINå¥ã€WHEREå¥å†…ã«ã¯çµ¶å¯¾ã«é…ç½®ã—ãªã„**
- **ãƒ†ãƒ¼ãƒ–ãƒ«åã¾ãŸã¯ã‚¨ã‚¤ãƒªã‚¢ã‚¹åã‚’å¿…ãšæŒ‡å®š**: `/*+ BROADCAST(table_name) */`

ã€ğŸš¨ REPARTITIONãƒ’ãƒ³ãƒˆé…ç½®ã®å³æ ¼ãªãƒ«ãƒ¼ãƒ« - ã‚¨ãƒ©ãƒ¼ä¿®æ­£ç‰ˆã€‘
- **ã‚µãƒ–ã‚¯ã‚¨ãƒªå†…éƒ¨ã®SELECTæ–‡ç›´å¾Œã«é…ç½®**
- **ãƒ‘ãƒ¼ãƒ†ã‚£ã‚·ãƒ§ãƒ³æ•°ã¨ã‚«ãƒ©ãƒ åã¯å¿…é ˆ**: `/*+ REPARTITION(200, column_name) */`
- **ã‚¹ãƒ”ãƒ«æ¤œå‡ºæ™‚ã®ã¿é©ç”¨**

ã€é‡è¦ãªåˆ¶ç´„ - ã‚¨ãƒ©ãƒ¼ä¿®æ­£ç‰ˆã€‘
- æ§‹æ–‡ã‚¨ãƒ©ãƒ¼ã‚’çµ¶å¯¾ã«ç™ºç”Ÿã•ã›ãªã„å®Œå…¨ãªSQLã‚¯ã‚¨ãƒªã‚’ç”Ÿæˆ
- ã™ã¹ã¦ã®ã‚«ãƒ©ãƒ åã€ãƒ†ãƒ¼ãƒ–ãƒ«åã‚’å®Œå…¨ã«è¨˜è¿°
- ãƒ—ãƒ¬ãƒ¼ã‚¹ãƒ›ãƒ«ãƒ€ãƒ¼ï¼ˆ...ã€[çœç•¥]ï¼‰ã¯ä¸€åˆ‡ä½¿ç”¨ç¦æ­¢
- å…ƒã®ã‚¯ã‚¨ãƒªã®DISTINCTå¥ã¯å¿…ãšä¿æŒ
- å®Ÿéš›ã«å®Ÿè¡Œã§ãã‚‹å®Œå…¨ãªSQLã‚¯ã‚¨ãƒªã®ã¿ã‚’å‡ºåŠ›

ã€å‡ºåŠ›å½¢å¼ã€‘
## ğŸ”§ ã‚¨ãƒ©ãƒ¼ä¿®æ­£æ¸ˆã¿æœ€é©åŒ–SQL

**ä¿®æ­£ã—ãŸå†…å®¹**:
- [å…·ä½“çš„ãªã‚¨ãƒ©ãƒ¼ä¿®æ­£ç®‡æ‰€]
- [é©ç”¨ã—ãŸæœ€é©åŒ–æ‰‹æ³•]

```sql
[å®Œå…¨ãªSQL - ã‚¨ãƒ©ãƒ¼ä¿®æ­£æ¸ˆã¿]
```

## ä¿®æ­£è©³ç´°
[ã‚¨ãƒ©ãƒ¼ã®åŸå› ã¨ä¿®æ­£æ–¹æ³•ã®è©³ç´°èª¬æ˜]
"""

    # è¨­å®šã•ã‚ŒãŸLLMãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ã‚’ä½¿ç”¨
    provider = LLM_CONFIG["provider"]
    
    try:
        if provider == "databricks":
            optimized_result = _call_databricks_llm(error_feedback_prompt)
        elif provider == "openai":
            optimized_result = _call_openai_llm(error_feedback_prompt)
        elif provider == "azure_openai":
            optimized_result = _call_azure_openai_llm(error_feedback_prompt)
        elif provider == "anthropic":
            optimized_result = _call_anthropic_llm(error_feedback_prompt)
        else:
            return "âš ï¸ è¨­å®šã•ã‚ŒãŸLLMãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ãŒèªè­˜ã§ãã¾ã›ã‚“"
        
        return optimized_result
        
    except Exception as e:
        return f"âš ï¸ ã‚¨ãƒ©ãƒ¼ä¿®æ­£SQLç”Ÿæˆä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(e)}"


def execute_explain_with_retry_logic(original_query: str, analysis_result: str, metrics: Dict[str, Any], max_retries: int = 2) -> Dict[str, Any]:
    """
    EXPLAINå®Ÿè¡Œã¨ã‚¨ãƒ©ãƒ¼ä¿®æ­£ã®å†è©¦è¡Œãƒ­ã‚¸ãƒƒã‚¯
    æœ€å¤§2å›ã¾ã§è‡ªå‹•ä¿®æ­£ã‚’è©¦è¡Œã—ã€å¤±æ•—æ™‚ã¯å…ƒã‚¯ã‚¨ãƒªã‚’ä½¿ç”¨
    """
    from datetime import datetime
    
    print(f"\nğŸ”„ EXPLAINå®Ÿè¡Œã¨è‡ªå‹•ã‚¨ãƒ©ãƒ¼ä¿®æ­£ï¼ˆæœ€å¤§{max_retries}å›è©¦è¡Œï¼‰")
    print("=" * 60)
    
    # åˆå›ã®æœ€é©åŒ–ã‚¯ã‚¨ãƒªç”Ÿæˆ
    print("ğŸ¤– ã‚¹ãƒ†ãƒƒãƒ—1: åˆå›æœ€é©åŒ–ã‚¯ã‚¨ãƒªç”Ÿæˆ")
    optimized_query = generate_optimized_query_with_llm(original_query, analysis_result, metrics)
    
    # thinking_enabledå¯¾å¿œ: ãƒªã‚¹ãƒˆå½¢å¼ã®å ´åˆã¯ãƒ¡ã‚¤ãƒ³ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’æŠ½å‡º
    if isinstance(optimized_query, list):
        optimized_query_str = extract_main_content_from_thinking_response(optimized_query)
    else:
        optimized_query_str = str(optimized_query)
    
    # SQLã‚¯ã‚¨ãƒªéƒ¨åˆ†ã®ã¿ã‚’æŠ½å‡º
    extracted_sql = extract_sql_from_llm_response(optimized_query_str)
    current_query = extracted_sql if extracted_sql else original_query
    
    retry_count = 0
    all_attempts = []  # å…¨è©¦è¡Œã®è¨˜éŒ²
    
    while retry_count <= max_retries:
        attempt_num = retry_count + 1
        print(f"\nğŸ” è©¦è¡Œ {attempt_num}/{max_retries + 1}: EXPLAINå®Ÿè¡Œ")
        
        # EXPLAINå®Ÿè¡Œ
        explain_result = execute_explain_and_save_to_file(current_query)
        
        # æˆåŠŸæ™‚ã®å‡¦ç†
        if 'explain_file' in explain_result and 'error_file' not in explain_result:
            print(f"âœ… è©¦è¡Œ {attempt_num} ã§æˆåŠŸã—ã¾ã—ãŸï¼")
            
            # æˆåŠŸè¨˜éŒ²
            attempt_record = {
                'attempt': attempt_num,
                'status': 'success',
                'query': current_query,
                'explain_file': explain_result.get('explain_file'),
                'plan_lines': explain_result.get('plan_lines', 0)
            }
            all_attempts.append(attempt_record)
            
            # æœ€çµ‚çµæœ
            return {
                'final_status': 'success',
                'final_query': current_query,
                'total_attempts': attempt_num,
                'all_attempts': all_attempts,
                'explain_result': explain_result,
                'optimized_result': optimized_query  # å…ƒã®å®Œå…¨ãªãƒ¬ã‚¹ãƒãƒ³ã‚¹
            }
        
        # ã‚¨ãƒ©ãƒ¼æ™‚ã®å‡¦ç†
        elif 'error_file' in explain_result:
            error_message = explain_result.get('error_message', 'Unknown error')
            print(f"âŒ è©¦è¡Œ {attempt_num} ã§ã‚¨ãƒ©ãƒ¼ç™ºç”Ÿ: {error_message}")
            
            # ã‚¨ãƒ©ãƒ¼è¨˜éŒ²
            attempt_record = {
                'attempt': attempt_num,
                'status': 'error',
                'query': current_query,
                'error_message': error_message,
                'error_file': explain_result.get('error_file')
            }
            all_attempts.append(attempt_record)
            
            # æœ€å¤§è©¦è¡Œå›æ•°ã«é”ã—ãŸå ´åˆ
            if retry_count >= max_retries:
                print(f"ğŸš¨ æœ€å¤§è©¦è¡Œå›æ•°ï¼ˆ{max_retries}å›ï¼‰ã«é”ã—ã¾ã—ãŸ")
                print("ğŸ“‹ å…ƒã®å‹•ä½œå¯èƒ½ã‚¯ã‚¨ãƒªã‚’ä½¿ç”¨ã—ã¾ã™")
                
                # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: å…ƒã‚¯ã‚¨ãƒªã§ã®ãƒ•ã‚¡ã‚¤ãƒ«ç”Ÿæˆ
                fallback_result = save_optimized_sql_files(
                    original_query, 
                    f"# ğŸš¨ æœ€é©åŒ–ã‚¯ã‚¨ãƒªã®EXPLAINå®Ÿè¡ŒãŒ{max_retries}å›ã¨ã‚‚å¤±æ•—ã—ãŸãŸã‚ã€å…ƒã‚¯ã‚¨ãƒªã‚’ä½¿ç”¨\n\n## æœ€å¾Œã®ã‚¨ãƒ©ãƒ¼æƒ…å ±\n{error_message}\n\n## å…ƒã®ã‚¯ã‚¨ãƒª\n```sql\n{original_query}\n```",
                    metrics,
                    analysis_result
                )
                
                # å¤±æ•—æ™‚ã®ãƒ­ã‚°è¨˜éŒ²
                timestamp = datetime.now().strftime("%Y%m%d-%H%M%S")
                log_filename = f"output_optimization_failure_log_{timestamp}.txt"
                
                try:
                    with open(log_filename, 'w', encoding='utf-8') as f:
                        f.write(f"# æœ€é©åŒ–ã‚¯ã‚¨ãƒªç”Ÿæˆå¤±æ•—ãƒ­ã‚°\n")
                        f.write(f"å®Ÿè¡Œæ—¥æ™‚: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
                        f.write(f"æœ€å¤§è©¦è¡Œå›æ•°: {max_retries}å›\n")
                        f.write(f"æœ€çµ‚çµæœ: å…ƒã‚¯ã‚¨ãƒªã‚’ä½¿ç”¨\n\n")
                        
                        f.write("=" * 80 + "\n")
                        f.write("å…¨è©¦è¡Œã®è©³ç´°è¨˜éŒ²:\n")
                        f.write("=" * 80 + "\n\n")
                        
                        for attempt in all_attempts:
                            f.write(f"ã€è©¦è¡Œ {attempt['attempt']}ã€‘\n")
                            f.write(f"ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹: {attempt['status']}\n")
                            if attempt['status'] == 'error':
                                f.write(f"ã‚¨ãƒ©ãƒ¼: {attempt['error_message']}\n")
                                f.write(f"ã‚¨ãƒ©ãƒ¼ãƒ•ã‚¡ã‚¤ãƒ«: {attempt.get('error_file', 'N/A')}\n")
                            f.write(f"ä½¿ç”¨ã‚¯ã‚¨ãƒªé•·: {len(attempt['query'])} æ–‡å­—\n\n")
                        
                        f.write("=" * 80 + "\n")
                        f.write("å…ƒã®ã‚¯ã‚¨ãƒªï¼ˆãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ä½¿ç”¨ï¼‰:\n")
                        f.write("=" * 80 + "\n\n")
                        f.write(original_query)
                    
                    print(f"ğŸ“„ å¤±æ•—ãƒ­ã‚°ã‚’ä¿å­˜: {log_filename}")
                    
                except Exception as log_error:
                    print(f"âŒ å¤±æ•—ãƒ­ã‚°ã®ä¿å­˜ã«ã‚‚å¤±æ•—: {str(log_error)}")
                
                return {
                    'final_status': 'fallback_to_original',
                    'final_query': original_query,
                    'total_attempts': attempt_num,
                    'all_attempts': all_attempts,
                    'fallback_files': fallback_result,
                    'failure_log': log_filename
                }
            
            # å†è©¦è¡Œã™ã‚‹å ´åˆã®ã‚¨ãƒ©ãƒ¼ä¿®æ­£
            retry_count += 1
            print(f"ğŸ”§ è©¦è¡Œ {retry_count + 1} ã«å‘ã‘ã¦ã‚¨ãƒ©ãƒ¼ä¿®æ­£ä¸­...")
            
            # ã‚¨ãƒ©ãƒ¼æƒ…å ±ã‚’å«ã‚ã¦å†ç”Ÿæˆ
            corrected_query = generate_optimized_query_with_error_feedback(
                original_query, 
                analysis_result, 
                metrics, 
                error_message
            )
            
            # thinking_enabledå¯¾å¿œ
            if isinstance(corrected_query, list):
                corrected_query_str = extract_main_content_from_thinking_response(corrected_query)
            else:
                corrected_query_str = str(corrected_query)
            
            # SQLã‚¯ã‚¨ãƒªéƒ¨åˆ†ã®ã¿ã‚’æŠ½å‡º
            extracted_sql = extract_sql_from_llm_response(corrected_query_str)
            current_query = extracted_sql if extracted_sql else current_query
            
            print(f"âœ… ã‚¨ãƒ©ãƒ¼ä¿®æ­£ã‚¯ã‚¨ãƒªã‚’ç”Ÿæˆã—ã¾ã—ãŸï¼ˆ{len(current_query)} æ–‡å­—ï¼‰")
    
    # ã“ã“ã«ã¯åˆ°é”ã—ãªã„ã¯ãšã ãŒã€å®‰å…¨ã®ãŸã‚
    return {
        'final_status': 'unexpected_error',
        'final_query': original_query,
        'total_attempts': retry_count + 1,
        'all_attempts': all_attempts
    }


def extract_sql_from_llm_response(llm_response: str) -> str:
    """
    LLMãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‹ã‚‰SQLã‚¯ã‚¨ãƒªéƒ¨åˆ†ã®ã¿ã‚’æŠ½å‡º
    """
    import re
    
    # SQLã‚³ãƒ¼ãƒ‰ãƒ–ãƒ­ãƒƒã‚¯ã‚’æ¤œç´¢ï¼ˆ```sql ... ```ï¼‰
    sql_pattern = r'```sql\s*(.*?)\s*```'
    matches = re.findall(sql_pattern, llm_response, re.DOTALL | re.IGNORECASE)
    
    if matches:
        # æœ€é•·ã®SQLãƒ–ãƒ­ãƒƒã‚¯ã‚’é¸æŠ
        sql_query = max(matches, key=len).strip()
        return sql_query
    
    # SQLã‚³ãƒ¼ãƒ‰ãƒ–ãƒ­ãƒƒã‚¯ãŒè¦‹ã¤ã‹ã‚‰ãªã„å ´åˆã€åˆ¥ã®ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’è©¦è¡Œ
    # ```ã®ã¿ã®ã‚³ãƒ¼ãƒ‰ãƒ–ãƒ­ãƒƒã‚¯
    code_pattern = r'```\s*(.*?)\s*```'
    matches = re.findall(code_pattern, llm_response, re.DOTALL)
    
    for match in matches:
        match = match.strip()
        # SQLã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã§å§‹ã¾ã‚‹ã‹ãƒã‚§ãƒƒã‚¯
        if re.match(r'^(SELECT|WITH|CREATE|INSERT|UPDATE|DELETE|EXPLAIN)', match, re.IGNORECASE):
            return match
    
    # ãƒ‘ã‚¿ãƒ¼ãƒ³ãƒãƒƒãƒã—ãªã„å ´åˆã¯å…ƒã®ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‚’ãã®ã¾ã¾è¿”ã™
    return llm_response.strip()


def execute_explain_and_save_to_file(original_query: str) -> Dict[str, str]:
    """
    ã‚ªãƒªã‚¸ãƒŠãƒ«ã‚¯ã‚¨ãƒªã®EXPLAINæ–‡ã‚’å®Ÿè¡Œã—ã€çµæœã‚’ãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜
    CTASã®å ´åˆã¯SELECTéƒ¨åˆ†ã®ã¿ã‚’æŠ½å‡ºã—ã¦EXPLAINæ–‡ã«æ¸¡ã™
    """
    from datetime import datetime
    import os
    
    if not original_query or not original_query.strip():
        print("âŒ ã‚ªãƒªã‚¸ãƒŠãƒ«ã‚¯ã‚¨ãƒªãŒç©ºã§ã™")
        return {}
    
    # ãƒ•ã‚¡ã‚¤ãƒ«åã®ç”Ÿæˆ
    timestamp = datetime.now().strftime("%Y%m%d-%H%M%S")
    explain_filename = f"output_explain_plan_{timestamp}.txt"
    
    # CTASã®å ´åˆã¯SELECTéƒ¨åˆ†ã®ã¿ã‚’æŠ½å‡º
    query_for_explain = extract_select_from_ctas(original_query)
    
    # EXPLAINæ–‡ã®ç”Ÿæˆ
    explain_query = f"EXPLAIN {query_for_explain}"
    
    # EXPLAINæ–‡ã®å®Ÿè¡Œ
    try:
        print("ğŸ”„ EXPLAINæ–‡ã‚’å®Ÿè¡Œä¸­...")
        
        # ã‚«ã‚¿ãƒ­ã‚°ã¨ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã®è¨­å®šã‚’å–å¾—
        catalog = globals().get('CATALOG', 'main')
        database = globals().get('DATABASE', 'default')
        
        print(f"ğŸ“‚ ä½¿ç”¨ã‚«ã‚¿ãƒ­ã‚°: {catalog}")
        print(f"ğŸ—‚ï¸ ä½¿ç”¨ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹: {database}")
        
        # ã‚«ã‚¿ãƒ­ã‚°ã¨ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‚’è¨­å®š
        spark.sql(f"USE CATALOG {catalog}")
        spark.sql(f"USE DATABASE {database}")
        
        # Databricksç’°å¢ƒã§Spark SQLã‚’å®Ÿè¡Œ
        result = spark.sql(explain_query)
        
        # çµæœã‚’åé›†
        explain_result = result.collect()
        
        # çµæœã‚’ãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜
        with open(explain_filename, 'w', encoding='utf-8') as f:
            f.write(f"# EXPLAINå®Ÿè¡Œçµæœ\n")
            f.write(f"å®Ÿè¡Œæ—¥æ™‚: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
            f.write(f"ã‚ªãƒªã‚¸ãƒŠãƒ«ã‚¯ã‚¨ãƒªæ–‡å­—æ•°: {len(original_query):,}\n")
            f.write("\n" + "=" * 80 + "\n")
            f.write("EXPLAINçµæœ:\n")
            f.write("=" * 80 + "\n\n")
            
            for row in explain_result:
                f.write(str(row[0]) + "\n")
        
        print(f"âœ… EXPLAINçµæœã‚’ä¿å­˜: {explain_filename}")
        print(f"ğŸ“Š å®Ÿè¡Œãƒ—ãƒ©ãƒ³è¡Œæ•°: {len(explain_result):,}")
        
        # çµæœã®ãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼è¡¨ç¤º
        print("\nğŸ“‹ EXPLAINçµæœã®ãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼:")
        print("-" * 50)
        preview_lines = min(10, len(explain_result))
        for i, row in enumerate(explain_result[:preview_lines]):
            print(f"{i+1:2d}: {str(row[0])[:100]}...")
        
        if len(explain_result) > preview_lines:
            print(f"... (æ®‹ã‚Š {len(explain_result) - preview_lines} è¡Œã¯ {explain_filename} ã‚’å‚ç…§)")
        print("-" * 50)
        
        return {
            'explain_file': explain_filename,
            'plan_lines': len(explain_result)
        }
        
    except Exception as e:
        error_message = str(e)
        print(f"âŒ EXPLAINæ–‡ã®å®Ÿè¡Œã«å¤±æ•—: {error_message}")
        
        # è‡´å‘½çš„ãªã‚¨ãƒ©ãƒ¼ã®ãƒã‚§ãƒƒã‚¯
        fatal_error_patterns = [
            "Error occurred during query planning",
            "error occurred during query planning",
            "Query planning failed",
            "query planning failed",
            "Plan optimization failed",
            "plan optimization failed",
            "Failed to plan query",
            "failed to plan query",
            "Analysis exception",
            "analysis exception"
        ]
        
        # è‡´å‘½çš„ãªã‚¨ãƒ©ãƒ¼ãƒ‘ã‚¿ãƒ¼ãƒ³ã®ãƒã‚§ãƒƒã‚¯
        is_fatal_error = any(pattern in error_message.lower() for pattern in fatal_error_patterns)
        
        if is_fatal_error:
            print(f"ğŸš¨ FATAL: EXPLAINæ–‡ã§ã‚¯ã‚¨ãƒªãƒ—ãƒ©ãƒ³ãƒ‹ãƒ³ã‚°ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ")
            print(f"ğŸš¨ ã‚¨ãƒ©ãƒ¼è©³ç´°: {error_message}")
            print(f"ğŸš¨ å‡¦ç†ã‚’çµ‚äº†ã—ã¾ã™ã€‚")
            
            # ã‚¨ãƒ©ãƒ¼ãƒ•ã‚¡ã‚¤ãƒ«ã®ä¿å­˜
            error_filename = f"output_explain_fatal_error_{timestamp}.txt"
            try:
                with open(error_filename, 'w', encoding='utf-8') as f:
                    f.write(f"# FATAL EXPLAINå®Ÿè¡Œã‚¨ãƒ©ãƒ¼\n")
                    f.write(f"å®Ÿè¡Œæ—¥æ™‚: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
                    f.write(f"ã‚¨ãƒ©ãƒ¼å†…å®¹: {error_message}\n")
                    f.write(f"ã‚¨ãƒ©ãƒ¼ã‚¿ã‚¤ãƒ—: FATAL - Query Planning Error\n")
                    f.write("\n" + "=" * 80 + "\n")
                    f.write("å®Ÿè¡Œã—ã‚ˆã†ã¨ã—ãŸEXPLAINæ–‡:\n")
                    f.write("=" * 80 + "\n\n")
                    f.write(explain_query)
                
                print(f"ğŸ“„ Fatal ã‚¨ãƒ©ãƒ¼è©³ç´°ã‚’ä¿å­˜: {error_filename}")
                
            except Exception as file_error:
                print(f"âŒ Fatal ã‚¨ãƒ©ãƒ¼ãƒ•ã‚¡ã‚¤ãƒ«ã®ä¿å­˜ã«ã‚‚å¤±æ•—: {str(file_error)}")
            
            # ãƒ—ãƒ­ã‚°ãƒ©ãƒ ã‚’çµ‚äº†
            import sys
            sys.exit(1)
        
        # éè‡´å‘½çš„ãªã‚¨ãƒ©ãƒ¼ã®å ´åˆã¯å¾“æ¥é€šã‚Šã®å‡¦ç†
        error_filename = f"output_explain_error_{timestamp}.txt"
        try:
            with open(error_filename, 'w', encoding='utf-8') as f:
                f.write(f"# EXPLAINå®Ÿè¡Œã‚¨ãƒ©ãƒ¼\n")
                f.write(f"å®Ÿè¡Œæ—¥æ™‚: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
                f.write(f"ã‚¨ãƒ©ãƒ¼å†…å®¹: {error_message}\n")
                f.write("\n" + "=" * 80 + "\n")
                f.write("å®Ÿè¡Œã—ã‚ˆã†ã¨ã—ãŸEXPLAINæ–‡:\n")
                f.write("=" * 80 + "\n\n")
                f.write(explain_query)
            
            print(f"ğŸ“„ ã‚¨ãƒ©ãƒ¼è©³ç´°ã‚’ä¿å­˜: {error_filename}")
            
        except Exception as file_error:
            print(f"âŒ ã‚¨ãƒ©ãƒ¼ãƒ•ã‚¡ã‚¤ãƒ«ã®ä¿å­˜ã«ã‚‚å¤±æ•—: {str(file_error)}")
        
        return {
            'error_file': error_filename,
            'error_message': error_message
        }

# EXPLAINæ–‡å®Ÿè¡Œã®å®Ÿè¡Œ
print("\nğŸ” EXPLAINæ–‡å®Ÿè¡Œå‡¦ç†")
print("-" * 40)

# ã‚»ãƒ«43ã§æŠ½å‡ºã—ãŸã‚ªãƒªã‚¸ãƒŠãƒ«ã‚¯ã‚¨ãƒªãŒå¤‰æ•°ã«æ®‹ã£ã¦ã„ã‚‹ã‹ãƒã‚§ãƒƒã‚¯
try:
    # original_queryãŒæ—¢ã«å®šç¾©ã•ã‚Œã¦ã„ã‚‹ã‹ç¢ºèª
    original_query_for_explain = original_query
    print(f"âœ… ã‚ªãƒªã‚¸ãƒŠãƒ«ã‚¯ã‚¨ãƒªã‚’å–å¾—ã—ã¾ã—ãŸ ({len(original_query_for_explain)} æ–‡å­—)")
    
except NameError:
    print("âš ï¸ ã‚ªãƒªã‚¸ãƒŠãƒ«ã‚¯ã‚¨ãƒªãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
    print("   ã‚»ãƒ«43 (ã‚ªãƒªã‚¸ãƒŠãƒ«ã‚¯ã‚¨ãƒªæŠ½å‡º) ã‚’å…ˆã«å®Ÿè¡Œã—ã¦ãã ã•ã„")
    
    # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ©ãƒ¼ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰å†æŠ½å‡º
    try:
        print("ğŸ”„ ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ©ãƒ¼ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰å†æŠ½å‡ºã‚’è©¦è¡Œä¸­...")
        original_query_for_explain = extract_original_query_from_profiler_data(profiler_data)
        
        if original_query_for_explain:
            print(f"âœ… å†æŠ½å‡ºæˆåŠŸ ({len(original_query_for_explain)} æ–‡å­—)")
        else:
            print("âŒ å†æŠ½å‡ºã«å¤±æ•—ã—ã¾ã—ãŸ")
            original_query_for_explain = None
            
    except Exception as e:
        print(f"âŒ å†æŠ½å‡ºä¸­ã«ã‚¨ãƒ©ãƒ¼: {str(e)}")
        original_query_for_explain = None

# EXPLAINå®Ÿè¡Œãƒ•ãƒ©ã‚°ã®ç¢ºèª
explain_enabled = globals().get('EXPLAIN_ENABLED', 'N')
print(f"ğŸ” EXPLAINå®Ÿè¡Œè¨­å®š: {explain_enabled}")

if explain_enabled.upper() != 'Y':
    print("âš ï¸ EXPLAINå®Ÿè¡ŒãŒç„¡åŠ¹åŒ–ã•ã‚Œã¦ã„ã¾ã™")
    print("   EXPLAINæ–‡ã‚’å®Ÿè¡Œã™ã‚‹å ´åˆã¯ã€æœ€åˆã®ã‚»ãƒ«ã§EXPLAIN_ENABLED = 'Y'ã«è¨­å®šã—ã¦ãã ã•ã„")
elif original_query_for_explain and original_query_for_explain.strip():
    print("\nğŸš€ çµ±åˆSQLæœ€é©åŒ– & EXPLAINå®Ÿè¡Œï¼ˆè‡ªå‹•ã‚¨ãƒ©ãƒ¼ä¿®æ­£ä»˜ãï¼‰")
    
    # Sparkç’°å¢ƒã®ç¢ºèª
    try:
        spark_version = spark.version
        print(f"ğŸ“Š Sparkç’°å¢ƒ: {spark_version}")
    except Exception as e:
        print(f"âŒ Sparkç’°å¢ƒã®ç¢ºèªã«å¤±æ•—: {str(e)}")
        print("   Databricksç’°å¢ƒã§å®Ÿè¡Œã—ã¦ãã ã•ã„")
        spark = None
    
    if spark:
        # çµ±åˆå‡¦ç†: åˆ†æçµæœãŒå¿…è¦ãªã®ã§ç¢ºèª
        try:
            # analysis_resultãŒå®šç¾©ã•ã‚Œã¦ã„ã‚‹ã‹ãƒã‚§ãƒƒã‚¯
            if 'analysis_result' in globals():
                current_analysis_result = analysis_result
            else:
                print("âš ï¸ åˆ†æçµæœãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚ç°¡æ˜“åˆ†æã‚’å®Ÿè¡Œã—ã¾ã™...")
                current_analysis_result = "åˆ†æçµæœãŒåˆ©ç”¨ã§ããªã„ãŸã‚ã€åŸºæœ¬çš„ãªæœ€é©åŒ–ã®ã¿å®Ÿè¡Œ"
            
            # extracted_metricsãŒå®šç¾©ã•ã‚Œã¦ã„ã‚‹ã‹ãƒã‚§ãƒƒã‚¯  
            if 'extracted_metrics' in globals():
                current_metrics = extracted_metrics
            else:
                print("âš ï¸ ãƒ¡ãƒˆãƒªã‚¯ã‚¹ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚ç©ºã®ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã§å®Ÿè¡Œã—ã¾ã™...")
                current_metrics = {}
            
            # thinking_enabledå¯¾å¿œ
            if isinstance(current_analysis_result, list):
                analysis_result_str = extract_main_content_from_thinking_response(current_analysis_result)
            else:
                analysis_result_str = str(current_analysis_result)
            
            # ğŸš€ æ–°ã—ã„çµ±åˆå‡¦ç†: æœ€å¤§2å›å†è©¦è¡Œã®è‡ªå‹•ã‚¨ãƒ©ãƒ¼ä¿®æ­£
            retry_result = execute_explain_with_retry_logic(
                original_query_for_explain, 
                analysis_result_str, 
                current_metrics, 
                max_retries=2
            )
            
            # çµæœã®è¡¨ç¤º
            print(f"\nğŸ“Š æœ€çµ‚çµæœ: {retry_result['final_status']}")
            print(f"ğŸ”„ ç·è©¦è¡Œå›æ•°: {retry_result['total_attempts']}")
            
            if retry_result['final_status'] == 'success':
                print("âœ… æœ€é©åŒ–ã‚¯ã‚¨ãƒªã®EXPLAINå®Ÿè¡Œã«æˆåŠŸã—ã¾ã—ãŸï¼")
                
                # æˆåŠŸæ™‚ã®ãƒ•ã‚¡ã‚¤ãƒ«æƒ…å ±è¡¨ç¤º
                explain_result = retry_result.get('explain_result', {})
                if explain_result:
                    print("\nğŸ“ ç”Ÿæˆã•ã‚ŒãŸãƒ•ã‚¡ã‚¤ãƒ«:")
                    if 'explain_file' in explain_result:
                        print(f"   ğŸ“„ EXPLAINçµæœ: {explain_result['explain_file']}")
                    if 'plan_lines' in explain_result:
                        print(f"   ğŸ“Š å®Ÿè¡Œãƒ—ãƒ©ãƒ³è¡Œæ•°: {explain_result['plan_lines']:,}")
                
                # æœ€é©åŒ–ã•ã‚ŒãŸã‚¯ã‚¨ãƒªã®ä¿å­˜
                optimized_result = retry_result.get('optimized_result', '')
                final_query = retry_result.get('final_query', original_query_for_explain)
                
                # ãƒ•ã‚¡ã‚¤ãƒ«ä¿å­˜
                saved_files = save_optimized_sql_files(
                    original_query_for_explain,
                    optimized_result,
                    current_metrics,
                    analysis_result_str
                )
                
                print("\nğŸ“ æœ€é©åŒ–ãƒ•ã‚¡ã‚¤ãƒ«:")
                for file_type, filename in saved_files.items():
                    print(f"   ğŸ“„ {file_type}: {filename}")
                    
            elif retry_result['final_status'] == 'fallback_to_original':
                print("âš ï¸ æœ€é©åŒ–ã‚¯ã‚¨ãƒªã§ã‚¨ãƒ©ãƒ¼ãŒç¶™ç¶šã—ãŸãŸã‚ã€å…ƒã‚¯ã‚¨ãƒªã‚’ä½¿ç”¨ã—ã¾ã—ãŸ")
                
                # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯æ™‚ã®ãƒ•ã‚¡ã‚¤ãƒ«æƒ…å ±è¡¨ç¤º
                fallback_files = retry_result.get('fallback_files', {})
                failure_log = retry_result.get('failure_log', '')
                
                print("\nğŸ“ ç”Ÿæˆã•ã‚ŒãŸãƒ•ã‚¡ã‚¤ãƒ«:")
                for file_type, filename in fallback_files.items():
                    print(f"   ğŸ“„ {file_type}: {filename}")
                if failure_log:
                    print(f"   ğŸ“„ å¤±æ•—ãƒ­ã‚°: {failure_log}")
                    
            # å…¨è©¦è¡Œã®è©³ç´°è¡¨ç¤º
            print("\nğŸ“‹ è©¦è¡Œè©³ç´°:")
            for attempt in retry_result.get('all_attempts', []):
                status_icon = "âœ…" if attempt['status'] == 'success' else "âŒ"
                print(f"   {status_icon} è©¦è¡Œ {attempt['attempt']}: {attempt['status']}")
                if attempt['status'] == 'error':
                    print(f"      ã‚¨ãƒ©ãƒ¼: {attempt['error_message'][:100]}...")
                    
        except Exception as e:
            print(f"âŒ çµ±åˆå‡¦ç†ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿ: {str(e)}")
            print("   å¾“æ¥ã®EXPLAINå®Ÿè¡Œã«åˆ‡ã‚Šæ›¿ãˆã¾ã™...")
            
            # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: å¾“æ¥ã®EXPLAINå®Ÿè¡Œ
            explain_results = execute_explain_and_save_to_file(original_query_for_explain)
            
            if explain_results:
                print("\nğŸ“ ç”Ÿæˆã•ã‚ŒãŸãƒ•ã‚¡ã‚¤ãƒ«:")
                for file_type, filename in explain_results.items():
                    if file_type == 'explain_file':
                        print(f"   ğŸ“„ EXPLAINçµæœ: {filename}")
                    elif file_type == 'error_file':
                        print(f"   ğŸ“„ ã‚¨ãƒ©ãƒ¼ãƒ­ã‚°: {filename}")
                    elif file_type == 'plan_lines':
                        print(f"   ğŸ“Š å®Ÿè¡Œãƒ—ãƒ©ãƒ³è¡Œæ•°: {filename}")
                    elif file_type == 'error_message':
                        print(f"   âŒ ã‚¨ãƒ©ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸: {filename}")
        
        print("\nâœ… çµ±åˆSQLæœ€é©åŒ–å‡¦ç†ãŒå®Œäº†ã—ã¾ã—ãŸ")
        
    else:
        print("âŒ Sparkç’°å¢ƒãŒåˆ©ç”¨ã§ããªã„ãŸã‚ã€EXPLAINæ–‡ã¯å®Ÿè¡Œã§ãã¾ã›ã‚“")
        print("   Databricksç’°å¢ƒã§å®Ÿè¡Œã—ã¦ãã ã•ã„")
        
else:
    print("âŒ å®Ÿè¡Œå¯èƒ½ãªã‚ªãƒªã‚¸ãƒŠãƒ«ã‚¯ã‚¨ãƒªãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
    print("   ã‚»ãƒ«43ã§ã‚ªãƒªã‚¸ãƒŠãƒ«ã‚¯ã‚¨ãƒªã‚’æŠ½å‡ºã—ã¦ã‹ã‚‰å®Ÿè¡Œã—ã¦ãã ã•ã„")

print()

# COMMAND ----------

# MAGIC %md
# MAGIC ## ğŸ¤– å¾“æ¥ã®SQLæœ€é©åŒ–ï¼ˆå‚è€ƒï¼‰
# MAGIC
# MAGIC ã“ã®ã‚»ãƒ«ã¯æ–°ã—ã„çµ±åˆå‡¦ç†ã¨ã¯ç‹¬ç«‹ã—ãŸå¾“æ¥ã®æœ€é©åŒ–å‡¦ç†ã§ã™ã€‚
# MAGIC çµ±åˆå‡¦ç†ãŒå¤±æ•—ã—ãŸå ´åˆã‚„ãƒ‡ãƒãƒƒã‚°ç›®çš„ã§ä½¿ç”¨ã§ãã¾ã™ã€‚

# COMMAND ----------

# ğŸ¤– å¾“æ¥ã®ã‚¹ãƒ†ãƒƒãƒ—2: LLMã«ã‚ˆã‚‹SQLæœ€é©åŒ–ï¼ˆå‚è€ƒï¼‰
print("\nğŸ¤– å¾“æ¥ã®ã‚¹ãƒ†ãƒƒãƒ—2: LLMã«ã‚ˆã‚‹SQLæœ€é©åŒ–ï¼ˆå‚è€ƒï¼‰")
print("-" * 40)

# æ—¢å­˜å¤‰æ•°ã®ç¢ºèª
try:
    test_original_query = original_query
    test_analysis_result = analysis_result if 'analysis_result' in globals() else ""
    test_extracted_metrics = extracted_metrics if 'extracted_metrics' in globals() else {}
    
    print("ğŸ“‹ å¾“æ¥å‡¦ç†ç”¨ã®å¤‰æ•°ç¢ºèª:")
    print(f"   original_query: {len(str(test_original_query))} æ–‡å­—")
    print(f"   analysis_result: {len(str(test_analysis_result))} æ–‡å­—")
    print(f"   extracted_metrics: {len(test_extracted_metrics)} é …ç›®")
    
except NameError as e:
    print(f"âš ï¸ å¿…è¦ãªå¤‰æ•°ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {str(e)}")
    print("   ãƒ¡ã‚¤ãƒ³å‡¦ç†ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã‚’å…ˆã«å®Ÿè¡Œã—ã¦ãã ã•ã„")

if 'original_query' in globals() and original_query.strip():
    print(f"ğŸ”„ å¾“æ¥ã®{LLM_CONFIG['provider'].upper()}æœ€é©åŒ–ã‚’å®Ÿè¡Œä¸­...")
    
    # thinking_enabled: Trueã®å ´åˆã«analysis_resultãŒãƒªã‚¹ãƒˆã«ãªã‚‹ã“ã¨ãŒã‚ã‚‹ãŸã‚å¯¾å¿œ
    if 'analysis_result' in globals():
        if isinstance(analysis_result, list):
            # ãƒªã‚¹ãƒˆã®å ´åˆã¯ä¸»è¦ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã®ã¿ã‚’æŠ½å‡ºã—ã¦LLMã«æ¸¡ã™
            analysis_result_str = extract_main_content_from_thinking_response(analysis_result)
        else:
            analysis_result_str = str(analysis_result)
    else:
        analysis_result_str = "åˆ†æçµæœãŒåˆ©ç”¨ã§ãã¾ã›ã‚“"
    
    traditional_optimized_result = generate_optimized_query_with_llm(
        original_query, 
        analysis_result_str, 
        extracted_metrics if 'extracted_metrics' in globals() else {}
    )
    
    # thinking_enabled: Trueã®å ´åˆã«optimized_resultãŒãƒªã‚¹ãƒˆã«ãªã‚‹ã“ã¨ãŒã‚ã‚‹ãŸã‚å¯¾å¿œ
    optimized_result_display = optimized_result
    if isinstance(optimized_result, list):
        # è¡¨ç¤ºç”¨ã¯äººé–“ã«èª­ã¿ã‚„ã™ã„å½¢å¼ã«å¤‰æ›
        optimized_result_display = format_thinking_response(optimized_result)
        # ä¸»è¦ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã®ã¿ã‚’æŠ½å‡ºï¼ˆå¾Œç¶šå‡¦ç†ç”¨ï¼‰
        optimized_result = extract_main_content_from_thinking_response(optimized_result)
    
    if optimized_result and not str(optimized_result).startswith("âš ï¸"):
        print("âœ… SQLæœ€é©åŒ–ãŒå®Œäº†ã—ã¾ã—ãŸ")
        print(f"ğŸ“„ æœ€é©åŒ–çµæœã®è©³ç´°:")
        
        # æœ€é©åŒ–çµæœã®è©³ç´°ã‚’è¡¨ç¤ºï¼ˆ1000è¡Œã¾ã§ï¼‰
        lines = optimized_result_display.split('\n')
        max_display_lines = 1000
        
        if len(lines) <= max_display_lines:
            # å…¨è¡Œè¡¨ç¤º
            for line in lines:
                print(f"   {line}")
        else:
            # 1000è¡Œã¾ã§è¡¨ç¤º
            for line in lines[:max_display_lines]:
                print(f"   {line}")
            print(f"   ... (æ®‹ã‚Š {len(lines) - max_display_lines} è¡Œã¯çœç•¥ã€è©³ç´°ã¯ä¿å­˜ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ç¢ºèª)")
        
    else:
        print(f"âŒ SQLæœ€é©åŒ–ã«å¤±æ•—ã—ã¾ã—ãŸ")
        print(f"   ã‚¨ãƒ©ãƒ¼: {optimized_result}")
        optimized_result = "æœ€é©åŒ–ã®ç”Ÿæˆã«å¤±æ•—ã—ã¾ã—ãŸã€‚æ‰‹å‹•ã§ã®æœ€é©åŒ–ã‚’æ¤œè¨ã—ã¦ãã ã•ã„ã€‚"
else:
    print("âš ï¸ ã‚ªãƒªã‚¸ãƒŠãƒ«ã‚¯ã‚¨ãƒªãŒç©ºã®ãŸã‚ã€æœ€é©åŒ–ã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™")
    optimized_result = "ã‚ªãƒªã‚¸ãƒŠãƒ«ã‚¯ã‚¨ãƒªãŒè¦‹ã¤ã‹ã‚‰ãªã„ãŸã‚ã€æœ€é©åŒ–ã§ãã¾ã›ã‚“ã§ã—ãŸã€‚"

# COMMAND ----------

# MAGIC %md
# MAGIC ## ğŸ’¾ æœ€é©åŒ–çµæœã®ä¿å­˜
# MAGIC
# MAGIC ã“ã®ã‚»ãƒ«ã§ã¯ä»¥ä¸‹ã®å‡¦ç†ã‚’å®Ÿè¡Œã—ã¾ã™ï¼š
# MAGIC - æœ€é©åŒ–ã•ã‚ŒãŸSQLã‚¯ã‚¨ãƒªã®ãƒ•ã‚¡ã‚¤ãƒ«ä¿å­˜ï¼ˆæ¥é ­èª: output_ï¼‰
# MAGIC - ã‚ªãƒªã‚¸ãƒŠãƒ«ã‚¯ã‚¨ãƒªã€æœ€é©åŒ–ã‚¯ã‚¨ãƒªã€ãƒ¬ãƒãƒ¼ãƒˆã®ç”Ÿæˆ
# MAGIC - ç”Ÿæˆãƒ•ã‚¡ã‚¤ãƒ«ã®è©³ç´°æƒ…å ±è¡¨ç¤º

# COMMAND ----------

# ğŸ’¾ ã‚¹ãƒ†ãƒƒãƒ—3: æœ€é©åŒ–çµæœã®ä¿å­˜
print("\nğŸ’¾ ã‚¹ãƒ†ãƒƒãƒ—3: æœ€é©åŒ–çµæœã®ä¿å­˜")
print("-" * 40)

# å¿…è¦ãªå¤‰æ•°ãŒå®šç¾©ã•ã‚Œã¦ã„ã‚‹ã‹ãƒã‚§ãƒƒã‚¯ã—ã€ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤ã‚’è¨­å®š
missing_variables = []

# original_query ã®ãƒã‚§ãƒƒã‚¯
try:
    original_query
except NameError:
    missing_variables.append("original_query")
    original_query = ""

# optimized_result ã®ãƒã‚§ãƒƒã‚¯  
try:
    optimized_result
except NameError:
    missing_variables.append("optimized_result (ã‚»ãƒ«20ã‚’å®Ÿè¡Œã—ã¦ãã ã•ã„)")
    optimized_result = ""

# extracted_metrics ã®ãƒã‚§ãƒƒã‚¯
try:
    extracted_metrics
except NameError:
    missing_variables.append("extracted_metrics (ã‚»ãƒ«12ã‚’å®Ÿè¡Œã—ã¦ãã ã•ã„)")
    # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤ã¨ã—ã¦æœ€å°é™ã®æ§‹é€ ã‚’è¨­å®š
    extracted_metrics = {
        'query_info': {'query_id': 'unknown'},
        'overall_metrics': {},
        'bottleneck_indicators': {}
    }

# analysis_result ã®ãƒã‚§ãƒƒã‚¯
try:
    analysis_result
except NameError:
    missing_variables.append("analysis_result")
    analysis_result = ""

if missing_variables:
    print("âŒ å¿…è¦ãªå¤‰æ•°ãŒå®šç¾©ã•ã‚Œã¦ã„ã¾ã›ã‚“:")
    for var in missing_variables:
        print(f"   â€¢ {var}")
    print("\nâš ï¸ ä¸Šè¨˜ã®ã‚»ãƒ«ã‚’å…ˆã«å®Ÿè¡Œã—ã¦ã‹ã‚‰ã€ã“ã®ã‚»ãƒ«ã‚’å†å®Ÿè¡Œã—ã¦ãã ã•ã„ã€‚")
    print("ğŸ“‹ æ­£ã—ã„å®Ÿè¡Œé †åº: ã‚»ãƒ«11 â†’ ã‚»ãƒ«12 â†’ ... â†’ ã‚»ãƒ«19 â†’ ã‚»ãƒ«20 â†’ ã‚»ãƒ«21")
    print("\nğŸ”„ ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤ã‚’ä½¿ç”¨ã—ã¦å‡¦ç†ã‚’ç¶™ç¶šã—ã¾ã™ã€‚")

# å¤‰æ•°ãŒå­˜åœ¨ã™ã‚‹ï¼ˆã¾ãŸã¯ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤ãŒè¨­å®šã•ã‚ŒãŸï¼‰å ´åˆã®å‡¦ç†
if original_query.strip() and str(optimized_result).strip():
    print("ğŸ“ ãƒ•ã‚¡ã‚¤ãƒ«ç”Ÿæˆä¸­...")
    
    try:
        saved_files = save_optimized_sql_files(
            original_query,
            optimized_result,
            extracted_metrics,
            analysis_result
        )
        
        print("âœ… ä»¥ä¸‹ã®ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ç”Ÿæˆã—ã¾ã—ãŸ:")
        for file_type, filename in saved_files.items():
            file_type_jp = {
                'original_file': 'ã‚ªãƒªã‚¸ãƒŠãƒ«SQLã‚¯ã‚¨ãƒª',
                'optimized_file': 'æœ€é©åŒ–SQLã‚¯ã‚¨ãƒª',
                'report_file': 'æœ€é©åŒ–ãƒ¬ãƒãƒ¼ãƒˆ'
            }
            print(f"   ğŸ“„ {file_type_jp.get(file_type, file_type)}: {filename}")
        
        # ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚ºã®ç¢ºèª
        import os
        print(f"\nğŸ“Š ç”Ÿæˆãƒ•ã‚¡ã‚¤ãƒ«ã®è©³ç´°:")
        for file_type, filename in saved_files.items():
            if os.path.exists(filename):
                file_size = os.path.getsize(filename)
                print(f"   {filename}: {file_size:,} bytes")
            else:
                print(f"   âš ï¸ {filename}: ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
        
    except Exception as e:
        print(f"âŒ ãƒ•ã‚¡ã‚¤ãƒ«ç”Ÿæˆä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(e)}")
        print("âš ï¸ ç©ºã®ãƒ•ã‚¡ã‚¤ãƒ«ãƒªã‚¹ãƒˆã‚’è¨­å®šã—ã¾ã™ã€‚")
        saved_files = {}
        
else:
    print("âš ï¸ ã‚¯ã‚¨ãƒªã¾ãŸã¯æœ€é©åŒ–çµæœãŒä¸å®Œå…¨ãªãŸã‚ã€ãƒ•ã‚¡ã‚¤ãƒ«ä¿å­˜ã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã—ãŸ")
    saved_files = {}



# COMMAND ----------

# MAGIC %md
# MAGIC ## ğŸ“ ãƒ¬ãƒãƒ¼ãƒˆæ¨æ•²å‡¦ç†
# MAGIC
# MAGIC ã“ã®ã‚»ãƒ«ã§ã¯ä»¥ä¸‹ã®å‡¦ç†ã‚’å®Ÿè¡Œã—ã¾ã™ï¼š
# MAGIC - ã‚»ãƒ«47ã§å‡ºåŠ›ã•ã‚ŒãŸãƒ¬ãƒãƒ¼ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ã®èª­ã¿è¾¼ã¿
# MAGIC - LLMã«ã‚ˆã‚‹ãƒ¬ãƒãƒ¼ãƒˆã®æ¨æ•²ï¼ˆèª­ã¿ã‚„ã™ãã€ç°¡æ½”ã«ï¼‰
# MAGIC - æ¨æ•²ã•ã‚ŒãŸãƒ¬ãƒãƒ¼ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ã®ç”Ÿæˆ

# COMMAND ----------

# ğŸ“ ãƒ¬ãƒãƒ¼ãƒˆæ¨æ•²å‡¦ç†
print("\nğŸ“ ãƒ¬ãƒãƒ¼ãƒˆæ¨æ•²å‡¦ç†")
print("-" * 40)

def find_latest_report_file() -> str:
    """æœ€æ–°ã®ãƒ¬ãƒãƒ¼ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ã‚’è¦‹ã¤ã‘ã‚‹"""
    import os
    import glob
    
    # ç¾åœ¨ã®ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã§ãƒ¬ãƒãƒ¼ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ¤œç´¢
    pattern = "output_optimization_report_*.md"
    report_files = glob.glob(pattern)
    
    if not report_files:
        return None
    
    # æœ€æ–°ã®ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å–å¾—ï¼ˆã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—é †ï¼‰
    latest_file = max(report_files, key=os.path.getctime)
    return latest_file

def refine_report_content_with_llm(report_content: str) -> str:
    """LLMã‚’ä½¿ã£ã¦ãƒ¬ãƒãƒ¼ãƒˆã‚’æ¨æ•²ã™ã‚‹"""
    
    # LLMãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ã®è¨­å®šç¢ºèª
    if not LLM_CONFIG or not LLM_CONFIG.get('provider'):
        print("âŒ LLMãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“")
        return report_content
    
    # Photonåˆ©ç”¨ç‡ã®æŠ½å‡ºã¨è©•ä¾¡åˆ¤å®š
    import re
    photon_pattern = r'åˆ©ç”¨ç‡[ï¼š:]\s*(\d+(?:\.\d+)?)%'
    photon_match = re.search(photon_pattern, report_content)
    
    photon_evaluation_instruction = ""
    if photon_match:
        photon_utilization = float(photon_match.group(1))
        if photon_utilization <= 80:
            photon_evaluation_instruction = """
ã€Photonåˆ©ç”¨ç‡è©•ä¾¡æŒ‡ç¤ºã€‘
- Photonåˆ©ç”¨ç‡ãŒ80%ä»¥ä¸‹ã®å ´åˆã¯ã€Œè¦æ”¹å–„ã€ã¾ãŸã¯ã€Œä¸è‰¯ã€ã®è©•ä¾¡ã‚’æ˜ç¢ºã«è¡¨ç¤ºã—ã¦ãã ã•ã„
- 80%ä»¥ä¸‹ã®å ´åˆã¯ã€æ”¹å–„ã®å¿…è¦æ€§ã‚’å¼·èª¿ã—ã€å…·ä½“çš„ãªæ”¹å–„ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ã‚’æç¤ºã—ã¦ãã ã•ã„
- è©•ä¾¡ä¾‹: ã€ŒPhotonåˆ©ç”¨ç‡: XX% (è©•ä¾¡: è¦æ”¹å–„)ã€
"""
        else:
            photon_evaluation_instruction = """
ã€Photonåˆ©ç”¨ç‡è©•ä¾¡æŒ‡ç¤ºã€‘
- Photonåˆ©ç”¨ç‡ãŒ80%ä»¥ä¸Šã®å ´åˆã¯ã€Œè‰¯å¥½ã€ã®è©•ä¾¡ã‚’è¡¨ç¤ºã—ã¦ãã ã•ã„
- è©•ä¾¡ä¾‹: ã€ŒPhotonåˆ©ç”¨ç‡: XX% (è©•ä¾¡: è‰¯å¥½)ã€
"""
    
    refinement_prompt = f"""ã‚ãªãŸã¯æŠ€è¡“æ–‡æ›¸ã®ç·¨é›†è€…ã§ã™ã€‚ä»¥ä¸‹ã®Databricks SQLãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹åˆ†æãƒ¬ãƒãƒ¼ãƒˆã‚’ã€èª­ã¿ã‚„ã™ãç°¡æ½”ã«æ¨æ•²ã—ã¦ãã ã•ã„ã€‚

ã€æ¨æ•²ã®è¦ä»¶ã€‘
1. å…¨ä½“çš„ãªæ§‹æˆã‚’æ•´ç†ã—ã€æƒ…å ±ã‚’è«–ç†çš„ã«é…ç½®ã™ã‚‹
2. å†—é•·ãªè¡¨ç¾ã‚’å‰Šé™¤ã—ã€ç°¡æ½”ã§åˆ†ã‹ã‚Šã‚„ã™ã„è¡¨ç¾ã«ä¿®æ­£ã™ã‚‹
3. é‡è¦ãªæƒ…å ±ãŒåŸ‹ã‚‚ã‚Œãªã„ã‚ˆã†ã€é©åˆ‡ãªè¦‹å‡ºã—ãƒ¬ãƒ™ãƒ«ã§æ§‹é€ åŒ–ã™ã‚‹
4. å°‚é–€ç”¨èªã¯æ®‹ã—ã¤ã¤ã€åˆ†ã‹ã‚Šã‚„ã™ã„èª¬æ˜ã‚’è¿½åŠ ã™ã‚‹
5. æ•°å€¤ãƒ‡ãƒ¼ã‚¿ã‚„ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã¯ä¿æŒã™ã‚‹
6. å®Ÿç”¨çš„ãªæ¨å¥¨äº‹é …ã‚’æ˜ç¢ºã«æç¤ºã™ã‚‹

ã€ğŸš¨ çµ¶å¯¾ã«å‰Šé™¤ãƒ»å¤‰æ›´ã—ã¦ã¯ã„ã‘ãªã„é‡è¦æƒ…å ±ã€‘
- **ç¾åœ¨ã®ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°ã‚­ãƒ¼æƒ…å ±**: ã€Œç¾åœ¨ã®ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°ã‚­ãƒ¼: XXã€ã¾ãŸã¯ã€Œè¨­å®šãªã—ã€è¡¨ç¤º
- **ãƒ•ã‚£ãƒ«ã‚¿ç‡æƒ…å ±**: ã€Œãƒ•ã‚£ãƒ«ã‚¿ç‡: X.X% (èª­ã¿è¾¼ã¿: XX.XXGB, ãƒ—ãƒ«ãƒ¼ãƒ³: XX.XXGB)ã€å½¢å¼
- **ãƒ‘ãƒ¼ã‚»ãƒ³ãƒ†ãƒ¼ã‚¸è¨ˆç®—**: å„å‡¦ç†ã®ã€Œå…¨ä½“ã®â—‹â—‹%ã€è¡¨ç¤ºï¼ˆä¸¦åˆ—å®Ÿè¡Œã‚’è€ƒæ…®ã—ãŸæ­£ç¢ºãªè¨ˆç®—çµæœï¼‰
- **æ¨å¥¨vsç¾åœ¨ã®æ¯”è¼ƒåˆ†æ**: æ¨å¥¨ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°ã‚­ãƒ¼ã¨ç¾åœ¨ã®ã‚­ãƒ¼ã®å¯¾æ¯”æƒ…å ±
- **å…·ä½“çš„æ•°å€¤ãƒ¡ãƒˆãƒªã‚¯ã‚¹**: å®Ÿè¡Œæ™‚é–“ã€ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿é‡ã€ã‚¹ãƒ”ãƒ«é‡ã€åˆ©ç”¨ç‡ç­‰
- **SQLå®Ÿè£…ä¾‹**: ALTER TABLEæ§‹æ–‡ã€CLUSTER BYæ–‡ã€ãƒ’ãƒ³ãƒˆå¥ç­‰ã®å…·ä½“ä¾‹
- **ãƒ†ãƒ¼ãƒ–ãƒ«åˆ¥è©³ç´°æƒ…å ±**: å„ãƒ†ãƒ¼ãƒ–ãƒ«ã®ãƒãƒ¼ãƒ‰æƒ…å ±ã€ãƒ•ã‚£ãƒ«ã‚¿åŠ¹ç‡ã€æ¨å¥¨äº‹é …

{photon_evaluation_instruction}

ã€ç¾åœ¨ã®ãƒ¬ãƒãƒ¼ãƒˆå†…å®¹ã€‘
{report_content}

ã€å‡ºåŠ›è¦ä»¶ã€‘
- æ¨æ•²ã•ã‚ŒãŸãƒ¬ãƒãƒ¼ãƒˆã‚’markdownå½¢å¼ã§å‡ºåŠ›
- æŠ€è¡“æƒ…å ±ã¯ç¶­æŒã—ã¤ã¤ã€å¯èª­æ€§ã‚’å‘ä¸Šã•ã›ã‚‹
- é‡è¦ãªãƒã‚¤ãƒ³ãƒˆã‚’å¼·èª¿ã—ã€ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ãƒ—ãƒ©ãƒ³ã‚’æ˜ç¢ºã«ã™ã‚‹
- Photonåˆ©ç”¨ç‡ã®è©•ä¾¡ã‚’æ˜ç¢ºã«è¡¨ç¤ºã™ã‚‹
- **å¿…é ˆ**: ç¾åœ¨ã®ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°ã‚­ãƒ¼æƒ…å ±ã¨ãƒ•ã‚£ãƒ«ã‚¿ç‡æƒ…å ±ã‚’å®Œå…¨ã«ä¿æŒ
- **å¿…é ˆ**: ãƒ‘ãƒ¼ã‚»ãƒ³ãƒ†ãƒ¼ã‚¸è¨ˆç®—å€¤ã¯å…ƒã®æ­£ç¢ºãªæ•°å€¤ã‚’ä½¿ç”¨
- **å¿…é ˆ**: ãƒ†ãƒ¼ãƒ–ãƒ«åˆ¥ã®è©³ç´°åˆ†ææƒ…å ±ï¼ˆç¾åœ¨ã®ã‚­ãƒ¼ã€æ¨å¥¨ã‚­ãƒ¼ã€ãƒ•ã‚£ãƒ«ã‚¿ç‡ï¼‰ã‚’å‰Šé™¤ã—ãªã„
- **å¿…é ˆ**: SQLå®Ÿè£…ä¾‹ï¼ˆALTER TABLEã€CLUSTER BYç­‰ï¼‰ã¯å®Œå…¨ãªå½¢ã§ä¿æŒ
"""
    
    try:
        # è¨­å®šã•ã‚ŒãŸLLMãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ã«åŸºã¥ã„ã¦æ¨æ•²ã‚’å®Ÿè¡Œ
        provider = LLM_CONFIG.get('provider', 'databricks')
        
        if provider == 'databricks':
            refined_content = _call_databricks_llm(refinement_prompt)
        elif provider == 'openai':
            refined_content = _call_openai_llm(refinement_prompt)
        elif provider == 'azure_openai':
            refined_content = _call_azure_openai_llm(refinement_prompt)
        elif provider == 'anthropic':
            refined_content = _call_anthropic_llm(refinement_prompt)
        else:
            print(f"âŒ æœªå¯¾å¿œã®LLMãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼: {provider}")
            return report_content
        
        # thinking_enabledå¯¾å¿œ: çµæœãŒãƒªã‚¹ãƒˆã®å ´åˆã®å‡¦ç†
        if isinstance(refined_content, list):
            refined_content = format_thinking_response(refined_content)
        
        return refined_content
        
    except Exception as e:
        print(f"âŒ LLMã«ã‚ˆã‚‹ãƒ¬ãƒãƒ¼ãƒˆæ¨æ•²ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿ: {str(e)}")
        return report_content

def save_refined_report(refined_content: str, original_filename: str) -> str:
    """æ¨æ•²ã•ã‚ŒãŸãƒ¬ãƒãƒ¼ãƒˆã‚’ä¿å­˜"""
    from datetime import datetime
    
    # æ¨æ•²ç‰ˆã®ãƒ•ã‚¡ã‚¤ãƒ«åã‚’ç”Ÿæˆ
    base_name = original_filename.replace('.md', '')
    timestamp = datetime.now().strftime("%Y%m%d-%H%M%S")
    refined_filename = f"{base_name}_refined_{timestamp}.md"
    
    try:
        with open(refined_filename, 'w', encoding='utf-8') as f:
            f.write(refined_content)
        
        print(f"âœ… æ¨æ•²ã•ã‚ŒãŸãƒ¬ãƒãƒ¼ãƒˆã‚’ä¿å­˜: {refined_filename}")
        return refined_filename
        
    except Exception as e:
        print(f"âŒ æ¨æ•²ãƒ¬ãƒãƒ¼ãƒˆã®ä¿å­˜ä¸­ã«ã‚¨ãƒ©ãƒ¼: {str(e)}")
        return None

def finalize_report_files(original_filename: str, refined_filename: str) -> str:
    """å…ƒã®ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å‰Šé™¤ã—ã€æ¨æ•²ç‰ˆãƒ•ã‚¡ã‚¤ãƒ«ã‚’å…ƒã®ãƒ•ã‚¡ã‚¤ãƒ«åã«ãƒªãƒãƒ¼ãƒ """
    import os
    
    try:
        # å…ƒã®ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å‰Šé™¤
        if os.path.exists(original_filename):
            os.remove(original_filename)
            print(f"ğŸ—‘ï¸ å…ƒã®ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å‰Šé™¤: {original_filename}")
        
        # æ¨æ•²ç‰ˆãƒ•ã‚¡ã‚¤ãƒ«ã‚’å…ƒã®ãƒ•ã‚¡ã‚¤ãƒ«åã«ãƒªãƒãƒ¼ãƒ 
        if os.path.exists(refined_filename):
            os.rename(refined_filename, original_filename)
            print(f"ğŸ“ æ¨æ•²ç‰ˆãƒ•ã‚¡ã‚¤ãƒ«ã‚’ãƒªãƒãƒ¼ãƒ : {refined_filename} â†’ {original_filename}")
            return original_filename
        else:
            print(f"âŒ æ¨æ•²ç‰ˆãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {refined_filename}")
            return None
            
    except Exception as e:
        print(f"âŒ ãƒ•ã‚¡ã‚¤ãƒ«æ“ä½œä¸­ã«ã‚¨ãƒ©ãƒ¼: {str(e)}")
        return None


# ãƒ¡ã‚¤ãƒ³å‡¦ç†
try:
    # æœ€æ–°ã®ãƒ¬ãƒãƒ¼ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ¤œç´¢
    latest_report = find_latest_report_file()
    
    if not latest_report:
        print("âŒ ãƒ¬ãƒãƒ¼ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
        print("âš ï¸ ã‚»ãƒ«47 (æœ€é©åŒ–çµæœã®ä¿å­˜) ã‚’å…ˆã«å®Ÿè¡Œã—ã¦ãã ã•ã„")
    else:
        print(f"ğŸ“„ å¯¾è±¡ãƒ¬ãƒãƒ¼ãƒˆãƒ•ã‚¡ã‚¤ãƒ«: {latest_report}")
        
        # ãƒ¬ãƒãƒ¼ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ã®å†…å®¹ã‚’èª­ã¿è¾¼ã¿
        with open(latest_report, 'r', encoding='utf-8') as f:
            original_content = f.read()
        
        print(f"ğŸ“Š å…ƒãƒ¬ãƒãƒ¼ãƒˆã‚µã‚¤ã‚º: {len(original_content):,} æ–‡å­—")
        
        # LLMã«ã‚ˆã‚‹æ¨æ•²ã‚’å®Ÿè¡Œ
        print("ğŸ¤– LLMã«ã‚ˆã‚‹æ¨æ•²ã‚’å®Ÿè¡Œä¸­...")
        refined_content = refine_report_content_with_llm(original_content)
        
        if refined_content != original_content:
            print(f"ğŸ“Š æ¨æ•²å¾Œã‚µã‚¤ã‚º: {len(refined_content):,} æ–‡å­—")
            
            # æ¨æ•²ã•ã‚ŒãŸãƒ¬ãƒãƒ¼ãƒˆã‚’ä¿å­˜
            refined_filename = save_refined_report(refined_content, latest_report)
            
            if refined_filename:
                print(f"ğŸ“„ æ¨æ•²ç‰ˆãƒ¬ãƒãƒ¼ãƒˆ: {refined_filename}")
                
                # ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚ºã®ç¢ºèª
                import os
                if os.path.exists(refined_filename):
                    file_size = os.path.getsize(refined_filename)
                    print(f"ğŸ“ æ¨æ•²ç‰ˆãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚º: {file_size:,} bytes")
                
                # å…ƒã®ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å‰Šé™¤ã—ã€æ¨æ•²ç‰ˆãƒ•ã‚¡ã‚¤ãƒ«ã‚’å…ƒã®ãƒ•ã‚¡ã‚¤ãƒ«åã«ãƒªãƒãƒ¼ãƒ 
                final_filename = finalize_report_files(latest_report, refined_filename)
                
                if final_filename:
                    print(f"ğŸ“„ æœ€çµ‚ãƒ¬ãƒãƒ¼ãƒˆãƒ•ã‚¡ã‚¤ãƒ«: {final_filename}")
                    
                    # æœ€çµ‚ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚ºã®ç¢ºèª
                    if os.path.exists(final_filename):
                        final_file_size = os.path.getsize(final_filename)
                        print(f"ğŸ“ æœ€çµ‚ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚º: {final_file_size:,} bytes")
                
                print("âœ… ãƒ¬ãƒãƒ¼ãƒˆæ¨æ•²å‡¦ç†ãŒå®Œäº†ã—ã¾ã—ãŸ")
                
                # æ¨æ•²ã®çµæœã‚’è¡¨ç¤ºï¼ˆæœ€åˆã®1000æ–‡å­—ï¼‰
                print("\nğŸ“‹ æ¨æ•²çµæœã®ãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼:")
                print("-" * 50)
                preview = refined_content[:1000]
                print(preview)
                if len(refined_content) > 1000:
                    print(f"\n... (æ®‹ã‚Š {len(refined_content) - 1000} æ–‡å­—ã¯ {final_filename or latest_report} ã‚’å‚ç…§)")
                print("-" * 50)
            else:
                print("âŒ æ¨æ•²ãƒ¬ãƒãƒ¼ãƒˆã®ä¿å­˜ã«å¤±æ•—ã—ã¾ã—ãŸ")
        else:
            print("âš ï¸ æ¨æ•²ã«ã‚ˆã‚‹å¤‰æ›´ã¯ã‚ã‚Šã¾ã›ã‚“ã§ã—ãŸ")
            
except Exception as e:
    print(f"âŒ ãƒ¬ãƒãƒ¼ãƒˆæ¨æ•²å‡¦ç†ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿ: {str(e)}")
    import traceback
    traceback.print_exc()

print()

# ğŸ§¹ ä¸­é–“ãƒ•ã‚¡ã‚¤ãƒ«ã®å‰Šé™¤å‡¦ç†ï¼ˆDEBUG_ENABLEãƒ•ãƒ©ã‚°ã«åŸºã¥ãï¼‰
debug_enabled = globals().get('DEBUG_ENABLE', 'N')
explain_enabled = globals().get('EXPLAIN_ENABLED', 'N')

if debug_enabled.upper() == 'Y':
    print("\nğŸ› ãƒ‡ãƒãƒƒã‚°ãƒ¢ãƒ¼ãƒ‰æœ‰åŠ¹: ä¸­é–“ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä¿æŒã—ã¾ã™")
    print("-" * 40)
    print("ğŸ’¡ DEBUG_ENABLE=Y ã®ãŸã‚ã€ã™ã¹ã¦ã®ä¸­é–“ãƒ•ã‚¡ã‚¤ãƒ«ãŒä¿æŒã•ã‚Œã¾ã™")
    print("ğŸ“ ä»¥ä¸‹ã®ãƒ•ã‚¡ã‚¤ãƒ«ãŒä¿æŒã•ã‚Œã¾ã™:")
    
    import glob
    import os
    
    # ä¿æŒã•ã‚Œã‚‹ãƒ•ã‚¡ã‚¤ãƒ«ä¸€è¦§ã‚’è¡¨ç¤º
    explain_files = glob.glob("output_explain_plan_*.txt") if explain_enabled.upper() == 'Y' else []
    
    if explain_files:
        print(f"   ğŸ” EXPLAINçµæœãƒ•ã‚¡ã‚¤ãƒ«: {len(explain_files)} å€‹")
        for file_path in explain_files[:3]:  # æœ€å¤§3å€‹ã¾ã§è¡¨ç¤º
            print(f"      ğŸ“„ {file_path}")
        if len(explain_files) > 3:
            print(f"      ... ä»– {len(explain_files) - 3} å€‹")
    
    print("âœ… ãƒ‡ãƒãƒƒã‚°ãƒ¢ãƒ¼ãƒ‰: ãƒ•ã‚¡ã‚¤ãƒ«å‰Šé™¤å‡¦ç†ã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã—ãŸ")
else:
    print("\nğŸ§¹ ä¸­é–“ãƒ•ã‚¡ã‚¤ãƒ«ã®å‰Šé™¤å‡¦ç†")
    print("-" * 40)
    print("ğŸ’¡ DEBUG_ENABLE=N ã®ãŸã‚ã€ä¸­é–“ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å‰Šé™¤ã—ã¾ã™")
    print("ğŸ“ ä¿æŒã•ã‚Œã‚‹ãƒ•ã‚¡ã‚¤ãƒ«: output_optimization_report_*.md, output_optimized_query_*.sql")
    
    import glob
    import os
    
    if explain_enabled.upper() == 'Y':
        # EXPLAINçµæœãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ¤œç´¢
        explain_files = glob.glob("output_explain_plan_*.txt")
        
        if explain_files:
            print(f"ğŸ“ å‰Šé™¤å¯¾è±¡ã®EXPLAINçµæœãƒ•ã‚¡ã‚¤ãƒ«: {len(explain_files)} å€‹")
            
            deleted_count = 0
            for file_path in explain_files:
                try:
                    os.remove(file_path)
                    print(f"âœ… å‰Šé™¤å®Œäº†: {file_path}")
                    deleted_count += 1
                except Exception as e:
                    print(f"âŒ å‰Šé™¤å¤±æ•—: {file_path} - {str(e)}")
            
            print(f"ğŸ—‘ï¸ å‰Šé™¤å®Œäº†: {deleted_count}/{len(explain_files)} ãƒ•ã‚¡ã‚¤ãƒ«")
            print("ğŸ’¡ EXPLAINçµæœã¯LLMã«ã‚ˆã‚‹æœ€é©åŒ–å‡¦ç†ã§ä½¿ç”¨æ¸ˆã¿ã®ãŸã‚å‰Šé™¤ã—ã¾ã—ãŸ")
        else:
            print("ğŸ“ å‰Šé™¤å¯¾è±¡ã®EXPLAINçµæœãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ")
    else:
        print("âš ï¸ EXPLAINå®Ÿè¡ŒãŒç„¡åŠ¹åŒ–ã•ã‚Œã¦ã„ã‚‹ãŸã‚ã€EXPLAINçµæœãƒ•ã‚¡ã‚¤ãƒ«ã®å‰Šé™¤å‡¦ç†ã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã—ãŸ")

print()


print("ğŸ‰ ã™ã¹ã¦ã®å‡¦ç†ãŒå®Œäº†ã—ã¾ã—ãŸï¼")
print("ğŸ“ ç”Ÿæˆã•ã‚ŒãŸãƒ•ã‚¡ã‚¤ãƒ«ã‚’ç¢ºèªã—ã¦ã€åˆ†æçµæœã‚’æ´»ç”¨ã—ã¦ãã ã•ã„ã€‚")
